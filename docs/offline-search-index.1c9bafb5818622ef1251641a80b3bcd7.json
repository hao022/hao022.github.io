[{"body":"HuaTuo framework provides three data collection modes: autotracing, event, and metrics, covering different monitoring scenarios, helping users gain comprehensive insights into system performance.\nCollection Mode Comparison Mode Type Trigger Condition Data Output Use Case Autotracing Event-driven Triggered on system anomalies ES + Local Storage, Prometheus (optional) Non-routine operations, triggered on anomalies Event Event-driven Continuously running, triggered on preset thresholds ES + Local Storage, Prometheus (optional) Continuous operations, directly dump context Metrics Metric collection Passive collection Prometheus format Monitoring system metrics Autotracing Type: Event-driven (tracing). Function: Automatically tracks system anomalies and dump context when anomalies occur. Features: When a system anomaly occurs, autotracing is triggered automatically to dump relevant context. Data is stored to ES in real-time and stored locally for subsequent analysis and troubleshooting. It can also be monitored in Prometheus format for statistics and alerts. Suitable for scenarios with high performance overhead, such as triggering captures when metrics exceed a threshold or rise too quickly. Integrated Features: CPU anomaly tracking (cpu idle), D-state tracking (dload), container contention (waitrate), memory burst allocation (memburst), disk anomaly tracking (iotracer). Event Type: Event-driven (tracing). Function: Continuously operates within the system context, directly dump context when preset thresholds are met. Features: Unlike autotracing, event continuously operates within the system context, rather than being triggered by anomalies. Data is also stored to ES and locally, and can be monitored in Prometheus format. Suitable for continuous monitoring and real-time analysis, enabling timely detection of abnormal behaviors. The performance impact of event collection is negligible. Integrated Features: Soft interrupt anomalies (softirq), memory allocation anomalies (oom), soft lockups (softlockup), D-state processes (hungtask), memory reclamation (memreclaim), packet droped abnormal (dropwatch), network ingress latency (net_rx_latency). Metrics Type: Metric collection. Function: Collects performance metrics from subsystems. Features: Metric data can be sourced from regular procfs collection or derived from tracing (autotracing, event) data. Outputs in Prometheus format for easy integration into Prometheus monitoring systems. Unlike tracing data, metrics primarily focus on system performance metrics such as CPU usage, memory usage, and network traffic, etc. Suitable for monitoring system performance metrics, supporting real-time analysis and long-term trend observation. Integrated Features: CPU (sys, usr, util, load, nr_running, etc.), memory (vmstat, memory_stat, directreclaim, asyncreclaim, etc.), IO (d2c, q2c, freeze, flush, etc.), network (arp, socket mem, qdisc, netstat, netdev, sockstat, etc.). Multiple Purpose of Tracing Mode Both autotracing and event belong to the tracing collection mode, offering the following dual purposes:\nReal-time storage to ES and local storage: For tracing and analyzing anomalies, helping users quickly identify root causes. Output in Prometheus format: As metric data integrated into Prometheus monitoring systems, providing comprehensive system monitoring capabilities. By flexibly combining these three modes, users can comprehensively monitor system performance, capturing both contextual information during anomalies and continuous performance metrics to meet various monitoring needs.\n","categories":"","description":"","excerpt":"HuaTuo framework provides three data collection modes: autotracing, …","ref":"/docs/en/latest/development/mode/","tags":"","title":"Collection Framework"},{"body":"Run Only the Collector Start the Container docker run --privileged --cgroupns=host --network=host -v /sys:/sys -v /proc:/proc -v /run:/run huatuo/huatuo-bamai:latest ⚠️ This uses the default configuration file inside the container. The internal default configuration does not connect to Elasticsearch. For a complete setup, mount your own huatuo-bamai.conf using -v, and update the config according to your environment (kubelet access, Elasticsearch settings, local log storage path, etc.).\nDeploy All Components (Docker Compose) For local development and validation, using Docker Compose is the most convenient approach.\nYou can quickly launch a full environment containing the collector, Elasticsearch, Prometheus, Grafana, and other components.\ndocker compose --project-directory ./build/docker up It is recommended to install Docker Compose using the plugin method: https://docs.docker.com/compose/install/linux/\n","categories":"","description":"","excerpt":"Run Only the Collector Start the Container docker run --privileged …","ref":"/docs/en/latest/deployment/docker/","tags":"","title":"Docker"},{"body":"镜像下载 镜像存储地址: https://hub.docker.com/r/huatuo/huatuo-bamai/tags。\ndocker 启动容器 docker run --privileged --cgroupns=host --network=host -v /sys:/sys -v /proc:/proc -v /run:/run huatuo/huatuo-bamai:latest ⚠️：该方式使用容器内的默认配置文件，容器内的默认配置不会连接 kubelet 和 ES。\ndocker compose 启动容器 通过docker compose 方式，可以在本地快速搭建部署一套完整的环境自行管理采集器、ES、prometheus、grafana 等组件。\ndocker compose --project-directory ./build/docker up 安装docker compose 参考 https://docs.docker.com/compose/install/linux/\n","categories":"","description":"","excerpt":"镜像下载 镜像存储地址: https://hub.docker.com/r/huatuo/huatuo-bamai/tags。\ndocker …","ref":"/docs/zh/latest/deployment/docker/","tags":"","title":"Docker 容器部署"},{"body":" To help users quickly experience and deploy HUATUO, this document is divided into three sections: Quick Experience，Quick Start，Compilation \u0026 Deployment.\n1. Quick Experience This section helps you quickly explore the frontend capabilities. You can directly access demo station, such as viewing exception event overviews, exception event context information, metric curves, etc. (Account: huatuo passwd: huatuo1024).\nEvents, AutoTracing Dashboard（improvements in progress） Host Metrics Dashboard（improvements in progress） Container Metrics Dashboard（improvements in progress） 2. Quick Start 2.1 Quick Run If you want to understand the underlying principles and deploy HUATUO to your own monitoring system, you can start pre-compiled container images via Docker (Note: This method disables container information retrieval and ES storage functionality by default).\nDirect Execution：\n$ docker run --privileged --cgroupns=host --network=host -v /sys:/sys -v /proc:/proc -v /run:/run huatuo/huatuo-bamai:latest Metric Collection：In another terminal, collect metrics\n$ curl -s localhost:19704/metrics View Exception Events (Events, AutoTracing)：HUATUO stores collected kernel exception event information in ES (disabled by default) while retaining a copy in the local directory huatuo-local. Note: Typically, no files exist in this path (systems in normal state don’t trigger event collection). You can generate events by creating exception scenarios or modifying configuration thresholds.\n2.2 Quick Setup If you want to further understand HUATUO’s operational mechanisms, architecture design, monitoring dashboard, and custom deployment, you can quickly set up a complete local environment using docker compose.\n$ docker compose --project-directory ./build/docker up This command pulls the latest images and starts components including elasticsearch, prometheus, grafana，huatuo-bamai. After successful command execution, open your browser and visit http://localhost:3000 to access the monitoring dashboard (Grafana default admin account: admin, password: admin; Since your system is in normal state, the Events and AutoTracing dashboards typically won’t display data).\n3. Compilation \u0026 Deployment 3.1 Compilation To isolate the developer’s local environment and simplify the compilation process, we provide containerized compilation. You can directly use docker build to construct the completed image (including the underlying collector huatuo-bamai, BPF objects, tools, etc.). Run the following command in the project root directory:\n$ docker build --network host -t huatuo/huatuo-bamai:latest . 3.2 Execution Run container:\n$ docker run --privileged --cgroupns=host --network=host -v /sys:/sys -v /proc:/proc -v /run:/run huatuo/huatuo-bamai:latest Or copy all files from the container path /home/huatuo-bamai and run manually locally:\n$ ./huatuo-bamai --region example --config huatuo-bamai.conf Management: Can be managed using systemd/supervisord/k8s-DaemonSet, etc.\n3.3 Configuration Container Information Configuration\nHUATUO obtains POD/container information by calling the kubelet interface. Configure the access interface and certificates according to your actual environment. Empty configuration \"\" indicates disabling this functionality.\n[Pod] KubeletPodListURL = \"http://127.0.0.1:10255/pods\" KubeletPodListHTTPSURL = \"https://127.0.0.1:10250/pods\" KubeletPodClientCertPath = \"/var/lib/kubelet/pki/kubelet-client-current.pem\" Storage Configuration\nMetric Storage (Metric): All metrics are stored in Prometheus. You can access the :19704/metrics interface to obtain metrics.\nException Event Storage (Events, AutoTracing): All kernel events and AutoTracing events are stored in ES. Note: If the configuration is empty, ES storage is not activated, and events are only stored in the local directory huatuo-local.\nES storage configuration is as follows:\n[Storage.ES] Address = \"http://127.0.0.1:9200\" Username = \"elastic\" Password = \"huatuo-bamai\" Index = \"huatuo_bamai\" Local storage configuration is as follows:\n# tracer's record data # Path: all but the last element of path for per tracer # RotationSize: the maximum size in Megabytes of a record file before it gets rotated for per subsystem # MaxRotation: the maximum number of old log files to retain for per subsystem [Storage.LocalFile] Path = \"huatuo-local\" RotationSize = 100 MaxRotation = 10 Event Thresholds\nAll kernel event collections (Events and AutoTracing) can have configurable trigger thresholds. The default thresholds are empirical data repeatedly validated in actual production environments. You can modify thresholds in huatuo-bamai.conf according to your requirements.\nResource Limits\nTo ensure host machine stability, we have implemented resource limits for the collector. LimitInitCPU represents CPU resources occupied during collector startup, while LimitCPU/LimitMem represent resource limits for normal operation after successful startup:\n[RuntimeCgroup] LimitInitCPU = 0.5 LimitCPU = 2.0 # limit memory (MB) LimitMem = 2048 ","categories":"","description":"","excerpt":" To help users quickly experience and deploy HUATUO, this document is …","ref":"/docs/en/latest/quick-start/","tags":"","title":"Getting started"},{"body":"简介 HUATUO（华佗） 是由滴滴开源并依托 CCF （中国计算机学会） 孵化的操作系统深度可观测项目，专注为复杂云原生通用计算，AI 计算，裸金属基础服务等提供操作系统内核级深度观测能力。该项目核心成员为一群开源技术爱好者，基础技术研究者。\n内核版本 理论支持 4.18 之后的所有版本，主要测试内核、和操作系统发行版如下：\nHUATUO 内核版本 操作系统发行版 1.0 4.18.x CentOS 8.x 1.0 5.4.x OpenCloudOS V8/Ubuntu 20.04 1.0 5.10.x OpenEuler 22.03/Anolis OS 8.10 1.0 5.15.x Ubuntu 22.04 1.0 6.6.x OpenEuler 24.03/Anolis OS 23.3/OpenCloudOS V9 1.0 6.8.x Ubuntu 24.04 1.0 6.14.x Fedora 42 联系我们 微信群（备注姓名+单位）和公众号： ","categories":"","description":"","excerpt":"简介 HUATUO（华佗） 是由滴滴开源并依托 CCF （中国计算机学会） 孵化的操作系统深度可观测项目，专注为复杂云原生通用计算，AI 计 …","ref":"/docs/zh/latest/","tags":"","title":"latest"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/en/latest/","tags":"","title":"latest"},{"body":" Subsystem Metric Description Unit Dimension Source cpu cpu_util_sys Time of running kernel processes percentage of host % host Calculate base on cpuacct.stat and cpuacct.usage cpu cpu_util_usr Time of running user processes percentage of host % host Calculate base on cpuacct.stat and cpuacct.usage cpu cpu_util_total Total time of running percentage of host % host Calculate base on cpuacct.stat and cpuacct.usage cpu cpu_util_container_sys Time of running kernel processes percentage of container % container Calculate base on cpuacct.stat and cpuacct.usage cpu cpu_util_container_usr Time of running user processes percentage of container % container Calculate base on cpuacct.stat and cpuacct.usage cpu cpu_util_container_total Total time of running percentage of container % container Calculate base on cpuacct.stat and cpuacct.usage cpu cpu_stat_container_burst_time Cumulative wall-time (in nanoseconds) that any CPUs has used above quota in respective periods ns container cpu.stat cpu cpu_stat_container_nr_bursts Number of periods burst occurs count container cpu.stat cpu cpu_stat_container_nr_throttled Number of times the group has been throttled/limited count container cpu.stat cpu cpu_stat_container_exter_wait_rate Wait rate caused by processes outside the container % container Calculate base on throttled_time/hierarchy_wait_sum/inner_wait_sum read from cpu.stat cpu cpu_stat_container_inner_wait_rate Wait rate caused by processes inside the container % container Calculate base on throttled_time/hierarchy_wait_sum/inner_wait_sum read from cpu.stat cpu cpu_stat_container_throttle_wait_rate Wait rate caused by throttle of container % container Calculate base on throttled_time/hierarchy_wait_sum/inner_wait_sum read from cpu.stat cpu cpu_stat_container_wait_rate Total wait rate: exter_wait_rate + inner_wait_rate + throttle_wait_rate % container Calculate base on throttled_time/hierarchy_wait_sum/inner_wait_sum read from cpu.stat cpu loadavg_container_container_nr_running The number of running tasks in the container count container get from kernel via netlink cpu loadavg_container_container_nr_uninterruptible The number of uninterruptible tasks in the container count container get from kernel via netlink cpu loadavg_load1 System load avg over the last 1 minute count host proc fs cpu loadavg_load5 System load avg over the last 5 minute count host proc fs cpu loadavg_load15 system load avg over the last 15 minute count host proc fs cpu monsoftirq_latency The number of NET_RX/NET_TX irq latency happend in the following regions:\n0~10 us\n100us ~ 1ms\n10us ~ 100us\n1ms ~ inf count host hook the softirq event and do time statistics via bpf cpu runqlat_container_nlat_01 The number of times when schedule latency of processes in the container is within 0~10ms count container hook the scheduling switch event and do time statistics via bpf cpu runqlat_container_nlat_02 The number of times when schedule latency of processes in the container is within 10~20ms count container hook the scheduling switch event and do time statistics via bpf cpu runqlat_container_nlat_03 The number of times when schedule latency of processes in the container is within 20~50ms count container hook the scheduling switch event and do time statistics via bpf cpu runqlat_container_nlat_04 The number of times when schedule latency of processes in the container is more than 50ms count container hook the scheduling switch event and do time statistics via bpf cpu runqlat_g_nlat_01 The number of times when schedule latency of processes in the host is within\n0~10ms count host hook the scheduling switch event and do time statistics via bpf cpu runqlat_g_nlat_02 The number of times when schedule latency of processes in the host is within 10~20ms count host hook the scheduling switch event and do time statistics via bpf cpu runqlat_g_nlat_03 The number of times when schedule latency of processes in the host is within 20~50ms count host hook the scheduling switch event and do time statistics via bpf cpu runqlat_g_nlat_04 The number of times when schedule latency of processes in the host is more than 50ms count host hook the scheduling switch event and do time statistics via bpf cpu reschedipi_oversell_probability The possibility of cpu overselling exists on the host where the vm is located 0-1 host hook the scheduling ipi event and do time statistics via bpf memory buddyinfo_blocks Kernel memory allocator information pages host proc fs memory memory_events_container_watermark_inc Counts of memory allocation watermark increasing count container memory.events memory memory_events_container_watermark_dec Counts of memory allocation watermark decreasing count container memory.events memory memory_others_container_local_direct_reclaim_time Time speed in page allocation in memory cgroup nanosecond container memory.local_direct_reclaim_time memory memory_others_container_directstall_time Memory cgroup’s direct reclaim time in try_charge nanosecond container memory.directstall_stat memory memory_others_container_asyncreclaim_time Memory cgroup’s direct reclaim time in cgroup async memory reclaim nanosecond container memory.asynreclaim_stat memory priority_reclaim_kswapd Kswapd’s reclaim stat in priority reclaiming pages host proc fs memory priority_reclaim_direct Direct reclaim stat in priority reclaiming pages host proc fs memory memory_stat_container_writeback Bytes of file/anon cache that are queued for syncing to disk bytes container memory.stat memory memory_stat_container_unevictable Bytes of memory that cannot be reclaimed (mlocked etc) bytes container memory.stat memory memory_stat_container_shmem Bytes of shmem memory bytes container memory.stat memory memory_stat_container_pgsteal_kswapd Bytes of reclaimed memory by kswapd and cswapd bytes container memory.stat memory memory_stat_container_pgsteal_globalkswapd Bytes of reclaimed memory by kswapd bytes container memory.stat memory memory_stat_container_pgsteal_globaldirect Bytes of reclaimed memory by direct reclaim during page allocation bytes container memory.stat memory memory_stat_container_pgsteal_direct Bytes of reclaimed memory by direct reclaim during page allocation and try_charge bytes container memory.stat memory memory_stat_container_pgsteal_cswapd Bytes of reclaimed memory by cswapd bytes container memory.stat memory memory_stat_container_pgscan_kswapd Bytes of scanned memory by kswapd and cswapd bytes container memory.stat memory memory_stat_container_pgscan_globalkswapd Bytes of scanned memory by kswapd bytes container memory.stat memory memory_stat_container_pgscan_globaldirect Bytes of scanned memory by direct reclaim during page allocation bytes container memory.stat memory memory_stat_container_pgscan_direct Bytes of scanned memory by direct reclaim during page allocation and try_charge bytes container memory.stat memory memory_stat_container_pgscan_cswapd Bytes of scanned memory by cswapd bytes container memory.stat memory memory_stat_container_pgrefill Bytes of memory that is scanned in active list bytes container memory.stat memory memory_stat_container_pgdeactivate Bytes of memory that is deactivated into inactive list bytes container memory.stat memory memory_stat_container_inactive_file Bytes of file-backed memory on inactive lru list. bytes container memory.stat memory memory_stat_container_inactive_anon Bytes of anonymous and swap cache memory on inactive lru list bytes container memory.stat memory memory_stat_container_dirty Bytes that are waiting to get written back to the disk bytes container memory.stat memory memory_stat_container_active_file Bytes of file-backed memory on active lru list bytes container memory.stat memory memory_stat_container_active_anon Bytes of anonymous and swap cache memory on active lru list bytes container memory.stat memory mountpoint_perm_ro Whether mountpoint is readonly or not bool host proc fs memory vmstat_allocstall_normal Host direct reclaim count on normal zone count host /proc/vmstat memory vmstat_allocstall_movable Host direct reclaim count on movable zone count host /proc/vmstat memory vmstat_compact_stall Count of memory compaction count host /proc/vmstat memory vmstat_nr_active_anon Number of anonymous pages on active lru pages host /proc/vmstat memory vmstat_nr_active_file Number of file-backed pages on active lru pages host /proc/vmstat memory vmstat_nr_boost_pages Number of pages in kswapd boosting pages host /proc/vmstat memory vmstat_nr_dirty Number of dirty pages pages host /proc/vmstat memory vmstat_nr_free_pages Number of free pages pages host /proc/vmstat memory vmstat_nr_inactive_anon Number of anonymous pages on inactive lru pages host /proc/vmstat memory vmstat_nr_inactive_file Number of file-backed pages on inactive lru pages host /proc/vmstat memory vmstat_nr_kswapd_boost Count of kswapd boosting pages host /proc/vmstat memory vmstat_nr_mlock Number of locked pages pages host /proc/vmstat memory vmstat_nr_shmem Number of shmem pages pages host /proc/vmstat memory vmstat_nr_slab_reclaimable Number of relcaimable slab pages pages host /proc/vmstat memory vmstat_nr_slab_unreclaimable Number of unrelcaimable slab pages pages host /proc/vmstat memory vmstat_nr_unevictable Number of unevictable pages pages host /proc/vmstat memory vmstat_nr_writeback Number of writebacking pages pages host /proc/vmstat memory vmstat_numa_pages_migrated Number of pages in numa migrating pages host /proc/vmstat memory vmstat_pgdeactivate Number of pages which are deactivated into inactive lru pages host /proc/vmstat memory vmstat_pgrefill Number of pages which are scanned on active lru pages host /proc/vmstat memory vmstat_pgscan_direct Number of pages which are scanned in direct reclaim pages host /proc/vmstat memory vmstat_pgscan_kswapd Number of pages which are scanned in kswapd reclaim pages host /proc/vmstat memory vmstat_pgsteal_direct Number of pages which are reclaimed in direct reclaim pages host /proc/vmstat memory vmstat_pgsteal_kswapd Number of pages which are reclaimed in kswapd reclaim pages host /proc/vmstat memory hungtask_happened Count of hungtask events count host performance and statistics monitoring for BPF Programs memory oom_happened Count of oom events count host,container performance and statistics monitoring for BPF Programs memory softlockup_happened Count of softlockup events count host performance and statistics monitoring for BPF Programs memory mmhostbpf_compactionstat Time speed in memory compaction nanosecond host performance and statistics monitoring for BPF Programs memory mmhostbpf_allocstallstat Time speed in memory direct reclaim on host nanosecond host performance and statistics monitoring for BPF Programs memory mmcgroupbpf_container_directstallcount Count of cgroup’s try_charge direct reclaim count container performance and statistics monitoring for BPF Programs IO iolatency_disk_d2c Statistics of io latency when accessing the disk, including the time consumed by the driver and hardware components count host performance and statistics monitoring for BPF Programs IO iolatency_disk_q2c Statistics of io latency for the entire io lifecycle when accessing the disk count host performance and statistics monitoring for BPF Programs IO iolatency_container_d2c Statistics of io latency when accessing the disk, including the time consumed by the driver and hardware components count container performance and statistics monitoring for BPF Programs IO iolatency_container_q2c Statistics of io latency for the entire io lifecycle when accessing the disk count container performance and statistics monitoring for BPF Programs IO iolatency_disk_flush Statistics of delay for flush operations on disk raid device count host performance and statistics monitoring for BPF Programs IO iolatency_container_flush Statistics of delay for flush operations on disk raid devices caused by containers count container performance and statistics monitoring for BPF Programs IO iolatency_disk_freeze Statistics of disk freeze events count host performance and statistics monitoring for BPF Programs network tcp_mem_limit_pages System TCP total memory size limit pages system proc fs network tcp_mem_usage_bytes The total number of bytes of TCP memory used by the system bytes system tcp_mem_usage_pages * page_size network tcp_mem_usage_pages The total size of TCP memory used by the system pages system proc fs network tcp_mem_usage_percent The percentage of TCP memory used by the system to the limit size % system tcp_mem_usage_pages / tcp_mem_limit_pages network arp_entries The number of arp cache entries count host,container proc fs network arp_total Total number of arp cache entries count system proc fs network qdisc_backlog The number of bytes queued to be sent bytes host sum of same level(parent major) for a device network qdisc_bytes_total The number of bytes sent bytes host sum of same level(parent major) for a device network qdisc_current_queue_length The number of packets queued for sending count host sum of same level(parent major) for a device network qdisc_drops_total The number of discarded packets count host sum of same level(parent major) for a device network qdisc_overlimits_total The number of queued packets exceeds the limit count host sum of same level(parent major) for a device network qdisc_packets_total The number of packets sent count host sum of same level(parent major) for a device network qdisc_requeues_total The number of packets that were not sent successfully and were requeued count host sum of same level(parent major) for a device network ethtool_hardware_rx_dropped_errors Statistics of inbound packet droped or errors of interface count host related to hardware drivers, such as mlx, ixgbe, bnxt_en, etc. network netdev_receive_bytes_total Number of good received bytes bytes host,container proc fs network netdev_receive_compressed_total Number of correctly received compressed packets count host,container proc fs network netdev_receive_dropped_total Number of packets received but not processed count host,container proc fs network netdev_receive_errors_total Total number of bad packets received on this network device count host,container proc fs network netdev_receive_fifo_total Receiver FIFO error counter count host,container proc fs network netdev_receive_frame_total Receiver frame alignment errors count host,container proc fs network netdev_receive_multicast_total Multicast packets received. For hardware interfaces this statistic is commonly calculated at the device level (unlike rx_packets) and therefore may include packets which did not reach the host count host,container proc fs network netdev_receive_packets_total Number of good packets received by the interface count host,container proc fs network netdev_transmit_bytes_total Number of good transmitted bytes, corresponding to tx_packets bytes host,container proc fs network netdev_transmit_carrier_total Number of frame transmission errors due to loss of carrier during transmission count host,container proc fs network netdev_transmit_colls_total Number of collisions during packet transmissions count host,container proc fs network netdev_transmit_compressed_total Number of transmitted compressed packets count host,container proc fs network netdev_transmit_dropped_total Number of packets dropped on their way to transmission, e.g. due to lack of resources count host,container proc fs network netdev_transmit_errors_total Total number of transmit problems count host,container proc fs network netdev_transmit_fifo_total Number of frame transmission errors due to device FIFO underrun / underflow count host,container proc fs network netdev_transmit_packets_total Number of packets successfully transmitted count host,container proc fs network netstat_TcpExt_ArpFilter - count host,container proc fs network netstat_TcpExt_BusyPollRxPackets - count host,container proc fs network netstat_TcpExt_DelayedACKLocked A delayed ACK timer expires, but the TCP stack can’t send an ACK immediately due to the socket is locked by a userspace program. The TCP stack will send a pure ACK later (after the userspace program unlock the socket). When the TCP stack sends the pure ACK later, the TCP stack will also update TcpExtDelayedACKs and exit the delayed ACK mode count host,container proc fs network netstat_TcpExt_DelayedACKLost It will be updated when the TCP stack receives a packet which has been ACKed. A Delayed ACK loss might cause this issue, but it would also be triggered by other reasons, such as a packet is duplicated in the network count host,container proc fs network netstat_TcpExt_DelayedACKs A delayed ACK timer expires. The TCP stack will send a pure ACK packet and exit the delayed ACK mode count host,container proc fs network netstat_TcpExt_EmbryonicRsts resets received for embryonic SYN_RECV sockets count host,container proc fs network netstat_TcpExt_IPReversePathFilter - count host,container proc fs network netstat_TcpExt_ListenDrops When kernel receives a SYN from a client, and if the TCP accept queue is full, kernel will drop the SYN and add 1 to TcpExtListenOverflows. At the same time kernel will also add 1 to TcpExtListenDrops. When a TCP socket is in LISTEN state, and kernel need to drop a packet, kernel would always add 1 to TcpExtListenDrops. So increase TcpExtListenOverflows would let TcpExtListenDrops increasing at the same time, but TcpExtListenDrops would also increase without TcpExtListenOverflows increasing, e.g. a memory allocation fail would also let TcpExtListenDrops increase count host,container proc fs network netstat_TcpExt_ListenOverflows When kernel receives a SYN from a client, and if the TCP accept queue is full, kernel will drop the SYN and add 1 to TcpExtListenOverflows. At the same time kernel will also add 1 to TcpExtListenDrops. When a TCP socket is in LISTEN state, and kernel need to drop a packet, kernel would always add 1 to TcpExtListenDrops. So increase TcpExtListenOverflows would let TcpExtListenDrops increasing at the same time, but TcpExtListenDrops would also increase without TcpExtListenOverflows increasing, e.g. a memory allocation fail would also let TcpExtListenDrops increase count host,container proc fs network netstat_TcpExt_LockDroppedIcmps ICMP packets dropped because socket was locked count host,container proc fs network netstat_TcpExt_OfoPruned The TCP stack tries to discard packet on the out of order queue count host,container proc fs network netstat_TcpExt_OutOfWindowIcmps ICMP pkts dropped because they were out-of-window count host,container proc fs network netstat_TcpExt_PAWSActive Packets are dropped by PAWS in Syn-Sent status count host,container proc fs network netstat_TcpExt_PAWSEstab Packets are dropped by PAWS in any status other than Syn-Sent count host,container proc fs network netstat_TcpExt_PFMemallocDrop - count host,container proc fs network netstat_TcpExt_PruneCalled The TCP stack tries to reclaim memory for a socket. After updates this counter, the TCP stack will try to collapse the out of order queue and the receiving queue. If the memory is still not enough, the TCP stack will try to discard packets from the out of order queue (and update the TcpExtOfoPruned counter) count host,container proc fs network netstat_TcpExt_RcvPruned After ‘collapse’ and discard packets from the out of order queue, if the actually used memory is still larger than the max allowed memory, this counter will be updated. It means the ‘prune’ fails count host,container proc fs network netstat_TcpExt_SyncookiesFailed The MSS decoded from the SYN cookie is invalid. When this counter is updated, the received packet won’t be treated as a SYN cookie and the TcpExtSyncookiesRecv counter won’t be updated count host,container proc fs network netstat_TcpExt_SyncookiesRecv How many reply packets of the SYN cookies the TCP stack receives count host,container proc fs network netstat_TcpExt_SyncookiesSent It indicates how many SYN cookies are sent count host,container proc fs network netstat_TcpExt_TCPACKSkippedChallenge The ACK is skipped if the ACK is a challenge ACK count host,container proc fs network netstat_TcpExt_TCPACKSkippedFinWait2 The ACK is skipped in Fin-Wait-2 status, the reason would be either PAWS check fails or the received sequence number is out of window count host,container proc fs network netstat_TcpExt_TCPACKSkippedPAWS The ACK is skipped due to PAWS (Protect Against Wrapped Sequence numbers) check fails count host,container proc fs network netstat_TcpExt_TCPACKSkippedSeq The sequence number is out of window and the timestamp passes the PAWS check and the TCP status is not Syn-Recv, Fin-Wait-2, and Time-Wait count host,container proc fs network netstat_TcpExt_TCPACKSkippedSynRecv The ACK is skipped in Syn-Recv status. The Syn-Recv status means the TCP stack receives a SYN and replies SYN+ACK count host,container proc fs network netstat_TcpExt_TCPACKSkippedTimeWait The ACK is skipped in Time-Wait status, the reason would be either PAWS check failed or the received sequence number is out of window count host,container proc fs network netstat_TcpExt_TCPAbortFailed The kernel TCP layer will send RST if the RFC2525 2.17 section is satisfied. If an internal error occurs during this process, TcpExtTCPAbortFailed will be increased count host,container proc fs network netstat_TcpExt_TCPAbortOnClose Number of sockets closed when the user-mode program has data in the buffer count host,container proc fs network netstat_TcpExt_TCPAbortOnData It means TCP layer has data in flight, but need to close the connection count host,container proc fs network netstat_TcpExt_TCPAbortOnLinger When a TCP connection comes into FIN_WAIT_2 state, instead of waiting for the fin packet from the other side, kernel could send a RST and delete the socket immediately count host,container proc fs network netstat_TcpExt_TCPAbortOnMemory When an application closes a TCP connection, kernel still need to track the connection, let it complete the TCP disconnect process count host,container proc fs network netstat_TcpExt_TCPAbortOnTimeout This counter will increase when any of the TCP timers expire. In such situation, kernel won’t send RST, just give up the connection count host,container proc fs network netstat_TcpExt_TCPAckCompressed - count host,container proc fs network netstat_TcpExt_TCPAutoCorking When sending packets, the TCP layer will try to merge small packets to a bigger one count host,container proc fs network netstat_TcpExt_TCPBacklogDrop - count host,container proc fs network netstat_TcpExt_TCPChallengeACK The number of challenge acks sent count host,container proc fs network netstat_TcpExt_TCPDSACKIgnoredNoUndo When a DSACK block is invalid, one of these two counters would be updated. Which counter will be updated depends on the undo_marker flag of the TCP socket count host,container proc fs network netstat_TcpExt_TCPDSACKIgnoredOld When a DSACK block is invalid, one of these two counters would be updated. Which counter will be updated depends on the undo_marker flag of the TCP socket count host,container proc fs network netstat_TcpExt_TCPDSACKOfoRecv The TCP stack receives a DSACK, which indicate an out of order duplicate packet is received count host,container proc fs network netstat_TcpExt_TCPDSACKOfoSent The TCP stack receives an out of order duplicate packet, so it sends a DSACK to the sender count host,container proc fs network netstat_TcpExt_TCPDSACKOldSent The TCP stack receives a duplicate packet which has been acked, so it sends a DSACK to the sender count host,container proc fs network netstat_TcpExt_TCPDSACKRecv The TCP stack receives a DSACK, which indicates an acknowledged duplicate packet is received count host,container proc fs network netstat_TcpExt_TCPDSACKUndo Congestion window recovered without slow start using DSACK count host,container proc fs network netstat_TcpExt_TCPDeferAcceptDrop - count host,container proc fs network netstat_TcpExt_TCPDelivered - count host,container proc fs network netstat_TcpExt_TCPDeliveredCE - count host,container proc fs network netstat_TcpExt_TCPFastOpenActive When the TCP stack receives an ACK packet in the SYN-SENT status, and the ACK packet acknowledges the data in the SYN packet, the TCP stack understand the TFO cookie is accepted by the other side, then it updates this counter count host,container proc fs network netstat_TcpExt_TCPFastOpenActiveFail Fast Open attempts (SYN/data) failed because the remote does not accept it or the attempts timed out count host,container proc fs network netstat_TcpExt_TCPFastOpenBlackhole - count host,container proc fs network netstat_TcpExt_TCPFastOpenCookieReqd This counter indicates how many times a client wants to request a TFO cookie count host,container proc fs network netstat_TcpExt_TCPFastOpenListenOverflow When the pending fast open request number is larger than fastopenq-\u003emax_qlen, the TCP stack will reject the fast open request and update this counter count host,container proc fs network netstat_TcpExt_TCPFastOpenPassive This counter indicates how many times the TCP stack accepts the fast open request count host,container proc fs network netstat_TcpExt_TCPFastOpenPassiveFail This counter indicates how many times the TCP stack rejects the fast open request. It is caused by either the TFO cookie is invalid or the TCP stack finds an error during the socket creating process count host,container proc fs network netstat_TcpExt_TCPFastRetrans The TCP stack wants to retransmit a packet and the congestion control state is not ‘Loss’ count host,container proc fs network netstat_TcpExt_TCPFromZeroWindowAdv The TCP receive window is set to no-zero value from zero count host,container proc fs network netstat_TcpExt_TCPFullUndo - count host,container proc fs network netstat_TcpExt_TCPHPAcks If a packet set ACK flag and has no data, it is a pure ACK packet, if kernel handles it in the fast path, TcpExtTCPHPAcks will increase 1 count host,container proc fs network netstat_TcpExt_TCPHPHits If a TCP packet has data (which means it is not a pure ACK packet), and this packet is handled in the fast path, TcpExtTCPHPHits will increase 1 count host,container proc fs network netstat_TcpExt_TCPHystartDelayCwnd The sum of CWND detected by packet delay. Dividing this value by TcpExtTCPHystartDelayDetect is the average CWND which detected by the packet delay count host,container proc fs network netstat_TcpExt_TCPHystartDelayDetect How many times the packet delay threshold is detected count host,container proc fs network netstat_TcpExt_TCPHystartTrainCwnd The sum of CWND detected by ACK train length. Dividing this value by TcpExtTCPHystartTrainDetect is the average CWND which detected by the ACK train length count host,container proc fs network netstat_TcpExt_TCPHystartTrainDetect How many times the ACK train length threshold is detected count host,container proc fs network netstat_TcpExt_TCPKeepAlive This counter indicates many keepalive packets were sent. The keepalive won’t be enabled by default. A userspace program could enable it by setting the SO_KEEPALIVE socket option count host,container proc fs network netstat_TcpExt_TCPLossFailures Number of connections that enter the TCP_CA_Loss phase and then undergo RTO timeout count host,container proc fs network netstat_TcpExt_TCPLossProbeRecovery A packet loss is detected and recovered by TLP count host,container proc fs network netstat_TcpExt_TCPLossProbes A TLP probe packet is sent count host,container proc fs network netstat_TcpExt_TCPLossUndo - count host,container proc fs network netstat_TcpExt_TCPLostRetransmit A SACK points out that a retransmission packet is lost again count host,container proc fs network netstat_TcpExt_TCPMD5Failure - count host,container proc fs network netstat_TcpExt_TCPMD5NotFound - count host,container proc fs network netstat_TcpExt_TCPMD5Unexpected - count host,container proc fs network netstat_TcpExt_TCPMTUPFail - count host,container proc fs network netstat_TcpExt_TCPMTUPSuccess - count host,container proc fs network netstat_TcpExt_TCPMemoryPressures Number of times TCP ran low on memory count host,container proc fs network netstat_TcpExt_TCPMemoryPressuresChrono - count host,container proc fs network netstat_TcpExt_TCPMinTTLDrop - count host,container proc fs network netstat_TcpExt_TCPOFODrop The TCP layer receives an out of order packet but doesn’t have enough memory, so drops it. Such packets won’t be counted into TcpExtTCPOFOQueue count host,container proc fs network netstat_TcpExt_TCPOFOMerge The received out of order packet has an overlay with the previous packet. the overlay part will be dropped. All of TcpExtTCPOFOMerge packets will also be counted into TcpExtTCPOFOQueue count host,container proc fs network netstat_TcpExt_TCPOFOQueue The TCP layer receives an out of order packet and has enough memory to queue it count host,container proc fs network netstat_TcpExt_TCPOrigDataSent Number of outgoing packets with original data (excluding retransmission but including data-in-SYN). This counter is different from TcpOutSegs because TcpOutSegs also tracks pure ACKs. TCPOrigDataSent is more useful to track the TCP retransmission rate count host,container proc fs network netstat_TcpExt_TCPPartialUndo Detected some erroneous retransmits, a partial ACK arrived while were fast retransmitting, so able to partially undo some of our CWND reduction count host,container proc fs network netstat_TcpExt_TCPPureAcks If a packet set ACK flag and has no data, it is a pure ACK packet, if kernel handles it in the fast path, TcpExtTCPHPAcks will increase 1, if kernel handles it in the slow path, TcpExtTCPPureAcks will increase 1 count host,container proc fs network netstat_TcpExt_TCPRcvCoalesce When packets are received by the TCP layer and are not be read by the application, the TCP layer will try to merge them. This counter indicate how many packets are merged in such situation. If GRO is enabled, lots of packets would be merged by GRO, these packets wouldn’t be counted to TcpExtTCPRcvCoalesce count host,container proc fs network netstat_TcpExt_TCPRcvCollapsed This counter indicates how many skbs are freed during ‘collapse’ count host,container proc fs network netstat_TcpExt_TCPRenoFailures Number of failures that enter the TCP_CA_Disorder phase and then undergo RTO count host,container proc fs network netstat_TcpExt_TCPRenoRecovery When the congestion control comes into Recovery state, if sack is used, TcpExtTCPSackRecovery increases 1, if sack is not used, TcpExtTCPRenoRecovery increases 1. These two counters mean the TCP stack begins to retransmit the lost packets count host,container proc fs network netstat_TcpExt_TCPRenoRecoveryFail Number of connections that enter the Recovery phase and then undergo RTO count host,container proc fs network netstat_TcpExt_TCPRenoReorder The reorder packet is detected by fast recovery. It would only be used if SACK is disabled count host,container proc fs network netstat_TcpExt_TCPReqQFullDoCookies - count host,container proc fs network netstat_TcpExt_TCPReqQFullDrop - count host,container proc fs network netstat_TcpExt_TCPRetransFail The TCP stack tries to deliver a retransmission packet to lower layers but the lower layers return an error count host,container proc fs network netstat_TcpExt_TCPSACKDiscard This counter indicates how many SACK blocks are invalid. If the invalid SACK block is caused by ACK recording, the TCP stack will only ignore it and won’t update this counter count host,container proc fs network netstat_TcpExt_TCPSACKReneging A packet was acknowledged by SACK, but the receiver has dropped this packet, so the sender needs to retransmit this packet count host,container proc fs network netstat_TcpExt_TCPSACKReorder The reorder packet detected by SACK count host,container proc fs network netstat_TcpExt_TCPSYNChallenge The number of challenge acks sent in response to SYN packets count host,container proc fs network netstat_TcpExt_TCPSackFailures Number of failures that enter the TCP_CA_Disorder phase and then undergo RTO count host,container proc fs network netstat_TcpExt_TCPSackMerged A skb is merged count host,container proc fs network netstat_TcpExt_TCPSackRecovery When the congestion control comes into Recovery state, if sack is used, TcpExtTCPSackRecovery increases 1, if sack is not used, TcpExtTCPRenoRecovery increases 1. These two counters mean the TCP stack begins to retransmit the lost packets count host,container proc fs network netstat_TcpExt_TCPSackRecoveryFail When the congestion control comes into Recovery state, if sack is used, TcpExtTCPSackRecovery increases 1 count host,container proc fs network netstat_TcpExt_TCPSackShiftFallback A skb should be shifted or merged, but the TCP stack doesn’t do it for some reasons count host,container proc fs network netstat_TcpExt_TCPSackShifted A skb is shifted count host,container proc fs network netstat_TcpExt_TCPSlowStartRetrans The TCP stack wants to retransmit a packet and the congestion control state is ‘Loss’ count host,container proc fs network netstat_TcpExt_TCPSpuriousRTOs The spurious retransmission timeout detected by the F-RTO algorithm count host,container proc fs network netstat_TcpExt_TCPSpuriousRtxHostQueues When the TCP stack wants to retransmit a packet, and finds that packet is not lost in the network, but the packet is not sent yet, the TCP stack would give up the retransmission and update this counter. It might happen if a packet stays too long time in a qdisc or driver queue count host,container proc fs network netstat_TcpExt_TCPSynRetrans Number of SYN and SYN/ACK retransmits to break down retransmissions into SYN, fast-retransmits, timeout retransmits, etc count host,container proc fs network netstat_TcpExt_TCPTSReorder The reorder packet is detected when a hole is filled count host,container proc fs network netstat_TcpExt_TCPTimeWaitOverflow Number of TIME_WAIT sockets unable to be allocated due to limit exceeding count host,container proc fs network netstat_TcpExt_TCPTimeouts TCP timeout events count host,container proc fs network netstat_TcpExt_TCPToZeroWindowAdv The TCP receive window is set to zero from a no-zero value count host,container proc fs network netstat_TcpExt_TCPWantZeroWindowAdv Depending on current memory usage, the TCP stack tries to set receive window to zero. But the receive window might still be a no-zero value count host,container proc fs network netstat_TcpExt_TCPWinProbe Number of ACK packets to be sent at regular intervals to make sure a reverse ACK packet opening back a window has not been lost count host,container proc fs network netstat_TcpExt_TCPWqueueTooBig - count host,container proc fs network netstat_TcpExt_TW TCP sockets finished time wait in fast timer count host,container proc fs network netstat_TcpExt_TWKilled TCP sockets finished time wait in slow timer count host,container proc fs network netstat_TcpExt_TWRecycled Time wait sockets recycled by time stamp count host,container proc fs network netstat_Tcp_ActiveOpens It means the TCP layer sends a SYN, and come into the SYN-SENT state. Every time TcpActiveOpens increases 1, TcpOutSegs should always increase 1 count host,container proc fs network netstat_Tcp_AttemptFails The number of times TCP connections have made a direct transition to the CLOSED state from either the SYN-SENT state or the SYN-RCVD state, plus the number of times TCP connections have made a direct transition to the LISTEN state from the SYN-RCVD state count host,container proc fs network netstat_Tcp_CurrEstab The number of TCP connections for which the current state is either ESTABLISHED or CLOSE-WAIT count host,container proc fs network netstat_Tcp_EstabResets The number of times TCP connections have made a direct transition to the CLOSED state from either the ESTABLISHED state or the CLOSE-WAIT state count host,container proc fs network netstat_Tcp_InCsumErrors Incremented when a TCP checksum failure is detected count host,container proc fs network netstat_Tcp_InErrs The total number of segments received in error (e.g., bad TCP checksums) count host,container proc fs network netstat_Tcp_InSegs The number of packets received by the TCP layer. As mentioned in RFC1213, it includes the packets received in error, such as checksum error, invalid TCP header and so on count host,container proc fs network netstat_Tcp_MaxConn The limit on the total number of TCP connections the entity can support. In entities where the maximum number of connections is dynamic, this object should contain the value -1 count host,container proc fs network netstat_Tcp_OutRsts The number of TCP segments sent containing the RST flag count host,container proc fs network netstat_Tcp_OutSegs The total number of segments sent, including those on current connections but excluding those containing only retransmitted octets count host,container proc fs network netstat_Tcp_PassiveOpens The number of times TCP connections have made a direct transition to the SYN-RCVD state from the LISTEN state count host,container proc fs network netstat_Tcp_RetransSegs The total number of segments retransmitted - that is, the number of TCP segments transmitted containing one or more previously transmitted octets count host,container proc fs network netstat_Tcp_RtoAlgorithm The algorithm used to determine the timeout value used for retransmitting unacknowledged octets count host,container proc fs network netstat_Tcp_RtoMax The maximum value permitted by a TCP implementation for the retransmission timeout, measured in milliseconds. More refined semantics for objects of this type depend upon the algorithm used to determine the retransmission timeout count host,container proc fs network netstat_Tcp_RtoMin The minimum value permitted by a TCP implementation for the retransmission timeout, measured in milliseconds. More refined semantics for objects of this type depend upon the algorithm used to determine the retransmission timeout count host,container proc fs network sockstat_FRAG_inuse - count host,container proc fs network sockstat_FRAG_memory - pages host,container proc fs network sockstat_RAW_inuse Number of RAW socket used count host,container proc fs network sockstat_TCP_alloc The number of TCP sockets that have been allocated count host,container proc fs network sockstat_TCP_inuse Established TCP socket number count host,container proc fs network sockstat_TCP_mem The total size of TCP memory used by the system pages system proc fs network sockstat_TCP_mem_bytes The total size of TCP memory used by the system bytes system sockstat_TCP_mem * page_size network sockstat_TCP_orphan Number of TCP connections waiting to be closed count host,container proc fs network sockstat_TCP_tw Number of TCP sockets to be terminated count host,container proc fs network sockstat_UDPLITE_inuse - count host,container proc fs network sockstat_UDP_inuse Number of UDP socket used count host,container proc fs network sockstat_UDP_mem The total size of udp memory used by the system pages system proc fs network sockstat_UDP_mem_bytes The total number of bytes of udp memory used by the system bytes system sockstat_UDP_mem * page_size network sockstat_sockets_used The number of sockets used by the system count system proc fs ","categories":"","description":"","excerpt":" Subsystem Metric Description Unit Dimension Source cpu cpu_util_sys …","ref":"/docs/en/latest/key-feature/metrics/","tags":"","title":"Metrics"},{"body":"为帮助用户全面深入洞察系统的运行状态，HUATUO 提供三种数据采集: metrics, event, autotracing. 用户可以根据具体场景和需求实现自己的观测数据采集。\n模式 模式 类型 触发条件 数据存储 适用场景 Metrics 指标数据 Pull 采集 Prometheus 系统性能指标 Event 异常事件 内核事件触发 ES + 本地存储，Prometheus（可选） 常态运行，事件触发，获取内核运行上下文 Autotracing 系统异常 系统异常触发 ES + 本地存储，Prometheus（可选） 系统异常触发，获取例如火焰图数据 指标 类型：指标采集。 功能：采集内核各子系统指标数据。 特点： 通过 Procfs 或 eBPF 方式采集。 Prometheus 格式输出，最终集成到 Prometheus/Grafana。 主要采集系统的基础指标，如 CPU 使用率、内存使用率、网络等。 适合用于监控系统运行状态，支持实时分析和长期趋势观察。 已集成： CPU sys, usr, util, load, nr_running … Memory vmstat, memory_stat, directreclaim, asyncreclaim … IO d2c, q2c, freeze, flush … Networking arp, socket mem, qdisc, netstat, netdev, socketstat … 事件 类型：Linux 内核事件采集。 功能：常态运行，事件触发并在达到预设阈值时，获取内核运行上下文。 特点： 常态运行，异常事件触发，支持阈值设定。 数据实时存储 ElasticSearch、物理机本地文件。 适合用于常态监控和实时分析，捕获系统更多异常行为观测数据。 已集成： 软中断异常 softirq 内存异常分配 oom 软锁定 softlockup D 状态进程 hungtask 内存回收 memreclaim 异常丢包 dropwatch 网络入向延迟 net_rx_latency 自动追踪 类型：系统异常追踪 功能：自动跟踪系统异常状态，并在异常发生时触发工具抓取现场信息。 特点： 系统出现异常时自动触发，捕获。 数据实时存储 ElasticSearch、物理机本地文件。 适用于获取现场时性能开销较大、指标突发的场景。 已集成： CPU 异常追踪 进程 D 状态追踪 容器内外争抢 内存突发分配 磁盘异常追踪 ","categories":"","description":"","excerpt":"为帮助用户全面深入洞察系统的运行状态，HUATUO 提供三种数据采集: metrics, event, autotracing. 用户可以根 …","ref":"/docs/zh/latest/development/mode/","tags":"","title":"采集模式"},{"body":"当前版本支持的指标:\n子系统 指标 描述 单位 统计纬度 指标来源 cpu cpu_util_sys cpu 系统态利用率 % 宿主 基于 cgroup cpuacct.stat 和 cpuacct.usage 计算 cpu cpu_util_usr cpu 用户态利用率 % 宿主 基于 cgroup cpuacct.stat 和 cpuacct.usage 计算 cpu cpu_util_total 容器 cpu 总利用率 % 宿主 基于 cgroup cpuacct.stat 和 cpuacct.usage 计算 cpu cpu_util_container_sys 容器 cpu 系统态利用率 % 容器 基于 cgroup cpuacct.stat 和 cpuacct.usage 计算 cpu cpu_util_container_usr 容器 cpu 用户态利用率 % 容器 基于 cgroup cpuacct.stat 和 cpuacct.usage 计算 cpu cpu_util_container_total 容器 cpu 总利用率 % 容器 基于 cgroup cpuacct.stat 和 cpuacct.usage 计算 cpu cpu_stat_container_burst_time 累计墙时（以纳秒为单位），周期内突发超出配额的时间 纳秒(ns) 容器 基于 cpu.stat 读取 cpu cpu_stat_container_nr_bursts 周期内突发次数 计数 容器 基于 cpu.stat 读取 cpu cpu_stat_container_nr_throttled cgroup 被 throttled/limited 的次数 计数 容器 基于 cpu.stat 读取 cpu cpu_stat_container_exter_wait_rate 容器外进程导致的等待率 % 容器 基于 cpu.stat 读取的 throttled_time hierarchy_wait_sum inner_wait_sum 计算 cpu cpu_stat_container_inner_wait_rate 容器内部进程导致的等待率 % 容器 基于 cpu.stat 读取的 throttled_time hierarchy_wait_sum inner_wait_sum 计算 cpu cpu_stat_container_throttle_wait_rate 容器被限制而引起的等待率 % 容器 基于 cpu.stat 读取的 throttled_time hierarchy_wait_sum inner_wait_sum 计算 cpu cpu_stat_container_wait_rate 总的等待率: exter_wait_rate + inner_wait_rate + throttle_wait_rate % 容器 基于 cpu.stat 读取的 throttled_time hierarchy_wait_sum inner_wait_sum 计算 cpu loadavg_container_container_nr_running 容器中运行的任务数量 计数 容器 从内核通过 netlink 获取 cpu loadavg_container_container_nr_uninterruptible 容器中不可中断任务的数量 计数 容器 从内核通过 netlink 获取 cpu loadavg_load1 系统过去 1 分钟的平均负载 计数 宿主 procfs cpu loadavg_load5 系统过去 5 分钟的平均负载 计数 宿主 procfs cpu loadavg_load15 系统过去 15 分钟的平均负载 计数 宿主 procfs cpu softirq_latency 在不同时间域发生的 NET_RX/NET_TX 中断延迟次数：\n0~10 us\n100us ~ 1ms\n10us ~ 100us\n1ms ~ inf 计数 宿主 BPF 软中断埋点统计 cpu runqlat_container_nlat_01 容器中进程调度延迟在 0~10 毫秒内的次数 计数 容器 bpf 调度切换埋点统计 cpu runqlat_container_nlat_02 容器中进程调度延迟在 10~20 毫秒之间的次数 计数 容器 bpf 调度切换埋点统计 cpu runqlat_container_nlat_03 容器中进程调度延迟在 20~50 毫秒之间的次数 计数 容器 bpf 调度切换埋点统计 cpu runqlat_container_nlat_04 容器中进程调度延迟超过 50 毫秒的次数 计数 容器 bpf 调度切换埋点统计 cpu runqlat_g_nlat_01 宿主中进程调度延迟在范围内 0～10 毫秒的次数 计数 宿主 bpf 调度切换埋点统计 cpu runqlat_g_nlat_02 宿主中进程调度延迟在范围内 10～20 毫秒的次数 计数 宿主 bpf 调度切换埋点统计 cpu runqlat_g_nlat_03 宿主中进程调度延迟在范围内 20～50 毫秒的次数 计数 宿主 bpf 调度切换埋点统计 cpu runqlat_g_nlat_04 宿主中进程调度延迟超过 50 毫秒的次数 计数 宿主 bpf 调度切换埋点统计 cpu reschedipi_oversell_probability vm 中 cpu 超卖检测 0-1 宿主 bpf 调度 ipi 埋点统计 memory buddyinfo_blocks 内核伙伴系统内存分配 页计数 宿主 procfs memory memory_events_container_watermark_inc 内存水位计数 计数 容器 memory.events memory memory_events_container_watermark_dec 内存水位计数 计数 容器 memory.events memory memory_others_container_local_direct_reclaim_time cgroup 中页分配速度 纳秒(ns) 容器 memory.local_direct_reclaim_time memory memory_others_container_directstall_time 直接回收时间 纳秒(ns) 容器 memory.directstall_stat memory memory_others_container_asyncreclaim_time 异步回收时间 纳秒(ns) 容器 memory.asynreclaim_stat memory memory_stat_container_writeback 匿名/文件 cache sync 到磁盘排队字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_unevictable 无法回收的内存（如 mlocked） 字节(Bytes) 容器 memory.stat memory memory_stat_container_shmem 共享内存字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgsteal_kswapd kswapd 和 cswapd 回收的内存字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgsteal_globalkswapd 由 kswapd 回收的字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgsteal_globaldirect 过页面分配直接回收的内存字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgsteal_direct 页分配和 try_charge 期间直接回收的内存字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgsteal_cswapd 由 cswapd 回收的字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgscan_kswapd kswapd 和 cswapd 扫描的内存字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgscan_globalkswapd kswapd 扫描的内存字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgscan_globaldirect 扫描内存中通过直接回收在页面分配期间的字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgscan_direct 扫描内存的字节数，在页面分配和 try_charge 期间通过直接回收的字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgscan_cswapd 由 cswapd 扫描内存的字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgrefill 内存中扫描的字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgdeactivate 内存中未激活的部分被添加到非活动列表中 字节(Bytes) 容器 memory.stat memory memory_stat_container_inactive_file 文件内存中不活跃的 LRU 列表的字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_inactive_anon 匿名和交换缓存内存中不活跃的 LRU 列表的字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_dirty 等待写入磁盘的字节 字节(Bytes) 容器 memory.stat memory memory_stat_container_active_file 活跃内存中文件内存的大小 字节(Bytes) 容器 memory.stat memory memory_stat_container_active_anon 活跃内存中匿名和交换内存的大小 字节(Bytes) 容器 memory.stat memory mountpoint_perm_ro 挂在点是否为只读 布尔(bool) 宿主 procfs memory vmstat_allocstall_normal 宿主在 normal 域直接回收 计数 宿主 /proc/vmstat memory vmstat_allocstall_movable 宿主在 movable 域直接回收 计数 宿主 /proc/vmstat memory vmstat_compact_stall 内存压缩计数 计数 宿主 /proc/vmstat memory vmstat_nr_active_anon 活跃的匿名页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_active_file 活跃的文件页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_boost_pages kswapd boosting 页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_dirty 脏页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_free_pages 释放的页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_inactive_anon 非活跃的匿名页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_inactive_file 非活跃的文件页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_kswapd_boost kswapd boosting 次数计数 页计数 宿主 /proc/vmstat memory vmstat_nr_mlock 锁定的页面数量 页计数 宿主 /proc/vmstat memory vmstat_nr_shmem 共享内存页面数 页计数 宿主 /proc/vmstat memory vmstat_nr_slab_reclaimable 可回收的 slab 页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_slab_unreclaimable 无法回收的 slab 页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_unevictable 不可驱逐页面数量 页计数 宿主 /proc/vmstat memory vmstat_nr_writeback 写入页面数 页计数 宿主 /proc/vmstat memory vmstat_numa_pages_migrated NUMA 迁移中的页面数 页计数 宿主 /proc/vmstat memory vmstat_pgdeactivate 页数被停用进入非活动 LRU 页计数 宿主 /proc/vmstat memory vmstat_pgrefill 扫描的活跃 LRU 页面数 页计数 宿主 /proc/vmstat memory vmstat_pgscan_direct 扫描的页数 页计数 宿主 /proc/vmstat memory vmstat_pgscan_kswapd 扫描的页面数量，由 kswapd 回收的数量 页计数 宿主 /proc/vmstat memory vmstat_pgsteal_direct 直接回收的页面 页计数 宿主 /proc/vmstat memory vmstat_pgsteal_kswapd 被 kswapd 回收的数量 页计数 宿主 /proc/vmstat memory hungtask_counter hungtask 事件计数 计数 宿主 BPF 埋点统计 memory oom_host_counter oom 事件计数 计数 宿主 BPF 埋点统计 memory oom_container_counter oom 事件计数 计数 容器 BPF 埋点统计 memory softlockup_counter softlockup 事件计数 计数 宿主 BPF 埋点统计 memory memory_free_compaction 内存压缩的速度 纳秒(ns) 宿主 bpf 埋点统计 memory memory_free_allocstall 内存中主机直接回收速度 纳秒(ns) 宿主 bpf 埋点统计 memory memory_cgroup_container_directstall cgroup 尝试直接回收的计数 计数 容器 bpf 埋点统计 IO iolatency_disk_d2c 磁盘访问时的 io 延迟统计，包括驱动程序和硬件组件消耗的时间 计数 宿主 bpf 埋点统计 IO iolatency_disk_q2c 磁盘访问整个 I/O 生命周期时的 I/O 延迟统计 计数 宿主 bpf 埋点统计 IO iolatency_container_d2c 磁盘访问时的 I/O 延迟统计，包括驱动程序和硬件组件消耗的时间 计数 容器 bpf 埋点统计 IO iolatency_container_q2c 磁盘访问整个 I/O 生命周期时的 I/O 延迟统计 计数 容器 bpf 埋点统计 IO iolatency_disk_flush 磁盘 RAID 设备刷新操作延迟统计 计数 宿主 bpf 埋点统计 IO iolatency_container_flush 磁盘 RAID 设备上由容器引起的刷新操作延迟统计 计数 容器 bpf 埋点统计 IO iolatency_disk_freeze 磁盘 freese 事件 计数 宿主 bpf 埋点统计 network tcp_mem_limit_pages 系统 TCP 总内存大小限制 页计数 系统 procfs network tcp_mem_usage_bytes 系统使用的 TCP 内存总字节数 字节(Bytes) 系统 tcp_mem_usage_pages * page_size network tcp_mem_usage_pages 系统使用的 TCP 内存总量 页计数 系统 procfs network tcp_mem_usage_percent 系统使用的 TCP 内存百分比（相对 TCP 内存总限制） % 系统 tcp_mem_usage_pages / tcp_mem_limit_pages network arp_entries arp 缓存条目数量 计数 宿主，容器 procfs network arp_total 总 arp 缓存条目数 计数 系统 procfs network qdisc_backlog 待发送的字节数 字节(Bytes) 宿主 netlink qdisc 统计 network qdisc_bytes_total 已发送的字节数 字节(Bytes) 宿主 netlink qdisc 统计 network qdisc_current_queue_length 排队等待发送的包数量 计数 宿主 netlink qdisc 统计 network qdisc_drops_total 丢弃的数据包数量 计数 宿主 netlink qdisc 统计 network qdisc_overlimits_total 排队数据包里超限的数量 计数 宿主 netlink qdisc 统计 network qdisc_packets_total 已发送的包数量 计数 宿主 netlink qdisc 统计 network qdisc_requeues_total 重新入队的数量 计数 宿主 netlink qdisc 统计 network ethtool_hardware_rx_dropped_errors 接口接收丢包统计 计数 宿主 硬件驱动相关, 如 mlx, ixgbe, bnxt_en, etc. network netdev_receive_bytes_total 接口接收的字节数 字节(Bytes) 宿主，容器 procfs network netdev_receive_compressed_total 接口接收的压缩包数量 计数 宿主，容器 procfs network netdev_receive_dropped_total 接口接收丢弃的包数量 计数 宿主，容器 procfs network netdev_receive_errors_total 接口接收检测到错误的包数量 计数 宿主，容器 procfs network netdev_receive_fifo_total 接口接收 fifo 缓冲区错误数量 计数 宿主，容器 procfs network netdev_receive_frame_total 接口接收帧对齐错误 计数 宿主，容器 procfs network netdev_receive_multicast_total 多播数据包已接收的包数量，对于硬件接口，此统计通常在设备层计算（与 rx_packets 不同），因此可能包括未到达的数据包 计数 宿主，容器 procfs network netdev_receive_packets_total 接口接收到的有效数据包数量 计数 宿主，容器 procfs network netdev_transmit_bytes_total 接口发送的字节数 字节(Bytes) 宿主，容器 procfs network netdev_transmit_carrier_total 接口发送过程中由于载波丢失导致的帧传输错误数量 计数 宿主，容器 procfs network netdev_transmit_colls_total 接口发送碰撞计数 计数 宿主，容器 procfs network netdev_transmit_compressed_total 接口发送压缩数据包数量 计数 宿主，容器 procfs network netdev_transmit_dropped_total 数据包在传输过程中丢失的数量，如资源不足 计数 宿主，容器 procfs network netdev_transmit_errors_total 发送错误计数 计数 宿主，容器 procfs network netdev_transmit_fifo_total 帧传输错误数量 计数 宿主，容器 procfs network netdev_transmit_packets_total 发送数据包计数 计数 宿主，容器 procfs network netstat_TcpExt_ArpFilter 因 ARP 过滤规则而被拒绝的 ARP 请求/响应包数量 计数 宿主，容器 procfs network netstat_TcpExt_BusyPollRxPackets 通过 busy polling​​ 机制接收到的网络数据包数量 计数 宿主，容器 procfs network netstat_TcpExt_DelayedACKLocked 由于用户态锁住了sock，而无法发送delayed ack的次数 计数 宿主，容器 procfs network netstat_TcpExt_DelayedACKLost 当收到已确认的包时，它将被更新。延迟 ACK 丢失可能会引起这个问题，但其他原因也可能触发，例如网络中重复的包。 计数 宿主，容器 procfs network netstat_TcpExt_DelayedACKs 延迟的 ACK 定时器已过期。TCP 堆栈将发送一个纯 ACK 数据包并退出延迟 ACK 模式 计数 宿主，容器 procfs network netstat_TcpExt_EmbryonicRsts 收到初始 SYN_RECV 套接字的重置 计数 宿主，容器 procfs network netstat_TcpExt_IPReversePathFilter - 计数 宿主，容器 procfs network netstat_TcpExt_ListenDrops 当内核收到客户端的 SYN 请求时，如果 TCP 接受队列已满，内核将丢弃 SYN 并将 TcpExtListenOverflows 加 1。同时，内核也会将 TcpExtListenDrops 加 1。当一个 TCP 套接字处于监听状态，且内核需要丢弃一个数据包时，内核会始终将 TcpExtListenDrops 加 1。因此，增加 TcpExtListenOverflows 会导致 TcpExtListenDrops 同时增加，但 TcpExtListenDrops 也会在没有 TcpExtListenOverflows 增加的情况下增加，例如内存分配失败也会导致 TcpExtListenDrops 增加。 计数 宿主，容器 procfs network netstat_TcpExt_ListenOverflows 当内核收到客户端的 SYN 请求时，如果 TCP 接受队列已满，内核将丢弃 SYN 并将 TcpExtListenOverflows 加 1。同时，内核也会将 TcpExtListenDrops 加 1。当一个 TCP 套接字处于监听状态，且内核需要丢弃一个数据包时，内核会始终将 TcpExtListenDrops 加 1。因此，增加 TcpExtListenOverflows 会导致 TcpExtListenDrops 同时增加，但 TcpExtListenDrops 也会在没有 TcpExtListenOverflows 增加的情况下增加，例如内存分配失败也会导致 TcpExtListenDrops 增加。 计数 宿主，容器 procfs network netstat_TcpExt_LockDroppedIcmps 由于套接字被锁定，ICMP 数据包被丢弃 计数 宿主，容器 procfs network netstat_TcpExt_OfoPruned 协议栈尝试在乱序队列中丢弃数据包 计数 宿主，容器 procfs network netstat_TcpExt_OutOfWindowIcmps ICMP 数据包因超出窗口而被丢弃 计数 宿主，容器 procfs network netstat_TcpExt_PAWSActive 数据包在 Syn-Sent 状态被 PAWS 丢弃 计数 宿主，容器 procfs network netstat_TcpExt_PAWSEstab 数据包在除 Syn-Sent 之外的所有状态下都会被 PAWS 丢弃 计数 宿主，容器 procfs network netstat_TcpExt_PFMemallocDrop - 计数 宿主，容器 procfs network netstat_TcpExt_PruneCalled 协议栈尝试回收套接字内存。更新此计数器后，将尝试合并乱序队列和接收队列。如果内存仍然不足，将尝试丢弃乱序队列中的数据包（并更新 TcpExtOfoPruned 计数器）。 计数 宿主，容器 procfs network netstat_TcpExt_RcvPruned 在从顺序错误的队列中‘collapse’和丢弃数据包后，如果实际使用的内存仍然大于最大允许内存，则此计数器将被更新。这意味着‘prune’失败 计数 宿主，容器 procfs network netstat_TcpExt_SyncookiesFailed MSS 从 SYN cookie 解码出来的无效。当这个计数器更新时，接收到的数据包不会被当作 SYN cookie 处理，并且 TcpExtSyncookiesRecv 计数器不会更新 计数 宿主，容器 procfs network netstat_TcpExt_SyncookiesRecv 接收了多少个 SYN cookies 的回复数据包 计数 宿主，容器 procfs network netstat_TcpExt_SyncookiesSent 发送了多少个 SYN cookies 计数 宿主，容器 procfs network netstat_TcpExt_TCPACKSkippedChallenge ACK 为 challenge ACK 时，将跳过 ACK 计数 宿主，容器 procfs network netstat_TcpExt_TCPACKSkippedFinWait2 ACK 在 Fin-Wait-2 状态被跳过，原因可能是 PAWS 检查失败或接收到的序列号超出窗口 计数 宿主，容器 procfs network netstat_TcpExt_TCPACKSkippedPAWS 由于 PAWS（保护包装序列号）检查失败，ACK 被跳过 计数 宿主，容器 procfs network netstat_TcpExt_TCPACKSkippedSeq 序列号超出窗口范围，时间戳通过 PAWS 检查，TCP 状态不是 Syn-Recv、Fin-Wait-2 和 Time-Wait 计数 宿主，容器 procfs network netstat_TcpExt_TCPACKSkippedSynRecv ACK 在 Syn-Recv 状态中被跳过。Syn-Recv 状态表示协议栈收到一个 SYN 并回复 SYN+ACK 计数 宿主，容器 procfs network netstat_TcpExt_TCPACKSkippedTimeWait CK 在 Time-Wait 状态中被跳过，原因可能是 PAWS 检查失败或接收到的序列号超出窗口 计数 宿主，容器 procfs network netstat_TcpExt_TCPAbortFailed 内核 TCP 层将在满足 RFC2525 2.17 节时发送 RST。如果在处理过程中发生内部错误，TcpExtTCPAbortFailed 将增加 计数 宿主，容器 procfs network netstat_TcpExt_TCPAbortOnClose 用户模式程序缓冲区中有数据时关闭的套接字数量 计数 宿主，容器 procfs network netstat_TcpExt_TCPAbortOnData TCP 层有正在传输的数据，但需要关闭连接 计数 宿主，容器 procfs network netstat_TcpExt_TCPAbortOnLinger 当 TCP 连接进入 FIN_WAIT_2 状态时，内核不会等待来自另一侧的 fin 包，而是发送 RST 并立即删除套接字 计数 宿主，容器 procfs network netstat_TcpExt_TCPAbortOnMemory 当一个应用程序关闭 TCP 连接时，内核仍然需要跟踪该连接，让它完成 TCP 断开过程 计数 宿主，容器 procfs network netstat_TcpExt_TCPAbortOnTimeout 此计数器将在任何 TCP 计时器到期时增加。在这种情况下，内核不会发送 RST，而是放弃连接 计数 宿主，容器 procfs network netstat_TcpExt_TCPAckCompressed - 计数 宿主，容器 procfs network netstat_TcpExt_TCPAutoCorking 发送数据包时，TCP 层会尝试将小数据包合并成更大的一个 计数 宿主，容器 procfs network netstat_TcpExt_TCPBacklogDrop - 计数 宿主，容器 procfs network netstat_TcpExt_TCPChallengeACK challenge ack 发送的数量 计数 宿主，容器 procfs network netstat_TcpExt_TCPDSACKIgnoredNoUndo 当 DSACK 块无效时，这两个计数器中的一个将被更新。哪个计数器将被更新取决于 TCP 套接字的 undo_marker 标志 计数 宿主，容器 procfs network netstat_TcpExt_TCPDSACKIgnoredOld 当 DSACK 块无效时，这两个计数器中的一个将被更新。哪个计数器将被更新取决于 TCP 套接字的 undo_marker 标志 计数 宿主，容器 procfs network netstat_TcpExt_TCPDSACKOfoRecv 收到一个 DSACK，表示收到一个顺序错误的重复数据包 计数 宿主，容器 procfs network netstat_TcpExt_TCPDSACKOfoSent 收到一个乱序的重复数据包，因此向发送者发送 DSACK 计数 宿主，容器 procfs network netstat_TcpExt_TCPDSACKOldSent 收到一个已确认的重复数据包，因此向发送者发送 DSACK 计数 宿主，容器 procfs network netstat_TcpExt_TCPDSACKRecv 收到一个 DSACK，表示收到了一个已确认的重复数据包 计数 宿主，容器 procfs network netstat_TcpExt_TCPDSACKUndo - 计数 宿主，容器 procfs network netstat_TcpExt_TCPDeferAcceptDrop - 计数 宿主，容器 procfs network netstat_TcpExt_TCPDelivered - 计数 宿主，容器 procfs network netstat_TcpExt_TCPDeliveredCE - 计数 宿主，容器 procfs network netstat_TcpExt_TCPFastOpenActive 当 TCP 栈在 SYN-SENT 状态接收到一个 ACK 包，并且 ACK 包确认了 SYN 包中的数据，理解 TFO cookie 已被对方接受，然后它更新这个计数器 计数 宿主，容器 procfs network netstat_TcpExt_TCPFastOpenActiveFail Fast Open 失败 计数 宿主，容器 procfs network netstat_TcpExt_TCPFastOpenBlackhole - 计数 宿主，容器 procfs network netstat_TcpExt_TCPFastOpenCookieReqd 客户端想要请求 TFO cookie 的次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPFastOpenListenOverflow 挂起的 Fast Open 请求数量大于 fastopenq-\u003emax_qlen 时，协议栈将拒绝 Fast Open 请求并更新此计数器 计数 宿主，容器 procfs network netstat_TcpExt_TCPFastOpenPassive 指示 TCP 堆栈接受 Fast Open 请求的次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPFastOpenPassiveFail 协议栈拒绝 Fast Open 的次数，这是由于 TFO cookie 无效或 在创建套接字过程中发现错误所引起的 计数 宿主，容器 procfs network netstat_TcpExt_TCPFastRetrans 快速重传 计数 宿主，容器 procfs network netstat_TcpExt_TCPFromZeroWindowAdv TCP 接收窗口设置为非零值 计数 宿主，容器 procfs network netstat_TcpExt_TCPFullUndo - 计数 宿主，容器 procfs network netstat_TcpExt_TCPHPAcks 如果数据包设置了 ACK 标志且没有数据，则是一个纯 ACK 数据包，如果内核在快速路径中处理它，TcpExtTCPHPAcks 将增加 1 计数 宿主，容器 procfs network netstat_TcpExt_TCPHPHits 如果 TCP 数据包包含数据（这意味着它不是一个纯 ACK 数据包），并且此数据包在快速路径中处理，TcpExtTCPHPHits 将增加 1 计数 宿主，容器 procfs network netstat_TcpExt_TCPHystartDelayCwnd CWND 检测到的包延迟总和。将此值除以 TcpExtTCPHystartDelayDetect，即为通过包延迟检测到的平均 CWND 计数 宿主，容器 procfs network netstat_TcpExt_TCPHystartDelayDetect 检测到数据包延迟阈值次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPHystartTrainCwnd TCP Hystart 训练中使用的拥塞窗口大小，将此值除以 TcpExtTCPHystartTrainDetect 得到由 ACK 训练长度检测到的平均 CWND 计数 宿主，容器 procfs network netstat_TcpExt_TCPHystartTrainDetect TCP Hystart 训练检测的次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPKeepAlive 此计数器指示已发送的保活数据包。默认情况下不会启用保活功能。用户空间程序可以通过设置 SO_KEEPALIVE 套接字选项来启用它。 计数 宿主，容器 procfs network netstat_TcpExt_TCPLossFailures 丢失数据包而进行恢复失败的次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPLossProbeRecovery 检测到丢失的数据包恢复的次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPLossProbes TCP 检测到丢失的数据包数量，通常用于检测网络拥塞或丢包 计数 宿主，容器 procfs network netstat_TcpExt_TCPLossUndo TCP重传数据包成功到达目标端口，但之前已经由于超时或拥塞丢失，因此被视为“撤销”丢失的数据包数量 计数 宿主，容器 procfs network netstat_TcpExt_TCPLostRetransmit 丢包重传个数 计数 宿主，容器 procfs network netstat_TcpExt_TCPMD5Failure 校验错误 计数 宿主，容器 procfs network netstat_TcpExt_TCPMD5NotFound 校验错误 计数 宿主，容器 procfs network netstat_TcpExt_TCPMD5Unexpected 校验错误 计数 宿主，容器 procfs network netstat_TcpExt_TCPMTUPFail 使用 DSACK 无需慢启动即可恢复拥塞窗口 计数 宿主，容器 procfs network netstat_TcpExt_TCPMTUPSuccess - 计数 宿主，容器 procfs network netstat_TcpExt_TCPMemoryPressures 到达 tcp 内存压力位 low 的次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPMemoryPressuresChrono - 计数 宿主，容器 procfs network netstat_TcpExt_TCPMinTTLDrop - 计数 宿主，容器 procfs network netstat_TcpExt_TCPOFODrop TCP 层接收到一个乱序的数据包，但内存不足，因此丢弃它。此类数据包不会计入 TcpExtTCPOFOQueue 计数 计数 宿主，容器 procfs network netstat_TcpExt_TCPOFOMerge 接收到的顺序错误的包与上一个包有重叠。重叠部分将被丢弃。所有 TcpExtTCPOFOMerge 包也将计入 TcpExtTCPOFOQueue 计数 宿主，容器 procfs network netstat_TcpExt_TCPOFOQueue TCP 层接收到一个乱序的数据包，并且有足够的内存来排队它 计数 宿主，容器 procfs network netstat_TcpExt_TCPOrigDataSent 发送原始数据（不包括重传但包括 SYN 中的数据）的包数量。此计数器与 TcpOutSegs 不同，因为 TcpOutSegs 还跟踪纯 ACK。TCPOrigDataSent 更有助于跟踪 TCP 重传率 计数 宿主，容器 procfs network netstat_TcpExt_TCPPartialUndo 检测到一些错误的重传，在我们快速重传的同时，收到了部分确认，因此能够部分撤销我们的一些 CWND 减少 计数 宿主，容器 procfs network netstat_TcpExt_TCPPureAcks 如果数据包设置了 ACK 标志且没有数据，则是一个纯 ACK 数据包，如果内核在快速路径中处理它，TcpExtTCPHPAcks 将增加 1，如果内核在慢速路径中处理它，TcpExtTCPPureAcks 将增加 1 计数 宿主，容器 procfs network netstat_TcpExt_TCPRcvCoalesce 当数据包被 TCP 层接收但未被应用程序读取时，TCP 层会尝试合并它们。这个计数器表示在这种情况下合并了多少个数据包。如果启用了 GRO，GRO 会合并大量数据包，这些数据包不会被计算到 TcpExtTCPRcvCoalesce 中 计数 宿主，容器 procfs network netstat_TcpExt_TCPRcvCollapsed 在“崩溃”过程中释放了多少个 skbs 计数 宿主，容器 procfs network netstat_TcpExt_TCPRenoFailures TCP_CA_Disorder 阶段进入并经历 RTO 的重传失败次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPRenoRecovery 当拥塞控制进入恢复状态时，如果使用 sack，TcpExtTCPSackRecovery 增加 1，如果不使用 sack，TcpExtTCPRenoRecovery 增加 1。这两个计数器意味着协议栈开始重传丢失的数据包 计数 宿主，容器 procfs network netstat_TcpExt_TCPRenoRecoveryFail 进入恢复阶段并 RTO 的连接数 计数 宿主，容器 procfs network netstat_TcpExt_TCPRenoReorder 重排序数据包被快速恢复检测到。只有在 SACK 被禁用时才会使用 计数 宿主，容器 procfs network netstat_TcpExt_TCPReqQFullDoCookies - 计数 宿主，容器 procfs network netstat_TcpExt_TCPReqQFullDrop - 计数 宿主，容器 procfs network netstat_TcpExt_TCPRetransFail 尝试将重传数据包发送到下层，但下层返回错误 计数 宿主，容器 procfs network netstat_TcpExt_TCPSACKDiscard 有多少个 SACK 块无效。如果无效的 SACK 块是由 ACK 记录引起的，tcp 栈只会忽略它，而不会更新此计数器 计数 宿主，容器 procfs network netstat_TcpExt_TCPSACKReneging 一个数据包被 SACK 确认，但接收方已丢弃此数据包，因此发送方需要重传此数据包 计数 宿主，容器 procfs network netstat_TcpExt_TCPSACKReorder SACK 检测到的重排序数据包 计数 宿主，容器 procfs network netstat_TcpExt_TCPSYNChallenge 响应 SYN 数据包发送的 Challenge ack 数 计数 宿主，容器 procfs network netstat_TcpExt_TCPSackFailures TCP_CA_Disorder 阶段进入并经历 RTO 的重传失败次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPSackMerged skb 已合并计数 计数 宿主，容器 procfs network netstat_TcpExt_TCPSackRecovery 当拥塞控制进入恢复状态时，如果使用 sack，TcpExtTCPSackRecovery 增加 1，如果不使用 sack，TcpExtTCPRenoRecovery 增加 1。这两个计数器意味着 TCP 栈开始重传丢失的数据包 计数 宿主，容器 procfs network netstat_TcpExt_TCPSackRecoveryFail SACK 恢复失败的次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPSackShiftFallback skb 应该被移动或合并，但由于某些原因，TCP 堆栈没有这样做 计数 宿主，容器 procfs network netstat_TcpExt_TCPSackShifted skb 被移位 计数 宿主，容器 procfs network netstat_TcpExt_TCPSlowStartRetrans 重新传输一个数据包，拥塞控制状态为“丢失” 计数 宿主，容器 procfs network netstat_TcpExt_TCPSpuriousRTOs 虚假重传超时 计数 宿主，容器 procfs network netstat_TcpExt_TCPSpuriousRtxHostQueues 当 TCP 栈想要重传一个数据包，发现该数据包并未在网络中丢失，但数据包尚未发送，TCP 栈将放弃重传并更新此计数器。这可能会发生在数据包在 qdisc 或驱动程序队列中停留时间过长的情况下 计数 宿主，容器 procfs network netstat_TcpExt_TCPSynRetrans SYN 和 SYN/ACK 重传次数，将重传分解为 SYN、快速重传、超时重传等 计数 宿主，容器 procfs network netstat_TcpExt_TCPTSReorder tcp 栈在接收到时间截包而进行乱序包阀值调整的次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPTimeWaitOverflow TIME_WAIT 状态的套接字因超出限制而无法分配的数量 计数 宿主，容器 procfs network netstat_TcpExt_TCPTimeouts TCP 超时事件 计数 宿主，容器 procfs network netstat_TcpExt_TCPToZeroWindowAdv TCP 接收窗口从非零值设置为零 计数 宿主，容器 procfs network netstat_TcpExt_TCPWantZeroWindowAdv 根据当前内存使用情况，TCP 栈尝试将接收窗口设置为零。但接收窗口可能仍然是一个非零值 计数 宿主，容器 procfs network netstat_TcpExt_TCPWinProbe 定期发送的 ACK 数据包数量，以确保打开窗口的反向 ACK 数据包没有丢失 计数 宿主，容器 procfs network netstat_TcpExt_TCPWqueueTooBig - 计数 宿主，容器 procfs network netstat_TcpExt_TW TCP 套接字在快速计时器中完成 time wait 状态 计数 宿主，容器 procfs network netstat_TcpExt_TWKilled TCP 套接字在慢速计时器中完成 time wait 状态 计数 宿主，容器 procfs network netstat_TcpExt_TWRecycled 等待套接字通过时间戳回收 计数 宿主，容器 procfs network netstat_Tcp_ActiveOpens TCP 层发送一个 SYN，进入 SYN-SENT 状态。每当 TcpActiveOpens 增加 1 时，TcpOutSegs 应该始终增加 1 计数 宿主，容器 procfs network netstat_Tcp_AttemptFails TCP 连接从 SYN-SENT 状态或 SYN-RCVD 状态直接过渡到 CLOSED 状态次数，加上 TCP 连接从 SYN-RCVD 状态直接过渡到 LISTEN 状态次数 计数 宿主，容器 procfs network netstat_Tcp_CurrEstab TCP 连接数，当前状态为 ESTABLISHED 或 CLOSE-WAIT 计数 宿主，容器 procfs network netstat_Tcp_EstabResets TCP 连接从 ESTABLISHED 状态或 CLOSE-WAIT 状态直接过渡到 CLOSED 状态次数 计数 宿主，容器 procfs network netstat_Tcp_InCsumErrors TCP 校验和错误 计数 宿主，容器 procfs network netstat_Tcp_InErrs 错误接收到的段总数（例如，错误的 TCP 校验和） 计数 宿主，容器 procfs network netstat_Tcp_InSegs TCP 层接收到的数据包数量。如 RFC1213 所述，包括接收到的错误数据包，如校验和错误、无效 TCP 头等 计数 宿主，容器 procfs network netstat_Tcp_MaxConn 可以支持的总 TCP 连接数限制，在最大连接数动态的实体中，此对象应包含值-1 计数 宿主，容器 procfs network netstat_Tcp_OutRsts TCP 段中包含 RST 标志的数量 计数 宿主，容器 procfs network netstat_Tcp_OutSegs 发送的总段数，包括当前连接上的段，但不包括仅包含重传字节的段 计数 宿主，容器 procfs network netstat_Tcp_PassiveOpens TCP 连接从监听状态直接过渡到 SYN-RCVD 状态的次数 计数 宿主，容器 procfs network netstat_Tcp_RetransSegs 总重传段数 - 即包含一个或多个先前已传输字节的 TCP 段传输的数量 计数 宿主，容器 procfs network netstat_Tcp_RtoAlgorithm The algorithm used to determine the timeout value used for retransmitting unacknowledged octets 计数 宿主，容器 procfs network netstat_Tcp_RtoMax TCP 实现允许的重传超时最大值，以毫秒为单位 毫秒 宿主，容器 procfs network netstat_Tcp_RtoMin TCP 实现允许的重传超时最小值，以毫秒为单位 毫秒 宿主，容器 procfs network sockstat_FRAG_inuse - 计数 宿主，容器 procfs network sockstat_FRAG_memory - 页计数 宿主，容器 procfs network sockstat_RAW_inuse 使用的 RAW 套接字数量 计数 宿主，容器 procfs network sockstat_TCP_alloc TCP 已分配的套接字数量 计数 宿主，容器 procfs network sockstat_TCP_inuse 已建立的 TCP 套接字数量 计数 宿主，容器 procfs network sockstat_TCP_mem 系统使用的 TCP 内存总量 页计数 系统 procfs network sockstat_TCP_mem_bytes 系统使用的 TCP 内存总量 字节(Bytes) 系统 sockstat_TCP_mem * page_size network sockstat_TCP_orphan TCP 等待关闭的连接数 计数 宿主，容器 procfs network sockstat_TCP_tw TCP 套接字终止数量 计数 宿主，容器 procfs network sockstat_UDPLITE_inuse - 计数 宿主，容器 procfs network sockstat_UDP_inuse 使用的 UDP 套接字数量 计数 宿主，容器 procfs network sockstat_UDP_mem 系统使用的 UDP 内存总量 页计数 系统 procfs network sockstat_UDP_mem_bytes 系统使用的 UDP 内存字节数总和 字节(Bytes) 系统 sockstat_UDP_mem * page_size network sockstat_sockets_used 系统使用 socket 数量 计数 系统 procfs ","categories":"","description":"","excerpt":"当前版本支持的指标:\n子系统 指标 描述 单位 统计纬度 指标来源 cpu cpu_util_sys cpu 系统态利用率 % …","ref":"/docs/zh/latest/key-feature/metrics/","tags":"","title":"指标说明"},{"body":"1. Build with the Official Image To isolate the developer’s local environment and simplify the build process, we provide a containerized build method. You can directly use docker build to produce an image containing the core collector huatuo-bamai, BPF objects, tools, and more. Run the following in the project root directory:\ndocker build --network host -t huatuo/huatuo-bamai:v2.1.0 . 2. Build a Custom Image Dockerfile.dev:\nFROM golang:1.23.0-alpine AS base # Speed up Alpine package installation if needed # RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories RUN apk add --no-cache \\ make \\ clang15 \\ libbpf-dev \\ bpftool \\ curl \\ git ENV PATH=$PATH:/usr/lib/llvm15/bin # build huatuo components FROM base AS build ARG BUILD_PATH=${BUILD_PATH:-/go/huatuo-bamai} ARG RUN_PATH=${RUN_PATH:-/home/huatuo-bamai} WORKDIR ${BUILD_PATH} 2.1 Build the Dev Image docker build --network host -t huatuo/huatuo-bamai-dev:latest -f ./Dockerfile.dev . 2.2 Run the Dev Container docker run -it --privileged --cgroupns=host --network=host \\ -v /path/to/huatuo:/go/huatuo-bamai \\ huatuo/huatuo-bamai-dev:latest sh 2.3 Compile Inside the Container Run:\nmake Once the build completes, all artifacts are generated under ./_output.\n3. Build on a Physical Machine or VM The collector depends on the following tools. Install them based on your local environment:\nmake git clang15 libbpf bpftool curl Due to significant differences across local environments, build issues may occur.\nTo avoid environment inconsistencies and simplify troubleshooting, we strongly recommend using the Docker build approach whenever possible.\n","categories":"","description":"","excerpt":"1. Build with the Official Image To isolate the developer’s local …","ref":"/docs/en/v2.1.0/tutorials/compile/","tags":"","title":"Compile"},{"body":"1 官方镜像编译 为隔离开发者本地环境和简化编译流程，我们提供容器化编译方式，你可以直接通过 docker build，构建完成的镜像（包含底层采集器 huatuo-bamai、bpf obj、工具等）。在项目根目录运行：\ndocker build --network host -t huatuo/huatuo-bamai:v2.1.0 . 2 自定义镜像编译 文件 Dockerfile.dev 内容：\nFROM golang:1.23.0-alpine AS base # 镜像构建加速 # RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories RUN apk add --no-cache \\ make \\ clang15 \\ libbpf-dev \\ bpftool \\ curl \\ git ENV PATH=$PATH:/usr/lib/llvm15/bin # build huatuo components FROM base AS build ARG BUILD_PATH=${BUILD_PATH:-/go/huatuo-bamai} ARG RUN_PATH=${RUN_PATH:-/home/huatuo-bamai} WORKDIR ${BUILD_PATH} 2.1 构建 dev 镜像 docker build --network host -t huatuo/huatuo-bamai-dev:latest -f ./Dockerfile.dev . 2.2 运行 dev 容器 docker run -it --privileged --cgroupns=host --network=host -v /path/to/huatuo:/go/huatuo-bamai huatuo/huatuo-bamai-dev:latest sh 2.3 编译 容器内执行\nmake 编译完成后，所有产出物在 ./_output 下。\n3 物理机或 VM 编译 采集器编译依赖以下工具，可自行根据本地实际环境安装：\nmake git clang15 libbpf bpftool curl 考虑本地环境差异过大，遇到编译可能问题会很多，为隔离环境差异，排查问题也方便建议尽量使用 docker 编译方式。\n","categories":"","description":"","excerpt":"1 官方镜像编译 为隔离开发者本地环境和简化编译流程，我们提供容器化编译方式，你可以直接通过 docker build，构建完成的镜像（包含 …","ref":"/docs/zh/v2.1.0/tutorials/compile/","tags":"","title":"编译"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/zh/","tags":"","title":""},{"body":"The framework provides convenient APIs, including module startup, data storage, container information, BPF-related (load, attach, read, detach, unload), etc. You can implement custom collection logic and flexibly choose the appropriate collection mode and storage method.\nTracing Type Based on your scenarios, you can implement the ITracingEvent interface in the core/autotracing or core/events directory to complete tracing-type collection.\n// ITracingEvent represents a tracing/event type ITracingEvent interface { Start(ctx context.Context) error } example:\ntype exampleTracing struct{} // Register callback func init() { tracing.RegisterEventTracing(\"example\", newExample) } // Create tracing func newExample() (*tracing.EventTracingAttr, error) { return \u0026tracing.EventTracingAttr{ TracingData: \u0026exampleTracing{}, Internal: 10, // Interval for enable tracing again (in seconds) Flag: tracing.FlagTracing, // mark as tracing type }, nil } // Implement ITracingEvent func (t *exampleTracing) Start(ctx context.Context) error { // do something ... // Save data to ES and local file storage.Save(\"example\", ccontainerID, time.Now(), tracerData) } // Implement Collector interface for Prometheus format output (optional) func (c *exampleTracing) Update() ([]*metric.Data, error) { // from tracerData to prometheus.Metric ... return data, nil } Metric Type Implement the Collector interface in the path core/metrics to complete metric-type collection.\ntype Collector interface { // Get new metrics and expose them via prometheus registry. Update() ([]*Data, error) } example:\ntype exampleMetric struct{} // Register callback func init() { tracing.RegisterEventTracing(\"example\", newExample) } // Create Metric func newExample() (*tracing.EventTracingAttr, error) { return \u0026tracing.EventTracingAttr{ TracingData: \u0026filenrCollector{ metric: []*metric.Data{ metric.NewGaugeData(\"name1\", 0, \"description of example_name1\", nil), metric.NewGaugeData(\"name2\", 0, \"description of example_name2\", nil), }, }, Flag: tracing.FlagMetric, // mark as Metric type }, nil } // Implement Collector interface for Prometheus format output func (c *exampleMetric) Update() ([]*metric.Data, error) { // do something ... return data, nil } The path core of the project includes multiple useful examples of the three collection modules, covering BPF code, map data interaction, container information, and more. For further details, refer to the corresponding code implementations.\n","categories":"","description":"","excerpt":"The framework provides convenient APIs, including module startup, data …","ref":"/docs/en/v2.1.0/tutorials/addcase/custom/","tags":"","title":"Add Custom Collection"},{"body":"HUATUO 为减少理解成本，提供了最简单的 daemonset 部署方式。\n通过 daemonset 方式部署 HUATUO 采集器有如下步骤。\n1 拉取采集器配置文件 curl -L -o huatuo-bamai.conf https://github.com/ccfos/huatuo/raw/v2.1.0/huatuo-bamai.conf 配置文件需要根据你的实际环境作修改，如连接 kubelet 和 elasticsrearch 的相关配置。\n2 创建 configMap kubectl create configmap huatuo-bamai-config --from-file=./huatuo-bamai.conf 3 部署采集器 kubectl apply -f huatuo-daemonset.minimal.yaml 文件 huatuo-daemonset.minimal.yaml 内容：\napiVersion: apps/v1 kind: DaemonSet metadata: name: huatuo namespace: default labels: app: huatuo spec: selector: matchLabels: app: huatuo template: metadata: labels: app: huatuo spec: containers: - name: huatuo image: docker.io/huatuo/huatuo-bamai:v2.1.0 resources: limits: cpu: '1' memory: 2Gi requests: cpu: 500m memory: 512Mi securityContext: privileged: true volumeMounts: - name: proc mountPath: /proc - name: sys mountPath: /sys - name: run mountPath: /run - name: var mountPath: /var - name: etc mountPath: /etc - name: record mountPath: /home/huatuo-bamai/record - name: huatuo-bamai-config-volume mountPath: /home/huatuo-bamai/conf/huatuo-bamai.conf subPath: huatuo-bamai.conf volumes: - name: proc hostPath: path: /proc - name: sys hostPath: path: /sys - name: run hostPath: path: /run - name: var hostPath: path: /var - name: etc hostPath: path: /etc - name: record hostPath: path: /var/log/huatuo/record type: DirectoryOrCreate - name: huatuo-bamai-config-volume configMap: name: huatuo-bamai-config hostNetwork: true hostPID: true ","categories":"","description":"","excerpt":"HUATUO 为减少理解成本，提供了最简单的 daemonset 部署方式。\n通过 daemonset 方式部署 HUATUO 采集器有如下 …","ref":"/docs/zh/v2.1.0/tutorials/deploy/daemonset/","tags":"","title":"Daemonset"},{"body":"HUATUO provides the simplest DaemonSet deployment option to minimize setup complexity.\nDeploying the HUATUO collector via DaemonSet involves the following steps:\n1. Download the Collector Configuration File curl -L -o huatuo-bamai.conf https://github.com/ccfos/huatuo/raw/v2.1.0/huatuo-bamai.conf Modify this configuration file according to your environment, such as kubelet connection settings and Elasticsearch settings.\n2. Create a ConfigMap kubectl create configmap huatuo-bamai-config --from-file=./huatuo-bamai.conf 3. Deploy the Collector kubectl apply -f huatuo-daemonset.minimal.yaml Contents of huatuo-daemonset.minimal.yaml:\napiVersion: apps/v1 kind: DaemonSet metadata: name: huatuo namespace: default labels: app: huatuo spec: selector: matchLabels: app: huatuo template: metadata: labels: app: huatuo spec: containers: - name: huatuo image: docker.io/huatuo/huatuo-bamai:v2.1.0 resources: limits: cpu: '1' memory: 2Gi requests: cpu: 500m memory: 512Mi securityContext: privileged: true volumeMounts: - name: proc mountPath: /proc - name: sys mountPath: /sys - name: run mountPath: /run - name: var mountPath: /var - name: etc mountPath: /etc - name: record mountPath: /home/huatuo-bamai/record - name: huatuo-bamai-config-volume mountPath: /home/huatuo-bamai/conf/huatuo-bamai.conf subPath: huatuo-bamai.conf volumes: - name: proc hostPath: path: /proc - name: sys hostPath: path: /sys - name: run hostPath: path: /run - name: var hostPath: path: /var - name: etc hostPath: path: /etc - name: record hostPath: path: /var/log/huatuo/record type: DirectoryOrCreate - name: huatuo-bamai-config-volume configMap: name: huatuo-bamai-config hostNetwork: true hostPID: true ","categories":"","description":"","excerpt":"HUATUO provides the simplest DaemonSet deployment option to minimize …","ref":"/docs/en/v2.1.0/tutorials/deploy/daemonset/","tags":"","title":"Daemonset"},{"body":" To help users quickly experience and deploy HUATUO, this document is divided into three sections: Quick Experience，Quick Start，Compilation \u0026 Deployment.\n1 Quick Experience This section helps you quickly explore the frontend capabilities. You can directly perform various query operations on the frontend dashboard, such as viewing exception event overviews, exception event context information, metric curves, etc. Access the dashboard example without login:\nEvents, AutoTracing Dashboard（improvements in progress） In the example, jumping to the flame graph page requires login. Account: huatuo Password: huatuo1024\nHost Metrics Dashboard（improvements in progress） Container Metrics Dashboard（improvements in progress） 2 Quick Start 2.1 Quick Run If you want to understand the underlying principles and deploy HUATUO to your own monitoring system, you can start pre-compiled container images via Docker (Note: This method disables container information retrieval and ES storage functionality by default).\nDirect Execution：\n$ docker run --privileged --cgroupns=host --network=host -v /sys:/sys -v /proc:/proc -v /run:/run huatuo/huatuo-bamai:latest Metric Collection：In another terminal, collect metrics\n$ curl -s localhost:19704/metrics View Exception Events (Events, AutoTracing)：HUATUO stores collected kernel exception event information in ES (disabled by default) while retaining a copy in the local directory huatuo-local. Note: Typically, no files exist in this path (systems in normal state don’t trigger event collection). You can generate events by creating exception scenarios or modifying configuration thresholds.\n2.2 Quick Setup If you want to further understand HUATUO’s operational mechanisms, architecture design, monitoring dashboard, and custom deployment, you can quickly set up a complete local environment using docker compose.\n$ docker compose --project-directory ./build/docker up This command pulls the latest images and starts components including elasticsearch, prometheus, grafana，huatuo-bamai. After successful command execution, open your browser and visit http://localhost:3000 to access the monitoring dashboard (Grafana default admin account: admin, password: admin; Since your system is in normal state, the Events and AutoTracing dashboards typically won’t display data).\n3 Compilation \u0026 Deployment 3.1 Compilation To isolate the developer’s local environment and simplify the compilation process, we provide containerized compilation. You can directly use docker build to construct the completed image (including the underlying collector huatuo-bamai, BPF objects, tools, etc.). Run the following command in the project root directory:\n$ docker build --network host -t huatuo/huatuo-bamai:latest . 3.2 Execution Run container:\n$ docker run --privileged --cgroupns=host --network=host -v /sys:/sys -v /proc:/proc -v /run:/run huatuo/huatuo-bamai:latest Or copy all files from the container path /home/huatuo-bamai and run manually locally:\n$ ./huatuo-bamai --region example --config huatuo-bamai.conf Management: Can be managed using systemd/supervisord/k8s-DaemonSet, etc.\n3.3 Configuration Container Information Configuration HUATUO obtains POD/container information by calling the kubelet interface. Configure the access interface and certificates according to your actual environment. Empty configuration \"\" indicates disabling this functionality.\n[Pod] KubeletPodListURL = \"http://127.0.0.1:10255/pods\" KubeletPodListHTTPSURL = \"https://127.0.0.1:10250/pods\" KubeletPodClientCertPath = \"/var/lib/kubelet/pki/kubelet-client-current.pem\" Storage Configuration Metric Storage (Metric): All metrics are stored in Prometheus. You can access the :19704/metrics interface to obtain metrics.\nException Event Storage (Events, AutoTracing): All kernel events and AutoTracing events are stored in ES. Note: If the configuration is empty, ES storage is not activated, and events are only stored in the local directory huatuo-local.\nES storage configuration is as follows:\n[Storage.ES] Address = \"http://127.0.0.1:9200\" Username = \"elastic\" Password = \"huatuo-bamai\" Index = \"huatuo_bamai\" Local storage configuration is as follows:\n# tracer's record data # Path: all but the last element of path for per tracer # RotationSize: the maximum size in Megabytes of a record file before it gets rotated for per subsystem # MaxRotation: the maximum number of old log files to retain for per subsystem [Storage.LocalFile] Path = \"huatuo-local\" RotationSize = 100 MaxRotation = 10 Event Thresholds All kernel event collections (Events and AutoTracing) can have configurable trigger thresholds. The default thresholds are empirical data repeatedly validated in actual production environments. You can modify thresholds in huatuo-bamai.conf according to your requirements.\nResource Limits To ensure host machine stability, we have implemented resource limits for the collector. LimitInitCPU represents CPU resources occupied during collector startup, while LimitCPU/LimitMem represent resource limits for normal operation after successful startup:\n[RuntimeCgroup] LimitInitCPU = 0.5 LimitCPU = 2.0 # limit memory (MB) LimitMem = 2048 ","categories":"","description":"","excerpt":" To help users quickly experience and deploy HUATUO, this document is …","ref":"/docs/en/v2.1.0/gettingstarted/quick-start/","tags":"","title":"Getting started"},{"body":" 为帮助大家快速体验、部署 HUATUO, 该文档分别从 极速体验，容器启动，编译部署 三部分说明。\n1. 极速体验 你可以直接登陆示例网站访问前端监控大盘示例，如内核指标、异常事件、火焰图等（账户：huatuo 密码：huatuo1024）。\nEvents, AutoTracing 大盘（持续完善）\n宿主机 Metrics 大盘（持续完善）\n容器 Metrics 大盘（持续完善）\n2. 容器启动 2.1 Docker 启动 通过 docker 启动已经编译好的容器镜像（注意：该方式默认关闭了获取容器信息功能，和 ES 存储功能）。\n启动容器： $ docker run --privileged --cgroupns=host --network=host -v /sys:/sys -v /proc:/proc -v /run:/run huatuo/huatuo-bamai:latest 获取指标：打开另外一个终端，通过 curl 获取。 $ curl -s localhost:19704/metrics 查看异常事件 (Events, AutoTracing)：HUATUO 会将采集到的内核异常事件信息在 ES （已关闭），和本地目录 huatuo-local 分别存储。注意：通常该路径下没有任何文件（正常状态的系统不会触发事件采集），你可以通过构造异常场景或者修改配置文件阈值产生事件。 2.2 Docker Compose 启动 通过 docker compose，可以快速地在本地搭建部署一套完整的环境。该命令拉取最新镜像，启动 elasticsearch, prometheus, grafana，huatuo-bamai 等组件。命令执行成功后，打开浏览器访问 http://localhost:3000 即可浏览监控大盘（grafana 默认管理员账户：admin 密码：admin； 系统正常状态不会触发 Events, AutoTracing）。\n$ docker compose --project-directory ./build/docker up 3. 编译部署 3.1 编译 为隔离开发者本地环境和简化编译流程，我们提供容器化编译，你可以直接通过 docker build，构建完成的镜像（包含底层采集器 huatuo-bamai、bpf obj、工具等）。在项目根目录运行：\n$ docker build --network host -t huatuo/huatuo-bamai:latest . 3.2 运行 运行容器：\n$ docker run --privileged --cgroupns=host --network=host -v /sys:/sys -v /proc:/proc -v /run:/run huatuo/huatuo-bamai:latest 或从容器 /home/huatuo-bamai 路径下拷贝出所有文件后本地手动运行：\n$ ./huatuo-bamai --region example --config huatuo-bamai.conf 注意：可使用 systemd/supervisord/k8s-DaemonSet 等方式托管运行。\n3.3 配置 配置容器信息 HUATUO 通过调用 kubelet 接口获取POD/容器信息。你可以根据实际环境配置访问接口和证书，KubeletAuthorizedPort = 0, KubeletReadOnlyPort = 0 表示禁用该功能。\n[Pod] KubeletClientCertPath = \"/etc/kubernetes/pki/apiserver-kubelet-client.crt,/etc/kubernetes/pki/apiserver-kubelet-client.key\" 配置存储\n指标存储 (Metric): 所有的指标都存储在 prometheus，你可以通过访问 :19704/metrics 接口获取指标。\n异常事件存储 (Events, AutoTracing): 所有的内核事件，和 Autotracing 事件都存储在 ES。注意：如果配置为空表示不启动 ES 存储，只在本地目录 huatuo-local 存储事件。\nES 存储配置如下：\n[Storage.ES] Address = \"http://127.0.0.1:9200\" Username = \"elastic\" Password = \"huatuo-bamai\" Index = \"huatuo_bamai\" 本地存储配置如下：\n# tracer's record data # Path: all but the last element of path for per tracer # RotationSize: the maximum size in Megabytes of a record file before it gets rotated for per subsystem # MaxRotation: the maximum number of old log files to retain for per subsystem [Storage.LocalFile] Path = \"huatuo-local\" RotationSize = 100 MaxRotation = 10 事件阈值 所有的内核事件采集 Events 和 AutoTracing 都可以配置触发阈值。默认的阈值都是在实际生产环境反复验证后的经验数据，你可以根据自身需求，在 huatuo-bamai.conf 中修改阈值。\n资源限制 为保障物理机稳定性，我们对采集器进行了资源限制，其中 LimitInitCPU 表示采集器启动阶段占用的 CPU 资源，LimitCPU/LimitMem 表示采集器启动成功后常态占用的资源限制：\n[RuntimeCgroup] LimitInitCPU = 0.5 LimitCPU = 2.0 LimitMem = 2048 ","categories":"","description":"","excerpt":" 为帮助大家快速体验、部署 HUATUO, 该文档分别从 极速体验，容器启动，编译部署 三部分说明。\n1. 极速体验 你可以直接登陆示例网站 …","ref":"/docs/zh/latest/quick-start/","tags":"","title":"快速开始"},{"body":" 为帮助大家快速体验、部署 HUATUO, 该文档分别从 极速体验，快速开始，编译部署 三部分说明。\n1 极速体验 该部分可以帮助你快速了解前端效果。你可以直接在前端大盘进行一些查询操作，如查看异常事件概览、异常事件上下文信息、指标曲线等，免登陆访问大盘示例：\nEvents, AutoTracing 大盘（完善中） 示例中，跳转火焰图页面需登录，账户：huatuo 密码：huatuo1024\n宿主机 Metrics 大盘（完善中） 容器 Metrics 大盘（完善中） 2 快速开始 2.1 快速运行 如果你想了解底层原理，将 HUATUO 部署到自己的监控系统。你可以通过 docker 启动已经编译好的容器镜像（注意：该方式默认关闭了获取容器信息功能，和 ES 存储功能）。\n直接运行：\n$ docker run --privileged --cgroupns=host --network=host -v /sys:/sys -v /proc:/proc -v /run:/run huatuo/huatuo-bamai:latest 获取指标：在另外一个终端获取指标\n$ curl -s localhost:19704/metrics 查看异常事件 (Events, AutoTracing)：HUATUO 会将采集到的内核异常事件信息在 ES 存储一份（已关闭），同时在本地目录 huatuo-local 留存一份。注意：通常该路径下没有任何文件（正常状态的系统不会触发事件采集），你可以通过构造异常场景或者修改配置文件阈值产生事件。\n2.2 快速搭建 如果你想更进一步了解 HUATUO 运行机制、架构设计、监控大盘、自定义部署等。通过 docker compose，可以快速地在本地搭建部署一套完整的环境。\n$ docker compose --project-directory ./build/docker up 该命令拉取最新镜像，同时启动 elasticsearch, prometheus, grafana，huatuo-bamai 等组件。命令执行成功后，打开浏览器访问 http://localhost:3000 即可浏览监控大盘（grafana 默认管理员账户：admin 密码：admin； 由于你的系统处于正常状态，Events, AutoTracing 大盘里通常没有数据）。\n3 编译部署 3.1 编译 为隔离开发者本地环境和简化编译流程，我们提供容器化编译方式，你可以直接通过 docker build，构建完成的镜像（包含底层采集器 huatuo-bamai、bpf obj、工具等）。在项目根目录运行：\n$ docker build --network host -t huatuo/huatuo-bamai:latest . 3.2 运行 运行容器：\n$ docker run --privileged --cgroupns=host --network=host -v /sys:/sys -v /proc:/proc -v /run:/run huatuo/huatuo-bamai:latest 或从容器 /home/huatuo-bamai 路径下拷贝出所有文件后本地手动运行：\n$ ./huatuo-bamai --region example --config huatuo-bamai.conf 托管：可使用 systemd/supervisord/k8s-DaemonSet 等方式托管运行。\n3.3 配置 配置容器信息 HUATUO 通过调用 kubelet 接口获取POD/容器信息。你可以根据实际环境配置访问接口和证书，配置为空“”，表示禁用该功能。\n[Pod] KubeletPodListURL = \"http://127.0.0.1:10255/pods\" KubeletPodListHTTPSURL = \"https://127.0.0.1:10250/pods\" KubeletPodClientCertPath = \"/var/lib/kubelet/pki/kubelet-client-current.pem\" 配置存储 指标存储 (Metric): 所有的指标都存储在 prometheus，你可以通过访问 :19704/metrics 接口获取指标。\n异常事件存储 (Events, AutoTracing): 所有的内核事件，和 Autotracing 事件都存储在 ES。注意：如果配置为空表示不启动 ES 存储，只在本地目录 huatuo-local 存储事件。\nES 存储配置如下：\n[Storage.ES] Address = \"http://127.0.0.1:9200\" Username = \"elastic\" Password = \"huatuo-bamai\" Index = \"huatuo_bamai\" 本地存储配置如下：\n# tracer's record data # Path: all but the last element of path for per tracer # RotationSize: the maximum size in Megabytes of a record file before it gets rotated for per subsystem # MaxRotation: the maximum number of old log files to retain for per subsystem [Storage.LocalFile] Path = \"huatuo-local\" RotationSize = 100 MaxRotation = 10 事件阈值 所有的内核事件采集 Events 和 AutoTracing 都可以配置触发阈值。默认的阈值都是在实际生产环境反复验证后的经验数据，你可以根据自身需求，在 huatuo-bamai.conf 中修改阈值。\n资源限制 为保障物理机稳定性，我们对采集器进行了资源限制，其中 LimitInitCPU 表示采集器启动阶段占用的 CPU 资源，LimitCPU/LimitMem 表示采集器启动成功后常态占用的资源限制：\n[RuntimeCgroup] LimitInitCPU = 0.5 LimitCPU = 2.0 # limit memory (MB) LimitMem = 2048 ","categories":"","description":"","excerpt":" 为帮助大家快速体验、部署 HUATUO, 该文档分别从 极速体验，快速开始，编译部署 三部分说明。\n1 极速体验 该部分可以帮助你快速了解 …","ref":"/docs/zh/v2.1.0/gettingstarted/quick-start/","tags":"","title":"快速开始"},{"body":"框架提供了非常便捷的 API，包括模块启动、数据存储、容器信息、bpf 相关 （load, attach, read, detach, unload）等，用户可通过自定义的采集逻辑，灵活选择合适的采集模式和数据存储的方式。\ntracing 类型 根据实际场景，你可以在 core/autotracing 或 core/events 目录下实现接口 ITracingEvent 即可完成 tracing 类型的采集。\n// ITracingEvent represents a tracing/event type ITracingEvent interface { Start(ctx context.Context) error } 步骤如下：\ntype exampleTracing struct{} // 注册回调 func init() { tracing.RegisterEventTracing(\"example\", newExample) } // 创建 tracing func newExample() (*tracing.EventTracingAttr, error) { return \u0026tracing.EventTracingAttr{ TracingData: \u0026exampleTracing{}, Internal: 10, // 再次开启 tracing 的间隔时间 seconds Flag: tracing.FlagTracing, // 标记为 tracing 类型 }, nil } // 实现接口 ITracingEvent func (t *exampleTracing) Start(ctx context.Context) error { // do something ... // 存储数据到 ES 和 本地 storage.Save(\"example\", ccontainerID, time.Now(), tracerData) } // 也可同时实现接口 Collector 以 Prometheus 格式输出 （可选） func (c *exampleTracing) Update() ([]*metric.Data, error) { // from tracerData to prometheus.Metric ... return data, nil } Metric 类型 在 core/metrics 目录下添加接口 Collector 的实现即可完成 Metric 类型的采集。\ntype Collector interface { // Get new metrics and expose them via prometheus registry. Update() ([]*Data, error) } 步骤如下：\ntype exampleMetric struct{} // 注册回调 func init() { tracing.RegisterEventTracing(\"example\", newExample) } // 创建 Metric func newExample() (*tracing.EventTracingAttr, error) { return \u0026tracing.EventTracingAttr{ TracingData: \u0026filenrCollector{ metric: []*metric.Data{ metric.NewGaugeData(\"name1\", 0, \"description of example_name1\", nil), metric.NewGaugeData(\"name2\", 0, \"description of example_name2\", nil), }, }, Flag: tracing.FlagMetric, // 标记为 Metric 类型 }, nil } // 实现接口 Collector 以 Prometheus 格式输出 func (c *exampleMetric) Update() ([]*metric.Data, error) { // do something ... return data, nil } 在项目 core 目录下已集成了 3 个采集模块的多种实际场景的示例，包括 bpf 代码、map 数据交互、容器信息等，更多详情可参考对应代码实现。\n","categories":"","description":"","excerpt":"框架提供了非常便捷的 API，包括模块启动、数据存储、容器信息、bpf 相关 （load, attach, read, detach, …","ref":"/docs/zh/v2.1.0/tutorials/addcase/custom/","tags":"","title":"添加自定义采集"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/en/","tags":"","title":"Documentation"},{"body":"Overview The Metrics type is used to collect system performance and other indicator data. It can output in Prometheus format, serving as a data provider through the /metrics (curl localhost:\u003cport\u003e/metrics) .\nType：Metrics collection\nFunction：Collects performance metrics from various subsystems\nCharacteristics：\nMetrics are primarily used to collect system performance metrics such as CPU usage, memory usage, network statistics, etc. They are suitable for monitoring system performance and support real-time analysis and long-term trend observation. Metrics can come from regular procfs/sysfs collection or be generated from tracing types (autotracing, event). Outputs in Prometheus format for seamless integration into the Prometheus observability ecosystem. Already Integrated：\ncpu (sys, usr, util, load, nr_running…) memory（vmstat, memory_stat, directreclaim, asyncreclaim…） IO (d2c, q2c, freeze, flush…) Network（arp, socket mem, qdisc, netstat, netdev, socketstat…） How to Add Statistical Metrics Simply implement the Collector interface and complete registration to add metrics to the system.\ntype Collector interface { // Get new metrics and expose them via prometheus registry. Update() ([]*Data, error) } 1. Create a Structure Create a structure that implements the Collector interface in the core/metrics directory:\ntype exampleMetric struct{ } 2. Register Callback Function func init() { tracing.RegisterEventTracing(\"example\", newExample) } func newExample() (*tracing.EventTracingAttr, error) { return \u0026tracing.EventTracingAttr{ TracingData: \u0026exampleMetric{}, Flag: tracing.FlagMetric, // Mark as Metric type }, nil } 3. Implement the Update Method func (c *exampleMetric) Update() ([]*metric.Data, error) { // do something ... return []*metric.Data{ metric.NewGaugeData(\"example\", value, \"description of example\", nil), }, nil } The core/metrics directory in the project has integrated various practical Metrics examples, along with rich underlying interfaces provided by the framework, including BPF program and map data interaction, container information, etc. For more details, refer to the corresponding code implementations.\n","categories":"","description":"","excerpt":"Overview The Metrics type is used to collect system performance and …","ref":"/docs/en/latest/development/metrics/","tags":"","title":"Add Metrics"},{"body":"HUATUO provides the simplest DaemonSet deployment option to minimize setup complexity. Deploying the HUATUO collector via DaemonSet involves the following steps:\n1. Download the Collector Configuration File curl -L -o huatuo-bamai.conf https://github.com/ccfos/huatuo/raw/main/huatuo-bamai.conf Modify this configuration file according to your environment, such as kubelet connection settings and Elasticsearch settings.\n2. Create a ConfigMap kubectl create configmap huatuo-bamai-config --from-file=./huatuo-bamai.conf 3. Deploy the Collector kubectl apply -f huatuo-daemonset.minimal.yaml Contents of huatuo-daemonset.minimal.yaml:\napiVersion: apps/v1 kind: DaemonSet metadata: name: huatuo namespace: default labels: app: huatuo spec: selector: matchLabels: app: huatuo template: metadata: labels: app: huatuo spec: containers: - name: huatuo image: docker.io/huatuo/huatuo-bamai:latest resources: limits: cpu: '1' memory: 2Gi requests: cpu: 500m memory: 512Mi securityContext: privileged: true volumeMounts: - name: proc mountPath: /proc - name: sys mountPath: /sys - name: run mountPath: /run - name: var mountPath: /var - name: etc mountPath: /etc - name: huatuo-local mountPath: /home/huatuo-bamai/huatuo-local - name: huatuo-bamai-config-volume mountPath: /home/huatuo-bamai/conf/huatuo-bamai.conf subPath: huatuo-bamai.conf volumes: - name: proc hostPath: path: /proc - name: sys hostPath: path: /sys - name: run hostPath: path: /run - name: var hostPath: path: /var - name: etc hostPath: path: /etc - name: huatuo-local hostPath: path: /var/log/huatuo/huatuo-local type: DirectoryOrCreate - name: huatuo-bamai-config-volume configMap: name: huatuo-bamai-config hostNetwork: true hostPID: true ","categories":"","description":"","excerpt":"HUATUO provides the simplest DaemonSet deployment option to minimize …","ref":"/docs/en/latest/deployment/daemonset/","tags":"","title":"Daemonset"},{"body":"The HUATUO collector huatuo-bamai runs on physical machines or VMs. We provide both binary packages and Docker images, and you can deploy them in any custom way, such as:\nSystemd and DaemonSet deployments are recommended for production. Docker / Compose is suitable for development and quick validation scenarios. Binary Download The latest binary package provided is v2.1.0; the master branch is for reference only.\nv2.1.0: https://github.com/ccfos/huatuo/releases/tag/v2.1.0 Mirror Download Docker images are stored on Docker Hub by default (https://hub.docker.com/u/huatuo).\n","categories":"","description":"","excerpt":"The HUATUO collector huatuo-bamai runs on physical machines or VMs. We …","ref":"/docs/en/latest/deployment/","tags":"","title":"Deploy"},{"body":"HUATUO currently supports the following exception context capture events:\nEvent Name Core Functionality Scenarios softirq Detects delayed response or prolonged disabling of host soft interrupts, and outputs kernel call stacks and process information when soft interrupts are disabled for extended periods., etc. This type of issue severely impacts network transmission/reception, leading to business spikes or timeout issues dropwatch Detects TCP packet loss and outputs host and network context information when packet loss occurs This type of issue mainly causes business spikes and latency net_rx_latency Captures latency events in network receive path from driver, protocol stack, to user-space receive process For network latency issues in the receive direction where the exact delay location is unclear, net_rx_latency calculates latency at the driver, protocol stack, and user copy paths using skb NIC ingress timestamps, filters timeout packets via preset thresholds, and locates the delay position oom Detects OOM events on the host or within containers When OOM occurs at host level or container dimension, captures process information triggering OOM, killed process information, and container details to troubleshoot memory leaks, abnormal exits, etc. softlockup When a softlockup occurs on the system, collects target process information and CPU details, and retrieves kernel stack information from all CPUs System softlockup events hungtask Provides count of all D-state processes in the system and kernel stack information Used to locate transient D-state process scenarios, preserving the scene for later problem tracking memreclaim Records process information when memory reclamation exceeds time threshold When memory pressure is excessively high, if a process requests memory at this time, it may enter direct reclamation (synchronous phase), potentially causing business process stalls. Recording the direct reclamation entry time helps assess the severity of impact on the process netdev Detects network device status changes Network card flapping, slave abnormalities in bond environments, etc. lacp Detects LACP status changes Detects LACP negotiation status in bond mode 4 Detect the long-term disabling of soft interrupts Feature Introduction\nThe Linux kernel contains various contexts such as process context, interrupt context, soft interrupt context, and NMI context. These contexts may share data, so to ensure data consistency and correctness, kernel code might disable soft or hard interrupts. Theoretically, the duration of single interrupt or soft interrupt disabling shouldn’t be too long. However, high-frequency system calls entering kernel mode and frequently executing interrupt disabling can also create a “long-term disable” phenomenon, slowing down system response. Issues related to “long interrupt or soft interrupt disabling” are very subtle with limited troubleshooting methods, yet have significant impact, typically manifesting as receive data timeouts in business applications. For this scenario, we built BPF-based detection capabilities for long hardware and software interrupt disables.\nExample\nBelow is an example of captured instances with overly long disabling interrupts, automatically uploaded to ES:\n{ \"_index\": \"***_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-06-11T16:05:16.251152703+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"comm\": \"observe-agent\", \"stack\": \"stack:\\nscheduler_tick/ffffffffa471dbc0 [kernel]\\nupdate_process_times/ffffffffa4789240 [kernel]\\ntick_sched_handle.isra.8/ffffffffa479afa0 [kernel]\\ntick_sched_timer/ffffffffa479b000 [kernel]\\n__hrtimer_run_queues/ffffffffa4789b60 [kernel]\\nhrtimer_interrupt/ffffffffa478a610 [kernel]\\n__sysvec_apic_timer_interrupt/ffffffffa4661a60 [kernel]\\nasm_call_sysvec_on_stack/ffffffffa5201130 [kernel]\\nsysvec_apic_timer_interrupt/ffffffffa5090500 [kernel]\\nasm_sysvec_apic_timer_interrupt/ffffffffa5200d30 [kernel]\\ndump_stack/ffffffffa506335e [kernel]\\ndump_header/ffffffffa5058eb0 [kernel]\\noom_kill_process.cold.9/ffffffffa505921a [kernel]\\nout_of_memory/ffffffffa48a1740 [kernel]\\nmem_cgroup_out_of_memory/ffffffffa495ff70 [kernel]\\ntry_charge/ffffffffa4964ff0 [kernel]\\nmem_cgroup_charge/ffffffffa4968de0 [kernel]\\n__add_to_page_cache_locked/ffffffffa4895c30 [kernel]\\nadd_to_page_cache_lru/ffffffffa48961a0 [kernel]\\npagecache_get_page/ffffffffa4897ad0 [kernel]\\ngrab_cache_page_write_begin/ffffffffa4899d00 [kernel]\\niomap_write_begin/ffffffffa49fddc0 [kernel]\\niomap_write_actor/ffffffffa49fe980 [kernel]\\niomap_apply/ffffffffa49fbd20 [kernel]\\niomap_file_buffered_write/ffffffffa49fc040 [kernel]\\nxfs_file_buffered_aio_write/ffffffffc0f3bed0 [xfs]\\nnew_sync_write/ffffffffa497ffb0 [kernel]\\nvfs_write/ffffffffa4982520 [kernel]\\nksys_write/ffffffffa4982880 [kernel]\\ndo_syscall_64/ffffffffa508d190 [kernel]\\nentry_SYSCALL_64_after_hwframe/ffffffffa5200078 [kernel]\", \"now\": 5532940660025295, \"offtime\": 237328905, \"cpu\": 1, \"threshold\": 100000000, \"pid\": 688073 }, \"tracer_time\": \"2025-06-11 16:05:16.251 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-06-11 16:05:16.251 +0800\", \"region\": \"***\", \"tracer_name\": \"softirq\", \"es_index_time\": 1749629116268 }, \"fields\": { \"time\": [ \"2025-06-11T08:05:16.251Z\" ] }, \"_ignored\": [ \"tracer_data.stack\" ], \"_version\": 1, \"sort\": [ 1749629116251 ] } The local host also stores identical data:\n2025-06-11 16:05:16 *** Region=*** { \"hostname\": \"***\", \"region\": \"***\", \"uploaded_time\": \"2025-06-11T16:05:16.251152703+08:00\", \"time\": \"2025-06-11 16:05:16.251 +0800\", \"tracer_name\": \"softirq\", \"tracer_time\": \"2025-06-11 16:05:16.251 +0800\", \"tracer_type\": \"auto\", \"tracer_data\": { \"offtime\": 237328905, \"threshold\": 100000000, \"comm\": \"observe-agent\", \"pid\": 688073, \"cpu\": 1, \"now\": 5532940660025295, \"stack\": \"stack:\\nscheduler_tick/ffffffffa471dbc0 [kernel]\\nupdate_process_times/ffffffffa4789240 [kernel]\\ntick_sched_handle.isra.8/ffffffffa479afa0 [kernel]\\ntick_sched_timer/ffffffffa479b000 [kernel]\\n__hrtimer_run_queues/ffffffffa4789b60 [kernel]\\nhrtimer_interrupt/ffffffffa478a610 [kernel]\\n__sysvec_apic_timer_interrupt/ffffffffa4661a60 [kernel]\\nasm_call_sysvec_on_stack/ffffffffa5201130 [kernel]\\nsysvec_apic_timer_interrupt/ffffffffa5090500 [kernel]\\nasm_sysvec_apic_timer_interrupt/ffffffffa5200d30 [kernel]\\ndump_stack/ffffffffa506335e [kernel]\\ndump_header/ffffffffa5058eb0 [kernel]\\noom_kill_process.cold.9/ffffffffa505921a [kernel]\\nout_of_memory/ffffffffa48a1740 [kernel]\\nmem_cgroup_out_of_memory/ffffffffa495ff70 [kernel]\\ntry_charge/ffffffffa4964ff0 [kernel]\\nmem_cgroup_charge/ffffffffa4968de0 [kernel]\\n__add_to_page_cache_locked/ffffffffa4895c30 [kernel]\\nadd_to_page_cache_lru/ffffffffa48961a0 [kernel]\\npagecache_get_page/ffffffffa4897ad0 [kernel]\\ngrab_cache_page_write_begin/ffffffffa4899d00 [kernel]\\niomap_write_begin/ffffffffa49fddc0 [kernel]\\niomap_write_actor/ffffffffa49fe980 [kernel]\\niomap_apply/ffffffffa49fbd20 [kernel]\\niomap_file_buffered_write/ffffffffa49fc040 [kernel]\\nxfs_file_buffered_aio_write/ffffffffc0f3bed0 [xfs]\\nnew_sync_write/ffffffffa497ffb0 [kernel]\\nvfs_write/ffffffffa4982520 [kernel]\\nksys_write/ffffffffa4982880 [kernel]\\ndo_syscall_64/ffffffffa508d190 [kernel]\\nentry_SYSCALL_64_after_hwframe/ffffffffa5200078 [kernel]\" } } Protocol Stack Packet Loss Detection Feature Introduction\nDuring packet transmission and reception, packets may be lost due to various reasons, potentially causing business request delays or even timeouts. dropwatch uses eBPF to observe kernel network packet discards, outputting packet loss network context such as source/destination addresses, source/destination ports, seq, seqack, pid, comm, stack information, etc. dropwatch mainly detects TCP protocol-related packet loss, using pre-set probes to filter packets and determine packet loss locations for root cause analysis.\nExample\nInformation captured by dropwatch is automatically uploaded to ES. Below is an example where kubelet failed to send data packet due to device packet loss:\n{ \"_index\": \"***_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-06-11T16:58:15.100223795+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"comm\": \"kubelet\", \"stack\": \"kfree_skb/ffffffff9a0cd5c0 [kernel]\\nkfree_skb/ffffffff9a0cd5c0 [kernel]\\nkfree_skb_list/ffffffff9a0cd670 [kernel]\\n__dev_queue_xmit/ffffffff9a0ea020 [kernel]\\nip_finish_output2/ffffffff9a18a720 [kernel]\\n__ip_queue_xmit/ffffffff9a18d280 [kernel]\\n__tcp_transmit_skb/ffffffff9a1ad890 [kernel]\\ntcp_connect/ffffffff9a1ae610 [kernel]\\ntcp_v4_connect/ffffffff9a1b3450 [kernel]\\n__inet_stream_connect/ffffffff9a1d25f0 [kernel]\\ninet_stream_connect/ffffffff9a1d2860 [kernel]\\n__sys_connect/ffffffff9a0c1170 [kernel]\\n__x64_sys_connect/ffffffff9a0c1240 [kernel]\\ndo_syscall_64/ffffffff9a2ea9f0 [kernel]\\nentry_SYSCALL_64_after_hwframe/ffffffff9a400078 [kernel]\", \"saddr\": \"10.79.68.62\", \"pid\": 1687046, \"type\": \"common_drop\", \"queue_mapping\": 11, \"dport\": 2052, \"pkt_len\": 74, \"ack_seq\": 0, \"daddr\": \"10.179.142.26\", \"state\": \"SYN_SENT\", \"src_hostname\": \"***\", \"sport\": 15402, \"dest_hostname\": \"***\", \"seq\": 1902752773, \"max_ack_backlog\": 0 }, \"tracer_time\": \"2025-06-11 16:58:15.099 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-06-11 16:58:15.099 +0800\", \"region\": \"***\", \"tracer_name\": \"dropwatch\", \"es_index_time\": 1749632295120 }, \"fields\": { \"time\": [ \"2025-06-11T08:58:15.099Z\" ] }, \"_ignored\": [ \"tracer_data.stack\" ], \"_version\": 1, \"sort\": [ 1749632295099 ] } The local host also stores identical data:\n2025-06-11 16:58:15 Host=*** Region=*** { \"hostname\": \"***\", \"region\": \"***\", \"uploaded_time\": \"2025-06-11T16:58:15.100223795+08:00\", \"time\": \"2025-06-11 16:58:15.099 +0800\", \"tracer_name\": \"dropwatch\", \"tracer_time\": \"2025-06-11 16:58:15.099 +0800\", \"tracer_type\": \"auto\", \"tracer_data\": { \"type\": \"common_drop\", \"comm\": \"kubelet\", \"pid\": 1687046, \"saddr\": \"10.79.68.62\", \"daddr\": \"10.179.142.26\", \"sport\": 15402, \"dport\": 2052, \"src_hostname\": ***\", \"dest_hostname\": \"***\", \"max_ack_backlog\": 0, \"seq\": 1902752773, \"ack_seq\": 0, \"queue_mapping\": 11, \"pkt_len\": 74, \"state\": \"SYN_SENT\", \"stack\": \"kfree_skb/ffffffff9a0cd5c0 [kernel]\\nkfree_skb/ffffffff9a0cd5c0 [kernel]\\nkfree_skb_list/ffffffff9a0cd670 [kernel]\\n__dev_queue_xmit/ffffffff9a0ea020 [kernel]\\nip_finish_output2/ffffffff9a18a720 [kernel]\\n__ip_queue_xmit/ffffffff9a18d280 [kernel]\\n__tcp_transmit_skb/ffffffff9a1ad890 [kernel]\\ntcp_connect/ffffffff9a1ae610 [kernel]\\ntcp_v4_connect/ffffffff9a1b3450 [kernel]\\n__inet_stream_connect/ffffffff9a1d25f0 [kernel]\\ninet_stream_connect/ffffffff9a1d2860 [kernel]\\n__sys_connect/ffffffff9a0c1170 [kernel]\\n__x64_sys_connect/ffffffff9a0c1240 [kernel]\\ndo_syscall_64/ffffffff9a2ea9f0 [kernel]\\nentry_SYSCALL_64_after_hwframe/ffffffff9a400078 [kernel]\" } } Protocol Stack Receive Latency Feature Introduction\nOnline business network latency issues are difficult to locate, as problems can occur in any direction or stage. For example, receive direction latency might be caused by issues in drivers, protocol stack, or user programs. Therefore, we developed net_rx_latency detection functionality, leveraging skb NIC ingress timestamps to check latency at driver, protocol stack, and user-space layers. When receive latency reaches thresholds, eBPF captures network context information (five-tuple, latency location, process info, etc.). Receive path: NIC -\u003e Driver -\u003e Protocol Stack -\u003e User Active Receive\nExample\nA business container received packets from the kernel with a latency over 90 seconds, tracked via net_rx_latency, ES query output:\n{ \"_index\": \"***_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"tracer_data\": { \"dport\": 49000, \"pkt_len\": 26064, \"comm\": \"nginx\", \"ack_seq\": 689410995, \"saddr\": \"10.156.248.76\", \"pid\": 2921092, \"where\": \"TO_USER_COPY\", \"state\": \"ESTABLISHED\", \"daddr\": \"10.134.72.4\", \"sport\": 9213, \"seq\": 1009085774, \"latency_ms\": 95973 }, \"container_host_namespace\": \"***\", \"container_hostname\": \"***.docker\", \"es_index_time\": 1749628496541, \"uploaded_time\": \"2025-06-11T15:54:56.404864955+08:00\", \"hostname\": \"***\", \"container_type\": \"normal\", \"tracer_time\": \"2025-06-11 15:54:56.404 +0800\", \"time\": \"2025-06-11 15:54:56.404 +0800\", \"region\": \"***\", \"container_level\": \"1\", \"container_id\": \"***\", \"tracer_name\": \"net_rx_latency\" }, \"fields\": { \"time\": [ \"2025-06-11T07:54:56.404Z\" ] }, \"_version\": 1, \"sort\": [ 1749628496404 ] } The local host also stores identical data:\n2025-06-11 15:54:46 Host=*** Region=*** ContainerHost=***.docker ContainerID=*** ContainerType=normal ContainerLevel=1 { \"hostname\": \"***\", \"region\": \"***\", \"container_id\": \"***\", \"container_hostname\": \"***.docker\", \"container_host_namespace\": \"***\", \"container_type\": \"normal\", \"container_level\": \"1\", \"uploaded_time\": \"2025-06-11T15:54:46.129136232+08:00\", \"time\": \"2025-06-11 15:54:46.129 +0800\", \"tracer_time\": \"2025-06-11 15:54:46.129 +0800\", \"tracer_name\": \"net_rx_latency\", \"tracer_data\": { \"comm\": \"nginx\", \"pid\": 2921092, \"where\": \"TO_USER_COPY\", \"latency_ms\": 95973, \"state\": \"ESTABLISHED\", \"saddr\": \"10.156.248.76\", \"daddr\": \"10.134.72.4\", \"sport\": 9213, \"dport\": 49000, \"seq\": 1009024958, \"ack_seq\": 689410995, \"pkt_len\": 20272 } } Host/Container Memory Overused Feature Introduction\nWhen programs request more memory than available system or process limits during runtime, it can cause system or application crashes. Common in memory leaks, big data processing, or insufficient resource configuration scenarios. By inserting BPF hooks in the OOM kernel flow, detailed OOM context information is captured and passed to user space, including process information, killed process information, and container details.\nExample\nWhen OOM occurs in a container, captured information:\n{ \"_index\": \"***_cases_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-06-11T17:09:07.236482841+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"victim_process_name\": \"java\", \"trigger_memcg_css\": \"0xff4b8d8be3818000\", \"victim_container_hostname\": \"***.docker\", \"victim_memcg_css\": \"0xff4b8d8be3818000\", \"trigger_process_name\": \"java\", \"victim_pid\": 3218745, \"trigger_pid\": 3218804, \"trigger_container_hostname\": \"***.docker\", \"victim_container_id\": \"***\", \"trigger_container_id\": \"***\", \"tracer_time\": \"2025-06-11 17:09:07.236 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-06-11 17:09:07.236 +0800\", \"region\": \"***\", \"tracer_name\": \"oom\", \"es_index_time\": 1749632947258 }, \"fields\": { \"time\": [ \"2025-06-11T09:09:07.236Z\" ] }, \"_version\": 1, \"sort\": [ 1749632947236 ] } Additionally, oom event implements Collector interface, which enables collecting statistics on host OOM occurrences via Prometheus, distinguishing between events from the host and containers.\nKernel Softlockup Feature Introduction\nSoftlockup is an abnormal state detected by the Linux kernel where a kernel thread (or process) on a CPU core occupies the CPU for a long time without scheduling, preventing the system from responding normally to other tasks. Causes include kernel code bugs, CPU overload, device driver issues, and others. When a softlockup occurs in the system, information about the target process and CPU is collected, kernel stack information from all CPUs is retrieved, and the number of occurrences of the issue is recorded.\nProcess Blocking Feature Introduction\nA D-state process (also known as Uninterruptible Sleep) is a special process state indicating that the process is blocked while waiting for certain system resources and cannot be awakened by signals or external interrupts. Common scenarios include disk I/O operations, kernel blocking, hardware failures, etc. hungtask captures the kernel stacks of all D-state processes within the system and records the count of such processes. It is used to locate transient scenarios where D-state processes appear momentarily, enabling root cause analysis even after the scenario has resolved.\nExample\n{ \"_index\": \"***_2025-06-10\", \"_type\": \"_doc\", \"_id\": \"8yyOV5cBGoYArUxjSdvr\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-06-10T09:57:12.202191192+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"cpus_stack\": \"2025-06-10 09:57:14 sysrq: Show backtrace of all active CPUs\\n2025-06-10 09:57:14 NMI backtrace for cpu 33\\n2025-06-10 09:57:14 CPU: 33 PID: 768309 Comm: huatuo-bamai Kdump: loaded Tainted: G S W OEL 5.10.0-216.0.0.115.v1.0.x86_64 #1\\n2025-06-10 09:57:14 Hardware name: Inspur SA5212M5/YZMB-00882-104, BIOS 4.1.12 11/27/2019\\n2025-06-10 09:57:14 Call Trace:\\n2025-06-10 09:57:14 dump_stack+0x57/0x6e\\n2025-06-10 09:57:14 nmi_cpu_backtrace.cold.0+0x30/0x65\\n2025-06-10 09:57:14 ? lapic_can_unplug_cpu+0x80/0x80\\n2025-06-10 09:57:14 nmi_trigger_cpumask_backtrace+0xdf/0xf0\\n2025-06-10 09:57:14 arch_trigger_cpumask_backtrace+0x15/0x20\\n2025-06-10 09:57:14 sysrq_handle_showallcpus+0x14/0x90\\n2025-06-10 09:57:14 __handle_sysrq.cold.8+0x77/0xe8\\n2025-06-10 09:57:14 write_sysrq_trigger+0x3d/0x60\\n2025-06-10 09:57:14 proc_reg_write+0x38/0x80\\n2025-06-10 09:57:14 vfs_write+0xdb/0x250\\n2025-06-10 09:57:14 ksys_write+0x59/0xd0\\n2025-06-10 09:57:14 do_syscall_64+0x39/0x80\\n2025-06-10 09:57:14 entry_SYSCALL_64_after_hwframe+0x62/0xc7\\n2025-06-10 09:57:14 RIP: 0033:0x4088ae\\n2025-06-10 09:57:14 Code: 48 83 ec 38 e8 13 00 00 00 48 83 c4 38 5d c3 cc cc cc cc cc cc cc cc cc cc cc cc cc 49 89 f2 48 89 fa 48 89 ce 48 89 df 0f 05 \u003c48\u003e 3d 01 f0 ff ff 76 15 48 f7 d8 48 89 c1 48 c7 c0 ff ff ff ff 48\\n2025-06-10 09:57:14 RSP: 002b:000000c000adcc60 EFLAGS: 00000212 ORIG_RAX: 0000000000000001\\n2025-06-10 09:57:14 RAX: ffffffffffffffda RBX: 0000000000000013 RCX: 00000000004088ae\\n2025-06-10 09:57:14 RDX: 0000000000000001 RSI: 000000000274ab18 RDI: 0000000000000013\\n2025-06-10 09:57:14 RBP: 000000c000adcca0 R08: 0000000000000000 R09: 0000000000000000\\n2025-06-10 09:57:14 R10: 0000000000000000 R11: 0000000000000212 R12: 000000c000adcdc0\\n2025-06-10 09:57:14 R13: 0000000000000002 R14: 000000c000caa540 R15: 0000000000000000\\n2025-06-10 09:57:14 Sending NMI from CPU 33 to CPUs 0-32,34-95:\\n2025-06-10 09:57:14 NMI backtrace for cpu 52 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 54 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 7 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 81 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 60 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 2 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 21 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 69 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 58 skipped: idling at intel_idle+0x6f/ ... \"pid\": 2567042 }, \"tracer_time\": \"2025-06-10 09:57:12.202 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-06-10 09:57:12.202 +0800\", \"region\": \"***\", \"tracer_name\": \"hungtask\", \"es_index_time\": 1749520632297 }, \"fields\": { \"time\": [ \"2025-06-10T01:57:12.202Z\" ] }, \"_ignored\": [ \"tracer_data.blocked_processes_stack\", \"tracer_data.cpus_stack\" ], \"_version\": 1, \"sort\": [ 1749520632202 ] } Additionally, the hungtask event implements the Collector interface, which also enables collecting statistics on host hungtask occurrences via Prometheus.\nContainer/Host Memory Reclamation Feature Introduction\nWhen memory pressure is excessively high, if a process requests memory at this time, it may enter direct reclamation. This phase involves synchronous reclamation and may cause business process stalls. Recording the time when a process enters direct reclamation helps us assess the severity of impact from direct reclamation on that process. The memreclaim event calculates whether the same process remains in direct reclamation for over 900ms within a 1-second cycle; if so, it records the process’s contextual information.\nExample\nWhen a business container’s chrome process enters direct reclamation, the ES query output is as follows:\n{ \"_index\": \"***_cases_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"tracer_data\": { \"comm\": \"chrome\", \"deltatime\": 1412702917, \"pid\": 1896137 }, \"container_host_namespace\": \"***\", \"container_hostname\": \"***.docker\", \"es_index_time\": 1749641583290, \"uploaded_time\": \"2025-06-11T19:33:03.26754495+08:00\", \"hostname\": \"***\", \"container_type\": \"normal\", \"tracer_time\": \"2025-06-11 19:33:03.267 +0800\", \"time\": \"2025-06-11 19:33:03.267 +0800\", \"region\": \"***\", \"container_level\": \"102\", \"container_id\": \"921d0ec0a20c\", \"tracer_name\": \"directreclaim\" }, \"fields\": { \"time\": [ \"2025-06-11T11:33:03.267Z\" ] }, \"_version\": 1, \"sort\": [ 1749641583267 ] } Network Device Status Feature Introduction\nNetwork card status changes often cause severe network issues, directly impacting overall host network quality, such as down/up states, MTU changes, etc. Taking the down state as an example, possible causes include operations by privileged processes, underlying cable issues, optical module failures, peer switch problems, etc. The netdev event is designed to detect network device status changes and currently implements monitoring for network card down/up events, distinguishing between administrator-initiated and underlying cause-induced status changes.\nExample\nWhen an administrator operation causes the eth1 network card to go down, the ES query event output is as follows:\n{ \"_index\": \"***_cases_2025-05-30\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-05-30T17:47:50.406913037+08:00\", \"hostname\": \"localhost.localdomain\", \"tracer_data\": { \"ifname\": \"eth1\", \"start\": false, \"index\": 3, \"linkstatus\": \"linkStatusAdminDown, linkStatusCarrierDown\", \"mac\": \"5c:6f:69:34:dc:72\" }, \"tracer_time\": \"2025-05-30 17:47:50.406 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-05-30 17:47:50.406 +0800\", \"region\": \"***\", \"tracer_name\": \"netdev_event\", \"es_index_time\": 1748598470407 }, \"fields\": { \"time\": [ \"2025-05-30T09:47:50.406Z\" ] }, \"_version\": 1, \"sort\": [ 1748598470406 ] } LACP Protocol Status Feature Introduction\nBond is a technology provided by the Linux system kernel that bundles multiple physical network interfaces into a single logical interface. Through bonding, bandwidth aggregation, failover, or load balancing can be achieved. LACP is a protocol defined by the IEEE 802.3ad standard for dynamically managing Link Aggregation Groups (LAG). Currently, there is no elegant method to obtain physical host LACP protocol negotiation exception events. HUATUO implements the lacp event, which uses BPF to instrument key protocol paths. When a change in link aggregation status is detected, it triggers an event to record relevant information.\nExample\nWhen the host network card eth1 experiences physical layer down/up fluctuations, the LACP dynamic negotiation status becomes abnormal. The ES query output is as follows:\n{ \"_index\": \"***_cases_2025-05-30\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-05-30T17:47:48.513318579+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"content\": \"/proc/net/bonding/bond0\\nEthernet Channel Bonding Driver: v4.18.0 (Apr 7, 2025)\\n\\nBonding Mode: load balancing (round-robin)\\nMII Status: down\\nMII Polling Interval (ms): 0\\nUp Delay (ms): 0\\nDown Delay (ms): 0\\nPeer Notification Delay (ms): 0\\n/proc/net/bonding/bond4\\nEthernet Channel Bonding Driver: v4.18.0 (Apr 7, 2025)\\n\\nBonding Mode: IEEE 802.3ad Dynamic link aggregation\\nTransmit Hash Policy: layer3+4 (1)\\nMII Status: up\\nMII Polling Interval (ms): 100\\nUp Delay (ms): 0\\nDown Delay (ms): 0\\nPeer Notification Delay (ms): 1000\\n\\n802.3ad info\\nLACP rate: fast\\nMin links: 0\\nAggregator selection policy (ad_select): stable\\nSystem priority: 65535\\nSystem MAC address: 5c:6f:69:34:dc:72\\nActive Aggregator Info:\\n\\tAggregator ID: 1\\n\\tNumber of ports: 2\\n\\tActor Key: 21\\n\\tPartner Key: 50013\\n\\tPartner Mac Address: 00:00:5e:00:01:01\\n\\nSlave Interface: eth0\\nMII Status: up\\nSpeed: 25000 Mbps\\nDuplex: full\\nLink Failure Count: 0\\nPermanent HW addr: 5c:6f:69:34:dc:72\\nSlave queue ID: 0\\nSlave active: 1\\nSlave sm_vars: 0x172\\nAggregator ID: 1\\nAggregator active: 1\\nActor Churn State: none\\nPartner Churn State: none\\nActor Churned Count: 0\\nPartner Churned Count: 0\\ndetails actor lacp pdu:\\n system priority: 65535\\n system mac address: 5c:6f:69:34:dc:72\\n port key: 21\\n port priority: 255\\n port number: 1\\n port state: 63\\ndetails partner lacp pdu:\\n system priority: 200\\n system mac address: 00:00:5e:00:01:01\\n oper key: 50013\\n port priority: 32768\\n port number: 16397\\n port state: 63\\n\\nSlave Interface: eth1\\nMII Status: up\\nSpeed: 25000 Mbps\\nDuplex: full\\nLink Failure Count: 17\\nPermanent HW addr: 5c:6f:69:34:dc:73\\nSlave queue ID: 0\\nSlave active: 0\\nSlave sm_vars: 0x172\\nAggregator ID: 1\\nAggregator active: 1\\nActor Churn State: monitoring\\nPartner Churn State: monitoring\\nActor Churned Count: 2\\nPartner Churned Count: 2\\ndetails actor lacp pdu:\\n system priority: 65535\\n system mac address: 5c:6f:69:34:dc:72\\n port key: 21\\n port priority: 255\\n port number: 2\\n port state: 15\\ndetails partner lacp pdu:\\n system priority: 200\\n system mac address: 00:00:5e:00:01:01\\n oper key: 50013\\n port priority: 32768\\n port number: 32781\\n port state: 31\\n\" }, \"tracer_time\": \"2025-05-30 17:47:48.513 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-05-30 17:47:48.513 +0800\", \"region\": \"***\", \"tracer_name\": \"lacp\", \"es_index_time\": 1748598468514 }, \"fields\": { \"time\": [ \"2025-05-30T09:47:48.513Z\" ] }, \"_ignored\": [ \"tracer_data.content\" ], \"_version\": 1, \"sort\": [ 1748598468513 ] } ","categories":"","description":"","excerpt":"HUATUO currently supports the following exception context capture …","ref":"/docs/en/latest/key-feature/events/","tags":"","title":"Events"},{"body":"通过 K8s daemonset 方式在云原生集群部署。\n1. 获取配置文件 curl -L -o huatuo-bamai.conf https://github.com/ccfos/huatuo/raw/main/huatuo-bamai.conf 根据实际环境修改配置，如kubelet 和 elasticsrearch 的相关配置。\n2. 创建 configmap kubectl create configmap huatuo-bamai-config --from-file=./huatuo-bamai.conf 3. 部署采集器 kubectl apply -f https://github.com/ccfos/huatuo/blob/main/build/huatuo-daemonset.minimal.yaml huatuo-daemonset.minimal.yaml：\napiVersion: apps/v1 kind: DaemonSet metadata: name: huatuo namespace: default labels: app: huatuo spec: selector: matchLabels: app: huatuo template: metadata: labels: app: huatuo spec: containers: - name: huatuo image: docker.io/huatuo/huatuo-bamai:latest resources: limits: cpu: '1' memory: 2Gi requests: cpu: 500m memory: 512Mi securityContext: privileged: true volumeMounts: - name: proc mountPath: /proc - name: sys mountPath: /sys - name: run mountPath: /run - name: var mountPath: /var - name: etc mountPath: /etc - name: huatuo-local mountPath: /home/huatuo-bamai/huatuo-local - name: huatuo-bamai-config-volume mountPath: /home/huatuo-bamai/conf/huatuo-bamai.conf subPath: huatuo-bamai.conf volumes: - name: proc hostPath: path: /proc - name: sys hostPath: path: /sys - name: run hostPath: path: /run - name: var hostPath: path: /var - name: etc hostPath: path: /etc - name: huatuo-local hostPath: path: /var/log/huatuo/huatuo-local type: DirectoryOrCreate - name: huatuo-bamai-config-volume configMap: name: huatuo-bamai-config hostNetwork: true hostPID: true ","categories":"","description":"","excerpt":"通过 K8s daemonset 方式在云原生集群部署。\n1. 获取配置文件 curl -L -o huatuo-bamai.conf …","ref":"/docs/zh/latest/deployment/daemonset/","tags":"","title":"K8s Daemonset 部署"},{"body":"当前版本支持 Linux 内核事件如下：\n事件名称 核心功能 场景 softirq 检测内核关闭中断时间过长，输出关闭软中断的内核调用栈，进程信息等 解决系统异常，应用异常导致的系统卡顿，网络延迟，调度延迟等问题。 softlockup 检测系统 softlockup 事件，提供目标进程，cpu 内核栈信息等 解决系统 softlockup 问题。 hungtask 检测系统 hungtask 事件，提供系统内所有 D 状态进程、栈信息等 定位瞬时批量出现 D 进程的场景，保留故障现场便于后期问题跟踪。 oom 检测宿主或容器内 oom 事件 聚焦物理机或者容器内存耗尽问题，输出更详细故障快照，解决业务因内存不可用导致的故障。 memory_reclaim_events 检测系统内存直接回收事件，记录直接回收耗时，进程，容器等信息 解决系统因内存压力过大，导致的业务进程的卡顿等场景。 ras 检测 CPU、Memory、PCIe 等硬件故障事件，输出具体详细故障信息 及时感知和预测硬件故障，降低因物理硬件不可用导致的业务有损问题。 dropwatch 检测内核网络协议栈丢包问题，输出丢包调用栈、网络信息等 解决协议栈丢包导致的业务毛刺和延迟等问题。 net_rx_latency 检测协议栈收方向驱动、协议、用户主动收过程的延迟事件 解决因协议栈接收延迟，应用响应接收延迟等导致的业务超时，毛刺等问题。 netdev_events 检测网卡链路状态变化，输出具体类型 感知网卡物理链路状态，解决因网卡故障导致的业务不可用问题。 netdev_bonding_lacp 检测 bonding lacp 协议状态变化，记录详细事件信息 解决 bonding lacp 模式下协议协商问题，界定物理机，交换机故障边界。 netdev_txqueue_timeout 检测网卡发送队列超时事件 定位网卡发送队列硬件故障问题。 软中断关闭 Linux 内核存在进程上下文，中断上下文，软中断上下文，NMI 上下文等概念，这些上下文之间可能存在共享数据情况，因此为了确保数据的一致性，正确性，内核代码可能会关闭软中断或者硬中断。从理论角度，单次关闭中断或者软中断时间不能太长，但高频的系统调用，陷入内核态频繁执行关闭中断或软中断，同样会造\"长时间关闭\"的现象，拖慢了系统的响应。“关闭中断，软中断时间过长”这类问题非常隐蔽，且定位手段有限，同时影响又非常大，体现在业务应用上一般为接收数据超时。针对这种场景我们基于BPF技术构建了检测硬件中断，软件中断关闭过长的能力。\n如下为抓取到的关闭中断过长的实例，这些信息被自动存储 ElasticSearch, 或者物理机磁盘文件.\n{ \"_index\": \"***_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-06-11T16:05:16.251152703+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"comm\": \"observe-agent\", \"stack\": \"stack:\\nscheduler_tick/ffffffffa471dbc0 [kernel]\\nupdate_process_times/ffffffffa4789240 [kernel]\\ntick_sched_handle.isra.8/ffffffffa479afa0 [kernel]\\ntick_sched_timer/ffffffffa479b000 [kernel]\\n__hrtimer_run_queues/ffffffffa4789b60 [kernel]\\nhrtimer_interrupt/ffffffffa478a610 [kernel]\\n__sysvec_apic_timer_interrupt/ffffffffa4661a60 [kernel]\\nasm_call_sysvec_on_stack/ffffffffa5201130 [kernel]\\nsysvec_apic_timer_interrupt/ffffffffa5090500 [kernel]\\nasm_sysvec_apic_timer_interrupt/ffffffffa5200d30 [kernel]\\ndump_stack/ffffffffa506335e [kernel]\\ndump_header/ffffffffa5058eb0 [kernel]\\noom_kill_process.cold.9/ffffffffa505921a [kernel]\\nout_of_memory/ffffffffa48a1740 [kernel]\\nmem_cgroup_out_of_memory/ffffffffa495ff70 [kernel]\\ntry_charge/ffffffffa4964ff0 [kernel]\\nmem_cgroup_charge/ffffffffa4968de0 [kernel]\\n__add_to_page_cache_locked/ffffffffa4895c30 [kernel]\\nadd_to_page_cache_lru/ffffffffa48961a0 [kernel]\\npagecache_get_page/ffffffffa4897ad0 [kernel]\\ngrab_cache_page_write_begin/ffffffffa4899d00 [kernel]\\niomap_write_begin/ffffffffa49fddc0 [kernel]\\niomap_write_actor/ffffffffa49fe980 [kernel]\\niomap_apply/ffffffffa49fbd20 [kernel]\\niomap_file_buffered_write/ffffffffa49fc040 [kernel]\\nxfs_file_buffered_aio_write/ffffffffc0f3bed0 [xfs]\\nnew_sync_write/ffffffffa497ffb0 [kernel]\\nvfs_write/ffffffffa4982520 [kernel]\\nksys_write/ffffffffa4982880 [kernel]\\ndo_syscall_64/ffffffffa508d190 [kernel]\\nentry_SYSCALL_64_after_hwframe/ffffffffa5200078 [kernel]\", \"now\": 5532940660025295, \"offtime\": 237328905, \"cpu\": 1, \"threshold\": 100000000, \"pid\": 688073 }, \"tracer_time\": \"2025-06-11 16:05:16.251 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-06-11 16:05:16.251 +0800\", \"region\": \"***\", \"tracer_name\": \"softirq\", \"es_index_time\": 1749629116268 }, \"fields\": { \"time\": [ \"2025-06-11T08:05:16.251Z\" ] }, \"_ignored\": [ \"tracer_data.stack\" ], \"_version\": 1, \"sort\": [ 1749629116251 ] } 协议栈丢包 在数据包收发过程中由于各类原因，可能出现丢包的现象，丢包可能会导致业务请求延迟，甚至超时。dropwatch 借助 eBPF 观测内核网络数据包丢弃情况，输出丢包网络上下文，如：源目的地址，源目的端口，seq, seqack, pid, comm, stack 信息等。dorpwatch 主要用于检测 TCP 协议相关的丢包，通过预先埋点过滤数据包，确定丢包位置以便于排查丢包根因。如下为抓取到的一案例：kubelet 在发送 SYN 时，由于设备丢包，导致数据包发送失败。\n{ \"_index\": \"***_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-06-11T16:58:15.100223795+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"comm\": \"kubelet\", \"stack\": \"kfree_skb/ffffffff9a0cd5c0 [kernel]\\nkfree_skb/ffffffff9a0cd5c0 [kernel]\\nkfree_skb_list/ffffffff9a0cd670 [kernel]\\n__dev_queue_xmit/ffffffff9a0ea020 [kernel]\\nip_finish_output2/ffffffff9a18a720 [kernel]\\n__ip_queue_xmit/ffffffff9a18d280 [kernel]\\n__tcp_transmit_skb/ffffffff9a1ad890 [kernel]\\ntcp_connect/ffffffff9a1ae610 [kernel]\\ntcp_v4_connect/ffffffff9a1b3450 [kernel]\\n__inet_stream_connect/ffffffff9a1d25f0 [kernel]\\ninet_stream_connect/ffffffff9a1d2860 [kernel]\\n__sys_connect/ffffffff9a0c1170 [kernel]\\n__x64_sys_connect/ffffffff9a0c1240 [kernel]\\ndo_syscall_64/ffffffff9a2ea9f0 [kernel]\\nentry_SYSCALL_64_after_hwframe/ffffffff9a400078 [kernel]\", \"saddr\": \"10.79.68.62\", \"pid\": 1687046, \"type\": \"common_drop\", \"queue_mapping\": 11, \"dport\": 2052, \"pkt_len\": 74, \"ack_seq\": 0, \"daddr\": \"10.179.142.26\", \"state\": \"SYN_SENT\", \"src_hostname\": \"***\", \"sport\": 15402, \"dest_hostname\": \"***\", \"seq\": 1902752773, \"max_ack_backlog\": 0 }, \"tracer_time\": \"2025-06-11 16:58:15.099 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-06-11 16:58:15.099 +0800\", \"region\": \"***\", \"tracer_name\": \"dropwatch\", \"es_index_time\": 1749632295120 }, \"fields\": { \"time\": [ \"2025-06-11T08:58:15.099Z\" ] }, \"_ignored\": [ \"tracer_data.stack\" ], \"_version\": 1, \"sort\": [ 1749632295099 ] } 协议栈延迟 线上业务网络延迟问题是比较难定位的，任何方向，任何的阶段都有可能出现问题。比如收方向的延迟，驱动、协议栈、用户程序等都有可能出现问题，因此我们开发了 net_rx_latency 检测功能，借助 skb 入网卡的时间戳，在驱动，协议栈层，用户态层检查延迟时间，当收包延迟达到阈值时，借助 eBPF 获取网络上下文信息（五元组、延迟位置、进程信息等）。收方向传输路径示意：网卡 -\u003e 驱动 -\u003e 协议栈 -\u003e 用户主动收。业务容器从内核收包延迟超过 90s，通过 net_rx_latency 追踪，输出如下：\n{ \"_index\": \"***_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"tracer_data\": { \"dport\": 49000, \"pkt_len\": 26064, \"comm\": \"nginx\", \"ack_seq\": 689410995, \"saddr\": \"10.156.248.76\", \"pid\": 2921092, \"where\": \"TO_USER_COPY\", \"state\": \"ESTABLISHED\", \"daddr\": \"10.134.72.4\", \"sport\": 9213, \"seq\": 1009085774, \"latency_ms\": 95973 }, \"container_host_namespace\": \"***\", \"container_hostname\": \"***.docker\", \"es_index_time\": 1749628496541, \"uploaded_time\": \"2025-06-11T15:54:56.404864955+08:00\", \"hostname\": \"***\", \"container_type\": \"normal\", \"tracer_time\": \"2025-06-11 15:54:56.404 +0800\", \"time\": \"2025-06-11 15:54:56.404 +0800\", \"region\": \"***\", \"container_level\": \"1\", \"container_id\": \"***\", \"tracer_name\": \"net_rx_latency\" }, \"fields\": { \"time\": [ \"2025-06-11T07:54:56.404Z\" ] }, \"_version\": 1, \"sort\": [ 1749628496404 ] } out_of_memory 程序运行时申请的内存超过了系统或进程可用的内存上限，导致系统或应用程序崩溃。常见于内存泄漏、大数据处理或资源配置不足的场景。通过在 oom 的内核流程插入 BPF 钩子，获取 oom 上下文的详细信息并传递到用户态。这些信息包括进程信息、被 kill 的进程信息、容器信息。\n{ \"_index\": \"***_cases_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-06-11T17:09:07.236482841+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"victim_process_name\": \"java\", \"trigger_memcg_css\": \"0xff4b8d8be3818000\", \"victim_container_hostname\": \"***.docker\", \"victim_memcg_css\": \"0xff4b8d8be3818000\", \"trigger_process_name\": \"java\", \"victim_pid\": 3218745, \"trigger_pid\": 3218804, \"trigger_container_hostname\": \"***.docker\", \"victim_container_id\": \"***\", \"trigger_container_id\": \"***\", \"tracer_time\": \"2025-06-11 17:09:07.236 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-06-11 17:09:07.236 +0800\", \"region\": \"***\", \"tracer_name\": \"oom\", \"es_index_time\": 1749632947258 }, \"fields\": { \"time\": [ \"2025-06-11T09:09:07.236Z\" ] }, \"_version\": 1, \"sort\": [ 1749632947236 ] } softlockup softlockup 是 Linux 内核检测到的一种异常状态，指某个 CPU 核心上的内核线程（或进程）长时间占用 CPU 且不调度，导致系统无法正常响应其他任务。如内核代码 bug、cpu 过载、设备驱动问题等都会导致 softlockup。当系统发生 softlockup 时，收集目标进程的信息以及 cpu 信息，获取各个 cpu 上的内核栈信息同时保存问题的发生次数。\nhungtask D 状态进程（也称为不可中断睡眠状态，Uninterruptible）是一种特殊的进程状态，表示进程因等待某些系统资源而阻塞，且不能被信号或外部中断唤醒。常见场景如：磁盘 I/O 操作、内核阻塞、硬件故障等。hungtask 捕获系统内所有 D 状态进程的内核栈并保存 D 进程的数量。用于定位瞬间出现一些 D 进程的场景，可以在现场消失后仍然分析到问题根因。\n{ \"_index\": \"***_2025-06-10\", \"_type\": \"_doc\", \"_id\": \"8yyOV5cBGoYArUxjSdvr\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-06-10T09:57:12.202191192+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"cpus_stack\": \"2025-06-10 09:57:14 sysrq: Show backtrace of all active CPUs\\n2025-06-10 09:57:14 NMI backtrace for cpu 33\\n2025-06-10 09:57:14 CPU: 33 PID: 768309 Comm: huatuo-bamai Kdump: loaded Tainted: G S W OEL 5.10.0-216.0.0.115.v1.0.x86_64 #1\\n2025-06-10 09:57:14 Hardware name: Inspur SA5212M5/YZMB-00882-104, BIOS 4.1.12 11/27/2019\\n2025-06-10 09:57:14 Call Trace:\\n2025-06-10 09:57:14 dump_stack+0x57/0x6e\\n2025-06-10 09:57:14 nmi_cpu_backtrace.cold.0+0x30/0x65\\n2025-06-10 09:57:14 ? lapic_can_unplug_cpu+0x80/0x80\\n2025-06-10 09:57:14 nmi_trigger_cpumask_backtrace+0xdf/0xf0\\n2025-06-10 09:57:14 arch_trigger_cpumask_backtrace+0x15/0x20\\n2025-06-10 09:57:14 sysrq_handle_showallcpus+0x14/0x90\\n2025-06-10 09:57:14 __handle_sysrq.cold.8+0x77/0xe8\\n2025-06-10 09:57:14 write_sysrq_trigger+0x3d/0x60\\n2025-06-10 09:57:14 proc_reg_write+0x38/0x80\\n2025-06-10 09:57:14 vfs_write+0xdb/0x250\\n2025-06-10 09:57:14 ksys_write+0x59/0xd0\\n2025-06-10 09:57:14 do_syscall_64+0x39/0x80\\n2025-06-10 09:57:14 entry_SYSCALL_64_after_hwframe+0x62/0xc7\\n2025-06-10 09:57:14 RIP: 0033:0x4088ae\\n2025-06-10 09:57:14 Code: 48 83 ec 38 e8 13 00 00 00 48 83 c4 38 5d c3 cc cc cc cc cc cc cc cc cc cc cc cc cc 49 89 f2 48 89 fa 48 89 ce 48 89 df 0f 05 \u003c48\u003e 3d 01 f0 ff ff 76 15 48 f7 d8 48 89 c1 48 c7 c0 ff ff ff ff 48\\n2025-06-10 09:57:14 RSP: 002b:000000c000adcc60 EFLAGS: 00000212 ORIG_RAX: 0000000000000001\\n2025-06-10 09:57:14 RAX: ffffffffffffffda RBX: 0000000000000013 RCX: 00000000004088ae\\n2025-06-10 09:57:14 RDX: 0000000000000001 RSI: 000000000274ab18 RDI: 0000000000000013\\n2025-06-10 09:57:14 RBP: 000000c000adcca0 R08: 0000000000000000 R09: 0000000000000000\\n2025-06-10 09:57:14 R10: 0000000000000000 R11: 0000000000000212 R12: 000000c000adcdc0\\n2025-06-10 09:57:14 R13: 0000000000000002 R14: 000000c000caa540 R15: 0000000000000000\\n2025-06-10 09:57:14 Sending NMI from CPU 33 to CPUs 0-32,34-95:\\n2025-06-10 09:57:14 NMI backtrace for cpu 52 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 54 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 7 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 81 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 60 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 2 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 21 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 69 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 58 skipped: idling at intel_idle+0x6f/ ... \"pid\": 2567042 }, \"tracer_time\": \"2025-06-10 09:57:12.202 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-06-10 09:57:12.202 +0800\", \"region\": \"***\", \"tracer_name\": \"hungtask\", \"es_index_time\": 1749520632297 }, \"fields\": { \"time\": [ \"2025-06-10T01:57:12.202Z\" ] }, \"_ignored\": [ \"tracer_data.blocked_processes_stack\", \"tracer_data.cpus_stack\" ], \"_version\": 1, \"sort\": [ 1749520632202 ] } 内存回收 内存压力过大时，如果此时进程申请内存，有可能进入直接回收，此时处于同步回收阶段，可能会造成业务进程的卡顿，在此记录进程进入直接回收的时间，有助于我们判断此进程被直接回收影响的剧烈程度。memreclaim event 计算同一个进程在 1s 周期，若进程处在直接回收状态超过 900ms， 则记录其上下文信息。\n{ \"_index\": \"***_cases_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"tracer_data\": { \"comm\": \"chrome\", \"deltatime\": 1412702917, \"pid\": 1896137 }, \"container_host_namespace\": \"***\", \"container_hostname\": \"***.docker\", \"es_index_time\": 1749641583290, \"uploaded_time\": \"2025-06-11T19:33:03.26754495+08:00\", \"hostname\": \"***\", \"container_type\": \"normal\", \"tracer_time\": \"2025-06-11 19:33:03.267 +0800\", \"time\": \"2025-06-11 19:33:03.267 +0800\", \"region\": \"***\", \"container_level\": \"102\", \"container_id\": \"921d0ec0a20c\", \"tracer_name\": \"directreclaim\" }, \"fields\": { \"time\": [ \"2025-06-11T11:33:03.267Z\" ] }, \"_version\": 1, \"sort\": [ 1749641583267 ] } 网络设备 网卡状态变化通常容易造成严重的网络问题，直接影响整机网络质量，如 down/up, MTU 改变等。以 down 状态为例，可能是有权限的进程操作、底层线缆、光模块、对端交换机等问题导致，netdev event 用于检测网络设备的状态变化，目前已实现网卡 down, up 的监控，并区分管理员或底层原因导致的网卡状态变化。 一次管理员操作导致 eth1 网卡 down 时，输出如下：\n{ \"_index\": \"***_cases_2025-05-30\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-05-30T17:47:50.406913037+08:00\", \"hostname\": \"localhost.localdomain\", \"tracer_data\": { \"ifname\": \"eth1\", \"start\": false, \"index\": 3, \"linkstatus\": \"linkStatusAdminDown, linkStatusCarrierDown\", \"mac\": \"5c:6f:69:34:dc:72\" }, \"tracer_time\": \"2025-05-30 17:47:50.406 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-05-30 17:47:50.406 +0800\", \"region\": \"***\", \"tracer_name\": \"netdev_event\", \"es_index_time\": 1748598470407 }, \"fields\": { \"time\": [ \"2025-05-30T09:47:50.406Z\" ] }, \"_version\": 1, \"sort\": [ 1748598470406 ] } bonding lacp Bond 是 Linux 系统内核提供的一种将多个物理网络接口绑定为一个逻辑接口的技术。通过绑定，可以实现带宽叠加、故障切换或负载均衡。LACP 是 IEEE 802.3ad 标准定义的协议，用于动态管理链路聚合组（LAG）。目前没有优雅获取物理机LACP 协议协商异常事件的方法，HUATUO 实现了 lacp event，通过 BPF 在协议关键路径插桩检测到链路聚合状态发生变化时，触发事件记录相关信息。\n在宿主网卡 eth1 出现物理层 down/up 抖动时，lacp 动态协商状态异常，输出如下：\n{ \"_index\": \"***_cases_2025-05-30\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-05-30T17:47:48.513318579+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"content\": \"/proc/net/bonding/bond0\\nEthernet Channel Bonding Driver: v4.18.0 (Apr 7, 2025)\\n\\nBonding Mode: load balancing (round-robin)\\nMII Status: down\\nMII Polling Interval (ms): 0\\nUp Delay (ms): 0\\nDown Delay (ms): 0\\nPeer Notification Delay (ms): 0\\n/proc/net/bonding/bond4\\nEthernet Channel Bonding Driver: v4.18.0 (Apr 7, 2025)\\n\\nBonding Mode: IEEE 802.3ad Dynamic link aggregation\\nTransmit Hash Policy: layer3+4 (1)\\nMII Status: up\\nMII Polling Interval (ms): 100\\nUp Delay (ms): 0\\nDown Delay (ms): 0\\nPeer Notification Delay (ms): 1000\\n\\n802.3ad info\\nLACP rate: fast\\nMin links: 0\\nAggregator selection policy (ad_select): stable\\nSystem priority: 65535\\nSystem MAC address: 5c:6f:69:34:dc:72\\nActive Aggregator Info:\\n\\tAggregator ID: 1\\n\\tNumber of ports: 2\\n\\tActor Key: 21\\n\\tPartner Key: 50013\\n\\tPartner Mac Address: 00:00:5e:00:01:01\\n\\nSlave Interface: eth0\\nMII Status: up\\nSpeed: 25000 Mbps\\nDuplex: full\\nLink Failure Count: 0\\nPermanent HW addr: 5c:6f:69:34:dc:72\\nSlave queue ID: 0\\nSlave active: 1\\nSlave sm_vars: 0x172\\nAggregator ID: 1\\nAggregator active: 1\\nActor Churn State: none\\nPartner Churn State: none\\nActor Churned Count: 0\\nPartner Churned Count: 0\\ndetails actor lacp pdu:\\n system priority: 65535\\n system mac address: 5c:6f:69:34:dc:72\\n port key: 21\\n port priority: 255\\n port number: 1\\n port state: 63\\ndetails partner lacp pdu:\\n system priority: 200\\n system mac address: 00:00:5e:00:01:01\\n oper key: 50013\\n port priority: 32768\\n port number: 16397\\n port state: 63\\n\\nSlave Interface: eth1\\nMII Status: up\\nSpeed: 25000 Mbps\\nDuplex: full\\nLink Failure Count: 17\\nPermanent HW addr: 5c:6f:69:34:dc:73\\nSlave queue ID: 0\\nSlave active: 0\\nSlave sm_vars: 0x172\\nAggregator ID: 1\\nAggregator active: 1\\nActor Churn State: monitoring\\nPartner Churn State: monitoring\\nActor Churned Count: 2\\nPartner Churned Count: 2\\ndetails actor lacp pdu:\\n system priority: 65535\\n system mac address: 5c:6f:69:34:dc:72\\n port key: 21\\n port priority: 255\\n port number: 2\\n port state: 15\\ndetails partner lacp pdu:\\n system priority: 200\\n system mac address: 00:00:5e:00:01:01\\n oper key: 50013\\n port priority: 32768\\n port number: 32781\\n port state: 31\\n\" }, \"tracer_time\": \"2025-05-30 17:47:48.513 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-05-30 17:47:48.513 +0800\", \"region\": \"***\", \"tracer_name\": \"lacp\", \"es_index_time\": 1748598468514 }, \"fields\": { \"time\": [ \"2025-05-30T09:47:48.513Z\" ] }, \"_ignored\": [ \"tracer_data.content\" ], \"_version\": 1, \"sort\": [ 1748598468513 ] } ","categories":"","description":"","excerpt":"当前版本支持 Linux 内核事件如下：\n事件名称 核心功能 场景 softirq 检测内核关闭中断时间过长，输出关闭软中断的内核调用栈，进 …","ref":"/docs/zh/latest/key-feature/events/","tags":"","title":"异常事件"},{"body":"HUATUO (华佗) 社区提供多种部署方式，具体如下：\n","categories":"","description":"","excerpt":"HUATUO (华佗) 社区提供多种部署方式，具体如下：\n","ref":"/docs/zh/latest/deployment/","tags":"","title":"应用部署"},{"body":"只需实现 Collector 接口并完成注册即可。\ntype Collector interface { Update() ([]*Data, error) } 创建 在 core/metrics/your-new-metric 目录创建 Collector 接口的结构体：\ntype exampleMetric struct{} 注册 func init() { tracing.RegisterEventTracing(\"example\", newExample) } func newExample() (*tracing.EventTracingAttr, error) { return \u0026tracing.EventTracingAttr{ TracingData: \u0026exampleMetric{}, Flag: tracing.FlagMetric, // 标记为 Metric 类型 }, nil } 实现 Update func (c *exampleMetric) Update() ([]*metric.Data, error) { // do something return []*metric.Data{ metric.NewGaugeData(\"example\", value, \"description of example\", nil), }, nil } 框架提供的丰富底层接口，包括 eBPF, Procfs, Cgroups, Storage, Utils, Pods 等。\n","categories":"","description":"","excerpt":"只需实现 Collector 接口并完成注册即可。\ntype Collector interface { Update() …","ref":"/docs/zh/latest/development/metrics/","tags":"","title":"自定义指标"},{"body":"Overview Type：Exception event-driven（tracing/autotracing） Function：Automatically tracks system abnormal states and triggers context information capture when exceptions occur Characteristics： When system abnormalities occur, autotracing automatically triggers and captures relevant context information Event data is stored locally in real-time and also sent to remote ES, while you can also generate Prometheus metrics for observation Suitable for significant performance overhead， such as triggering capture when detecting metrics rising above certain thresholds or rising too rapidly Already Integrated：abnormal usage tracking (cpu idle), D-state tracking (dload), container internal/external contention (waitrate), sudden memory allocation (memburst), disk abnormal tracking (iotracer) How to Add Autotracing AutoTracing only requires implementing the ITracingEvent interface and completing registration to add events to the system.\nThere is no implementation difference between AutoTracing and Event in the framework; they are only differentiated based on practical application scenarios.\n// ITracingEvent represents a autotracing or event type ITracingEvent interface { Start(ctx context.Context) error } 1. Create Structure type exampleTracing struct{} 2. Register Callback Function func init() { tracing.RegisterEventTracing(\"example\", newExample) } func newExample() (*tracing.EventTracingAttr, error) { return \u0026tracing.EventTracingAttr{ TracingData: \u0026exampleTracing{}, Internal: 10, // Interval in seconds before re-enabling tracing Flag: tracing.FlagTracing, // Mark as tracing type; | tracing.FlagMetric (optional) }, nil } 3. Implement ITracingEvent func (t *exampleTracing) Start(ctx context.Context) error { // detect your care about ... // Store data to ES and locally storage.Save(\"example\", ccontainerID, time.Now(), tracerData) } Additionally, you can optionally implement the Collector interface to output in Prometheus format:\nfunc (c *exampleTracing) Update() ([]*metric.Data, error) { // from tracerData to prometheus.Metric ... return data, nil } The core/autotracing directory in the project has integrated various practical autotracing 示examples, along with rich underlying interfaces provided by the framework, including BPF program and map data interaction, container information, etc. For more details, refer to the corresponding code implementations.\n","categories":"","description":"","excerpt":"Overview Type：Exception event-driven（tracing/autotracing） …","ref":"/docs/en/v2.1.0/tutorials/addcase/how-to-add-autotracing/","tags":"","title":"Add Autotracing"},{"body":"国内镜像下载 镜像默认存储在 dockerhub，也提供国内镜像获取方式，手动同步了采集器 huatuo/huatuo-bamai v2.1.0 版本到渡渡鸟镜像站，注意别下载 latest （渡渡鸟镜像站不会自动更新 latest 版本）:\nx86_64: https://docker.aityp.com/image/docker.io/huatuo/huatuo-bamai:v2.1.0\narm64: https://docker.aityp.com/image/docker.io/huatuo/huatuo-bamai:v2.1.0?platform=linux/arm64\n只部署采集器 启动容器 docker run --privileged --cgroupns=host --network=host -v /sys:/sys -v /proc:/proc -v /run:/run huatuo/huatuo-bamai:v2.1.0 ⚠️：这会使用容器内的默认配置文件，容器内的默认配置不会连接 ES，完整配置可将 huatuo-bamai.conf 通过 -v 方式挂载到容器内部，需要根据你的实际环境对 huatuo-bamai.conf 作修改，如连接 kubelet 和 elasticsrearch 的相关配置、本地存储异常事件日志的路径等。\n部署所有组件（docker compose） 在本地开发、验证使用 docker compose 是最便捷的方式，可通过该方式快速地在本地搭建部署一套完整的环境自行管理采集器、ES、prometheus、grafana 等组件。\ndocker compose --project-directory ./build/docker up docker compose 建议使用 plugins 方式安装，参考 https://docs.docker.com/compose/install/linux/\n","categories":"","description":"","excerpt":"国内镜像下载 镜像默认存储在 dockerhub，也提供国内镜像获取方式，手动同步了采集器 huatuo/huatuo-bamai …","ref":"/docs/zh/v2.1.0/tutorials/deploy/docker/","tags":"","title":"Docker"},{"body":"Run Only the Collector Start the Container docker run --privileged --cgroupns=host --network=host -v /sys:/sys -v /proc:/proc -v /run:/run huatuo/huatuo-bamai:v2.1.0 ⚠️ This uses the default configuration file inside the container. The internal default configuration does not connect to Elasticsearch. For a complete setup, mount your own huatuo-bamai.conf using -v, and update the config according to your environment (kubelet access, Elasticsearch settings, local log storage path, etc.).\nDeploy All Components (Docker Compose) For local development and validation, using Docker Compose is the most convenient approach.\nYou can quickly launch a full environment containing the collector, Elasticsearch, Prometheus, Grafana, and other components.\ndocker compose --project-directory ./build/docker up It is recommended to install Docker Compose using the plugin method: https://docs.docker.com/compose/install/linux/\n","categories":"","description":"","excerpt":"Run Only the Collector Start the Container docker run --privileged …","ref":"/docs/en/v2.1.0/tutorials/deploy/docker/","tags":"","title":"Docker"},{"body":"概述 类型：异常事件驱动（tracing/autotracing） 功能：自动跟踪系统异常状态，并在异常发生时再触发抓取现场上下文信息 特点： 当系统出现异常时，autotracing 会自动触发，捕获相关的上下文信息 事件数据会实时存储在本地并存储到远端ES，同时你也可以生成Prometheus 统计指标进行观测。 适用于获取现场时性能开销较大的场景，例如检测到指标上升到一定阈值、上升速度过快再触发抓取 已集成：cpu 异常使用跟踪（cpu idle）、D状态跟踪（dload）、容器内外部争抢（waitrate）、内存突发分配（memburst）、磁盘异常跟踪（iotracer） 如何添加 Autotracing ？ AutoTracing 只需实现 ITracingEvent 接口并完成注册，即可将事件添加到系统中。\nAutoTracing 与 Event 类型在框架实现上没有任何区别，只是针对不同的场景进行了实际应用的区分。\n// ITracingEvent represents a autotracing or event type ITracingEvent interface { Start(ctx context.Context) error } 1. 创建结构体 type exampleTracing struct{} 2. 注册回调函数 func init() { tracing.RegisterEventTracing(\"example\", newExample) } func newExample() (*tracing.EventTracingAttr, error) { return \u0026tracing.EventTracingAttr{ TracingData: \u0026exampleTracing{}, Internal: 10, // 再次开启 tracing 的间隔时间 seconds Flag: tracing.FlagTracing, // 标记为 tracing 类型； | tracing.FlagMetric（可选） }, nil } 3. 实现接口 ITracingEvent func (t *exampleTracing) Start(ctx context.Context) error { // detect your care about ... // 存储数据到 ES 和 本地 storage.Save(\"example\", ccontainerID, time.Now(), tracerData) } 另外也可同时实现接口 Collector 以 Prometheus 格式输出 （可选）\nfunc (c *exampleTracing) Update() ([]*metric.Data, error) { // from tracerData to prometheus.Metric ... return data, nil } 在项目 core/autotracing 目录下已集成了多种实际场景的 autotracing 示例，以及框架提供的丰富底层接口，包括 bpf prog，map 数据交互、容器信息等，更多详情可参考对应代码实现。\n","categories":"","description":"","excerpt":"概述 类型：异常事件驱动（tracing/autotracing） 功能：自动跟踪系统异常状态，并在异常发生时再触发抓取现场上下文信息 特 …","ref":"/docs/zh/v2.1.0/tutorials/addcase/how-to-add-autotracing/","tags":"","title":"如何添加自定义 Autotracing"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/en/v2.1.0/concepts/","tags":"","title":"Concepts"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/zh/v2.1.0/concepts/","tags":"","title":"概念"},{"body":"Overview Type: Exception event-driven（tracing/event） Function：Continuously runs in the system and captures context information when preset thresholds are reached Characteristics: Unlike autotracing, event runs continuously rather than being triggered only when exceptions occur. Event data is stored locally in real-time and also sent to remote ES. You can also generate Prometheus metrics for observation. Suitable for continuous monitoring and real-time analysis, enabling timely detection of abnormal behaviors in the system. The performance impact of event type collection is negligible. Already Integrated: Soft interrupt abnormalities（softirq）、abnormal memory allocation（oom）、soft lockups（softlockup）、D-state processes（hungtask）、memory reclaim（memreclaim）、abnormal packet loss（dropwatch）、network inbound latency (net_rx_latency), etc. How to Add Event Metrics Simply implement the ITracingEvent interface and complete registration to add events to the system.\nThere is no implementation difference between AutoTracing and Event in the framework; they are only differentiated based on practical application scenarios.\n// ITracingEvent represents a tracing/event type ITracingEvent interface { Start(ctx context.Context) error } 1. Create Event Structure type exampleTracing struct{} 2. Register Callback Function func init() { tracing.RegisterEventTracing(\"example\", newExample) } func newExample() (*tracing.EventTracingAttr, error) { return \u0026tracing.EventTracingAttr{ TracingData: \u0026exampleTracing{}, Internal: 10, // Interval in seconds before re-enabling tracing Flag: tracing.FlagTracing, // Mark as tracing type; | tracing.FlagMetric (optional) }, nil } 3. Implement the ITracingEvent Interface func (t *exampleTracing) Start(ctx context.Context) error { // do something ... // Store data to ES and locally storage.Save(\"example\", ccontainerID, time.Now(), tracerData) } Additionally, you can optionally implement the Collector interface to output in Prometheus format:\nfunc (c *exampleTracing) Update() ([]*metric.Data, error) { // from tracerData to prometheus.Metric ... return data, nil } The core/events directory in the project has integrated various practical events examples, along with rich underlying interfaces provided by the framework, including BPF program and map data interaction, container information, etc. For more details, refer to the corresponding code implementations.\n","categories":"","description":"","excerpt":"Overview Type: Exception event-driven（tracing/event） …","ref":"/docs/en/latest/development/events/","tags":"","title":"Add Event"},{"body":"HUATUO currently supports automatic tracing for the following metrics:\nTracing Name Core Function Scenario cpusys Host sys surge detection Service glitches caused by abnormal system load cpuidle Container CPU idle drop detection, providing call stacks, flame graphs, process context info, etc. Abnormal container CPU usage, helping identify process hotspots dload Tracks container loadavg and process states, automatically captures D-state process call info in containers System D-state surges are often related to unavailable resources or long-held locks; R-state process surges often indicate poor business logic design waitrate Container resource contention detection; provides info on contending containers during scheduling conflicts Container contention can cause service glitches; existing metrics lack specific contending container details; waitrate tracing provides this info for mixed-deployment resource isolation reference memburst Records context info during sudden memory allocations Detects short-term, large memory allocation events on the host, which may trigger direct reclaim or OOM iotracing Detects abnormal host disk I/O latency. Outputs context info like accessed filenames/paths, disk devices, inode numbers, containers, etc. Frequent disk I/O bandwidth saturation or access surges leading to application request latency or system performance jitter ","categories":"","description":"","excerpt":"HUATUO currently supports automatic tracing for the following metrics: …","ref":"/docs/en/latest/key-feature/autotracing/","tags":"","title":"Autotracing"},{"body":"1. Build with the Official Image To isolate the developer’s local environment and simplify the build process, we provide a containerized build method. You can directly use docker build to produce an image containing the core collector huatuo-bamai, BPF objects, tools, and more. Run the following in the project root directory:\ndocker build --network host -t huatuo/huatuo-bamai:latest . 2. Build a Custom Image Dockerfile.dev:\nFROM golang:1.23.0-alpine AS base # Speed up Alpine package installation if needed # RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories RUN apk add --no-cache \\ make \\ clang15 \\ libbpf-dev \\ bpftool \\ curl \\ git ENV PATH=$PATH:/usr/lib/llvm15/bin # build huatuo components FROM base AS build ARG BUILD_PATH=${BUILD_PATH:-/go/huatuo-bamai} ARG RUN_PATH=${RUN_PATH:-/home/huatuo-bamai} WORKDIR ${BUILD_PATH} 2.1 Build the Dev Image docker build --network host -t huatuo/huatuo-bamai-dev:latest -f ./Dockerfile.dev . 2.2 Run the Dev Container docker run -it --privileged --cgroupns=host --network=host \\ -v /path/to/huatuo:/go/huatuo-bamai \\ huatuo/huatuo-bamai-dev:latest sh 2.3 Compile Inside the Container Run:\nmake Once the build completes, all artifacts are generated under ./_output.\n3. Build on a Physical Machine or VM The collector depends on the following tools. Install them based on your local environment:\nmake git clang15 libbpf bpftool curl Due to significant differences across local environments, build issues may occur.\nTo avoid environment inconsistencies and simplify troubleshooting, we strongly recommend using the Docker build approach whenever possible.\n","categories":"","description":"","excerpt":"1. Build with the Official Image To isolate the developer’s local …","ref":"/docs/en/latest/compiling/","tags":"","title":"Compile"},{"body":"Install by RPM OpenCloudOS currently provides the v2.1.0 RPM package; the master is for reference only.\nTencent OpenCloudOS provides an official HUATUO package:\nhttps://mirrors.opencloudos.tech/epol/9/Everything/x86_64/os/Packages/huatuo-bamai-2.1.0-2.oc9.x86_64.rpm\nThis allows HUATUO to be quickly installed and enabled on OpenCloudOS.\nx86_64 architecture wget https://mirrors.opencloudos.tech/epol/9/Everything/x86_64/os/Packages/huatuo-bamai-2.1.0-2.oc9.x86_64.rpm arm64 architecture wget https://mirrors.opencloudos.tech/epol/9/Everything/aarch64/os/Packages/huatuo-bamai-2.1.0-2.oc9.aarch64.rpm Install HUATUO on OC8 sudo rpm -ivh huatuo-bamai*.rpm Other RPM-based operating systems can install HUATUO the same way.\nAs usual, you must update the config file according to your environment (e.g., kubelet connection, Elasticsearch settings).\nFull OpenCloudOS installation guide:\nhttps://mp.weixin.qq.com/s/Gmst4_FsbXUIhuJw1BXNnQ\nInstall by Binary Package The latest binary package provided is v2.1.0; the master branch is for reference only.\nYou can also download the binary package and configure/manage it manually.\nAgain, update the configuration file based on your actual environment (kubelet connection, Elasticsearch settings, etc.).\nv2.1.0: https://github.com/ccfos/huatuo/releases/tag/v2.1.0 ","categories":"","description":"","excerpt":"Install by RPM OpenCloudOS currently provides the v2.1.0 RPM package; …","ref":"/docs/en/latest/deployment/systemd/","tags":"","title":"Systemd"},{"body":"1. 腾讯云下载 腾讯操作系统 OpenCloudOS 提供 HUATUO 安装包\nwget https://mirrors.opencloudos.tech/epol/9/Everything/x86_64/os/Packages/huatuo-bamai-2.1.0-2.oc9.x86_64.rpm wget https://mirrors.opencloudos.tech/epol/9/Everything/aarch64/os/Packages/huatuo-bamai-2.1.0-2.oc9.aarch64.rpm 2. 安装 RPM sudo rpm -ivh huatuo-bamai*.rpm 3. 启动华佗 sudo systemctl start huatuo-bamai sudo systemctl enable huatuo-bamai 完整安装可参考https://mp.weixin.qq.com/s/Gmst4_FsbXUIhuJw1BXNnQ\n","categories":"","description":"","excerpt":"1. 腾讯云下载 腾讯操作系统 OpenCloudOS 提供 HUATUO 安装包\nwget …","ref":"/docs/zh/latest/deployment/systemd/","tags":"","title":"Systemd 物理机部署"},{"body":"1. 容器编译 可以执行如下命令，完成编译，静态代码检查。\n$ sh build/build-run-testing-image.sh 或者单独执行：\n1. 准备编译环境\n$ docker build --network host -t huatuo/huatuo-bamai-dev:latest -f ./Dockerfile.devel . 2. 启动编译容器\n$ docker run -it --privileged --cgroupns=host --network=host -v $(pwd):/go/huatuo-bamai huatuo/huatuo-bamai-dev:latest sh 3. 进入容器编译\n$ make 2. 物理机编译 2.1 安装依赖 Ubuntu 24.04:\napt install make git clang libbpf-dev linux-tools-common curl Fedora 40:\ndnf install make git clang libbpf-devel bpftool curl 2.2 编译 $ make 3. 镜像发布 通过 docker build 方式能够快速的发布，最新二进制容器镜像。\ndocker build --network host -t huatuo/huatuo-bamai:latest . ","categories":"","description":"","excerpt":"1. 容器编译 可以执行如下命令，完成编译，静态代码检查。\n$ sh build/build-run-testing-image.sh 或者 …","ref":"/docs/zh/latest/compiling/","tags":"","title":"源码编译"},{"body":"只需实现 ITracingEvent 接口并完成注册即可。\ntype ITracingEvent interface { Start(ctx context.Context) error } 创建 type exampleTracing struct{} 注册 func init() { tracing.RegisterEventTracing(\"example\", newExample) } func newExample() (*tracing.EventTracingAttr, error) { return \u0026tracing.EventTracingAttr{ TracingData: \u0026exampleTracing{}, Internal: 10, // 再次开启 tracing 的间隔时间，单位秒 Flag: tracing.FlagTracing, // 标记为 tracing 类型；tracing.FlagMetric（可选） }, nil } 实现 Start func (t *exampleTracing) Start(ctx context.Context) error { // do something ... // 存储数据到 ES 和 本地 storage.Save(\"example\", ccontainerID, time.Now(), tracerData) } 此外，可同时实现接口 Collector 并以 Prometheus 格式输出 （可选）\nfunc (c *exampleTracing) Update() ([]*metric.Data, error) { // from tracerData to prometheus.Metric ... return data, nil } ","categories":"","description":"","excerpt":"只需实现 ITracingEvent 接口并完成注册即可。\ntype ITracingEvent interface { Start(ctx …","ref":"/docs/zh/latest/development/events/","tags":"","title":"自定义事件"},{"body":"当前版本支持自动追踪如下：\n追踪名称 核心功能 场景 cpusys 物理机 cpu sys 突增检测，提供火焰图，进程上下文信息等 解决由于系统负载异常导致业务毛刺问题 cpuidle 容器 cpu idle 掉底检测，提供火焰图，进程上下文信息等 解决容器 cpu 使用异常，帮助业务描绘进程热点 dload 容器 loadavg 状态突增，自动抓取容器 D 状态进程调用信息 解决系统 D 状态突增、资源不可用或者锁被长期持等相关问题 memburst 物理机内存突发分配检测，自动捕获进程内存使用状态 物理机短时间内大量分配内存，可能引发直接回收或者 oom 等 iotracing 物理机磁盘 IO 延迟异常，自动捕获输进程，容器，磁盘，文件等信息 频繁出现磁盘 IO 带宽打满、磁盘访问突增，进而导致应用请求延迟或者系统性能抖动等 ","categories":"","description":"","excerpt":"当前版本支持自动追踪如下：\n追踪名称 核心功能 场景 cpusys 物理机 cpu sys 突增检测，提供火焰图，进程上下文信息等 解决由于 …","ref":"/docs/zh/latest/key-feature/autotracing/","tags":"","title":"自动追踪"},{"body":"Overview Type: Exception event-driven（tracing/event） Function：Continuously runs in the system and captures context information when preset thresholds are reached Characteristics: Unlike autotracing, event runs continuously rather than being triggered only when exceptions occur. Event data is stored locally in real-time and also sent to remote ES. You can also generate Prometheus metrics for observation. Suitable for continuous monitoring and real-time analysis, enabling timely detection of abnormal behaviors in the system. The performance impact of event type collection is negligible. Already Integrated: Soft interrupt abnormalities（softirq）、abnormal memory allocation（oom）、soft lockups（softlockup）、D-state processes（hungtask）、memory reclaim（memreclaim）、abnormal packet loss（dropwatch）、network inbound latency (net_rx_latency), etc. How to Add Event Metrics Simply implement the ITracingEvent interface and complete registration to add events to the system.\nThere is no implementation difference between AutoTracing and Event in the framework; they are only differentiated based on practical application scenarios.\n// ITracingEvent represents a tracing/event type ITracingEvent interface { Start(ctx context.Context) error } 1. Create Event Structure type exampleTracing struct{} 2. Register Callback Function func init() { tracing.RegisterEventTracing(\"example\", newExample) } func newExample() (*tracing.EventTracingAttr, error) { return \u0026tracing.EventTracingAttr{ TracingData: \u0026exampleTracing{}, Internal: 10, // Interval in seconds before re-enabling tracing Flag: tracing.FlagTracing, // Mark as tracing type; | tracing.FlagMetric (optional) }, nil } 3. Implement the ITracingEvent Interface func (t *exampleTracing) Start(ctx context.Context) error { // do something ... // Store data to ES and locally storage.Save(\"example\", ccontainerID, time.Now(), tracerData) } Additionally, you can optionally implement the Collector interface to output in Prometheus format:\nfunc (c *exampleTracing) Update() ([]*metric.Data, error) { // from tracerData to prometheus.Metric ... return data, nil } The core/events directory in the project has integrated various practical events examples, along with rich underlying interfaces provided by the framework, including BPF program and map data interaction, container information, etc. For more details, refer to the corresponding code implementations.\n","categories":"","description":"","excerpt":"Overview Type: Exception event-driven（tracing/event） …","ref":"/docs/en/v2.1.0/tutorials/addcase/how-to-add-events/","tags":"","title":"Add Event"},{"body":"HuaTuo framework provides three data collection modes: autotracing, event, and metrics, covering different monitoring scenarios, helping users gain comprehensive insights into system performance.\nCollection Mode Comparison Mode Type Trigger Condition Data Output Use Case Autotracing Event-driven Triggered on system anomalies ES + Local Storage, Prometheus (optional) Non-routine operations, triggered on anomalies Event Event-driven Continuously running, triggered on preset thresholds ES + Local Storage, Prometheus (optional) Continuous operations, directly dump context Metrics Metric collection Passive collection Prometheus format Monitoring system metrics Autotracing\nType: Event-driven (tracing). Function: Automatically tracks system anomalies and dump context when anomalies occur. Features: When a system anomaly occurs, autotracing is triggered automatically to dump relevant context. Data is stored to ES in real-time and stored locally for subsequent analysis and troubleshooting. It can also be monitored in Prometheus format for statistics and alerts. Suitable for scenarios with high performance overhead, such as triggering captures when metrics exceed a threshold or rise too quickly. Integrated Features: CPU anomaly tracking (cpu idle), D-state tracking (dload), container contention (waitrate), memory burst allocation (memburst), disk anomaly tracking (iotracer). Event\nType: Event-driven (tracing). Function: Continuously operates within the system context, directly dump context when preset thresholds are met. Features: Unlike autotracing, event continuously operates within the system context, rather than being triggered by anomalies. Data is also stored to ES and locally, and can be monitored in Prometheus format. Suitable for continuous monitoring and real-time analysis, enabling timely detection of abnormal behaviors. The performance impact of event collection is negligible. Integrated Features: Soft interrupt anomalies (softirq), memory allocation anomalies (oom), soft lockups (softlockup), D-state processes (hungtask), memory reclamation (memreclaim), packet droped abnormal (dropwatch), network ingress latency (net_rx_latency). Metrics\nType: Metric collection. Function: Collects performance metrics from subsystems. Features: Metric data can be sourced from regular procfs collection or derived from tracing (autotracing, event) data. Outputs in Prometheus format for easy integration into Prometheus monitoring systems. Unlike tracing data, metrics primarily focus on system performance metrics such as CPU usage, memory usage, and network traffic, etc. Suitable for monitoring system performance metrics, supporting real-time analysis and long-term trend observation. Integrated Features: CPU (sys, usr, util, load, nr_running, etc.), memory (vmstat, memory_stat, directreclaim, asyncreclaim, etc.), IO (d2c, q2c, freeze, flush, etc.), network (arp, socket mem, qdisc, netstat, netdev, sockstat, etc.). Multiple Purpose of Tracing Mode Both autotracing and event belong to the tracing collection mode, offering the following dual purposes:\nReal-time storage to ES and local storage: For tracing and analyzing anomalies, helping users quickly identify root causes. Output in Prometheus format: As metric data integrated into Prometheus monitoring systems, providing comprehensive system monitoring capabilities. By flexibly combining these three modes, users can comprehensively monitor system performance, capturing both contextual information during anomalies and continuous performance metrics to meet various monitoring needs.\n","categories":"","description":"","excerpt":"HuaTuo framework provides three data collection modes: autotracing, …","ref":"/docs/en/v2.1.0/concepts/event-metrics-mode/","tags":"","title":"Collection Framework"},{"body":"The HUATUO collector huatuo-bamai runs on physical machines or VMs.\nWe provide both binary packages and Docker images, and you can deploy them in any custom way, such as:\nsystemd and DaemonSet deployments are recommended for production. Docker / Compose is suitable for development and quick validation scenarios. Binary Download v2.1.0: https://github.com/ccfos/huatuo/releases/tag/v2.1.0 Mirror Download Docker images are stored on Docker Hub by default (https://hub.docker.com/u/huatuo).\n","categories":"","description":"","excerpt":"The HUATUO collector huatuo-bamai runs on physical machines or VMs.\nWe …","ref":"/docs/en/v2.1.0/tutorials/deploy/","tags":"","title":"Deploy"},{"body":"rpm 方式安装 腾讯 OpenCloudOS 提供了 HUATUO 安装包，因此在 OpenCloudOS 上可快速启用 HUATUO。\nx86_64 架构 wget https://mirrors.opencloudos.tech/epol/9/Everything/x86_64/os/Packages/huatuo-bamai-2.1.0-2.oc9.x86_64.rpm arm64 架构 wget https://mirrors.opencloudos.tech/epol/9/Everything/aarch64/os/Packages/huatuo-bamai-2.1.0-2.oc9.aarch64.rpm OC8 通过下载的 rpm 包安装 HUATUO sudo rpm -ivh huatuo-bamai*.rpm 类似其他使用 rpm 包管理的 OS 也可通过该方式下载和安装，配置文件需要根据你的实际环境作修改，如连接 kubelet 和 elasticsrearch 的相关配置等。\n腾讯 OpenCloudOS 完整安装可参考https://mp.weixin.qq.com/s/Gmst4_FsbXUIhuJw1BXNnQ\n二进制安装 可下载二进制包自定义配置托管方式，同上配置文件需要根据你的实际环境作修改，如连接 kubelet 和 elasticsrearch 的相关配置等。\nv2.1.0 https://github.com/ccfos/huatuo/releases/tag/v2.1.0 ","categories":"","description":"","excerpt":"rpm 方式安装 腾讯 OpenCloudOS 提供了 HUATUO 安装包，因此在 OpenCloudOS 上可快速启用 HUATUO。 …","ref":"/docs/zh/v2.1.0/tutorials/deploy/systemd/","tags":"","title":"Systemd"},{"body":"Install by RPM Tencent OpenCloudOS provides an official HUATUO package:\nhttps://mirrors.opencloudos.tech/epol/9/Everything/x86_64/os/Packages/huatuo-bamai-2.1.0-2.oc9.x86_64.rpm\nThis allows HUATUO to be quickly installed and enabled on OpenCloudOS.\nx86_64 architecture wget https://mirrors.opencloudos.tech/epol/9/Everything/x86_64/os/Packages/huatuo-bamai-2.1.0-2.oc9.x86_64.rpm arm64 architecture wget https://mirrors.opencloudos.tech/epol/9/Everything/aarch64/os/Packages/huatuo-bamai-2.1.0-2.oc9.aarch64.rpm Install HUATUO on OC8 sudo rpm -ivh huatuo-bamai*.rpm Other RPM-based operating systems can install HUATUO the same way.\nAs usual, you must update the config file according to your environment (e.g., kubelet connection, Elasticsearch settings).\nFull OpenCloudOS installation guide:\nhttps://mp.weixin.qq.com/s/Gmst4_FsbXUIhuJw1BXNnQ\nInstall by Binary Package You can also download the binary package and configure/manage it manually.\nAgain, update the configuration file based on your actual environment (kubelet connection, Elasticsearch settings, etc.).\nv2.1.0: https://github.com/ccfos/huatuo/releases/tag/v2.1.0 ","categories":"","description":"","excerpt":"Install by RPM Tencent OpenCloudOS provides an official HUATUO …","ref":"/docs/en/v2.1.0/tutorials/deploy/systemd/","tags":"","title":"Systemd"},{"body":"HUATUO 采集器 huatuo-bamai 运行在物理机或 VM 上，提供了二进制包和 docker 镜像，可完全通过自定义方式部署，如 k8s daemonset 方式、systemd/supervisord 托管方式等。\nsystemd 托管、daemonset 方式适合生产环境 docker / compose 方式适合开发和快速验证场景 二进制包下载 v2.1.0 https://github.com/ccfos/huatuo/releases/tag/v2.1.0 国内镜像下载 镜像默认存储在 dockerhub，也提供国内镜像获取方式，手动同步了采集器 huatuo/huatuo-bamai v2.1.0 版本到渡渡鸟镜像站，注意别下载 latest （渡渡鸟镜像站不会自动更新 latest 版本）:\nx86_64: https://docker.aityp.com/image/docker.io/huatuo/huatuo-bamai:v2.1.0\narm64: https://docker.aityp.com/image/docker.io/huatuo/huatuo-bamai:v2.1.0?platform=linux/arm64\n","categories":"","description":"","excerpt":"HUATUO 采集器 huatuo-bamai 运行在物理机或 VM 上，提供了二进制包和 docker 镜像，可完全通过自定义方式部署， …","ref":"/docs/zh/v2.1.0/tutorials/deploy/","tags":"","title":"部署"},{"body":"底层采集器提供三种数据采集模式：autotracing、event 和 metrics，分别针对不同的监控场景和需求，帮助用户全面掌握系统的运行状态。\n采集模式对比 模式 类型 触发条件 数据输出 适用场景 Autotracing 异常事件驱动 系统异常时触发 ES + 本地存储，Prometheus（可选） 不能常态运行，异常时触发运行 Event 异常事件驱动 常态运行 ES + 本地存储，Prometheus（可选） 常态运行，直接抓取上下文信息 Metrics 指标数据采集 被动采集 Prometheus 格式 监控系统性能指标 Autotracing\n类型：异常事件驱动（tracing）。 功能：自动跟踪系统异常状态，并在异常发生时再触发抓取现场上下文信息。 特点： 当系统出现异常时，autotracing 会自动触发，捕获相关的上下文信息。 数据会实时上报到 ES 并存储在本地，便于后续分析和排查问题，也可通过 Prometheus 格式进行监控，便于统计和告警。 适用于获取现场时性能开销较大的场景，例如检测到指标上升到一定阈值、上升速度过快再触发抓取。 已集成：cpu 异常使用跟踪（cpu idle）、D状态跟踪（dload）、容器内外部争抢（waitrate）、内存突发分配（memburst）、磁盘异常跟踪（iotracer）。 Event\n类型：异常事件驱动（tracing）。 功能：常态运行在系统上下文中，达到预设阈值直接抓取上下文信息。 特点： 与 autotracing 不同，event 是常态运行，而不是在异常时再触发。 数据同样会实时上报到 ES 并存储在本地，也可通过 Prometheus 格式进行监控。 适合用于常态监控和实时分析，能够及时发现系统中的异常行为， event 类型的采集对系统性能影响可忽略。 已集成：软中断异常（softirq）、内存异常分配（oom）、软锁定（softlockup）、D 状态进程（hungtask）、内存回收（memreclaim）、异常丢包（dropwatch）、网络入向延迟（net_rx_latency）。 Metrics\n类型：指标数据采集。 功能：采集各子系统的性能指标数据。 特点： 指标数据可以来自常规 procfs 采集，也可以从 tracing (autotracing,event) 类型获取数据。 以 Prometheus 格式输出，便于集成到 Prometheus 监控系统中。 与 tracing 类数据不同，metrics 主要用于采集系统的性能指标，如 CPU 使用率、内存使用率、网络等。 适合用于监控系统的性能指标，支持实时分析和长期趋势观察。 已集成：cpu (sys, usr, util, load, nr_running…), memory（vmstat, memory_stat, directreclaim, asyncreclaim…）, IO(d2c, q2c, freeze, flush…), 网络（arp, socket mem, qdisc, netstat, netdev, socketstat…） Tracing 模式的多重用途 autotracing 和 event 都属于 tracing 类数据采集模式，它们具备以下双重用途：\n实时保存到 ES 和 本地存储：用于异常事件的追踪和分析，帮助用户快速根因定位。 以 Prometheus 格式输出：作为指标数据集成到 Prometheus 监控系统中，提供更全面的系统监控能力。 通过这三种模式的灵活组合，用户可以全面监控系统的运行状态，既能捕获异常事件的上下文信息，也能持续采集性能指标数据，满足不同场景下的监控需求。\n添加自己的采集 根据实际场景，你可以在 core/autotracing 或 core/events 目录下实现接口 ITracingEvent 即可完成 tracing 类型的采集。\n// ITracingEvent represents a tracing/event type ITracingEvent interface { Start(ctx context.Context) error } 在 core/metrics 目录下添加接口 Collector 的实现即可完成 Metric 类型的采集。\ntype Collector interface { // Get new metrics and expose them via prometheus registry. Update() ([]*Data, error) } ","categories":"","description":"","excerpt":"底层采集器提供三种数据采集模式：autotracing、event 和 metrics，分别针对不同的监控场景和需求，帮助用户全面掌握系统的 …","ref":"/docs/zh/v2.1.0/concepts/event-metrics-mode/","tags":"","title":"采集框架"},{"body":"概述 类型：异常事件驱动（tracing/event） 功能：常态运行在系统达到预设阈值后抓取上下文信息 特点： 与 autotracing 不同，event 是常态运行，而不是在异常时再触发。 事件数据会实时存储在本地并存储到远端ES，同时你也可以生成Prometheus 统计指标进行观测。 适合用于常态监控和实时分析，能够及时发现系统中的异常行为， event 类型的采集对系统性能影响可忽略。 已集成：软中断异常（softirq）、内存异常分配（oom）、软锁定（softlockup）、D 状态进程（hungtask）、内存回收（memreclaim）、异常丢包（dropwatch）、网络入向延迟（net_rx_latency） 等 如何添加事件指标 只需实现 ITracingEvent 接口并完成注册，即可将事件添加到系统。\nAutoTracing 与 Event 类型在框架实现上没有任何区别，只是针对不同的场景进行了实际应用的区分。\n// ITracingEvent represents a tracing/event type ITracingEvent interface { Start(ctx context.Context) error } 1. 创建 Event 结构体 type exampleTracing struct{} 2. 注册回调函数 func init() { tracing.RegisterEventTracing(\"example\", newExample) } func newExample() (*tracing.EventTracingAttr, error) { return \u0026tracing.EventTracingAttr{ TracingData: \u0026exampleTracing{}, Internal: 10, // 再次开启 tracing 的间隔时间 seconds Flag: tracing.FlagTracing, // 标记为 tracing 类型；| tracing.FlagMetric（可选） }, nil } 3. 实现接口 ITracingEvent func (t *exampleTracing) Start(ctx context.Context) error { // do something ... // 存储数据到 ES 和 本地 storage.Save(\"example\", ccontainerID, time.Now(), tracerData) } 另外也可同时实现接口 Collector 以 Prometheus 格式输出 （可选）\nfunc (c *exampleTracing) Update() ([]*metric.Data, error) { // from tracerData to prometheus.Metric ... return data, nil } 在项目 core/events 目录下已集成了多种实际场景的 events 示例，以及框架提供的丰富底层接口，包括 bpf prog, map 数据交互、容器信息等，更多详情可参考对应代码实现。\n","categories":"","description":"","excerpt":"概述 类型：异常事件驱动（tracing/event） 功能：常态运行在系统达到预设阈值后抓取上下文信息 特点： 与 autotracing …","ref":"/docs/zh/v2.1.0/tutorials/addcase/how-to-add-events/","tags":"","title":"如何添加自定义 Event"},{"body":"Overview Type：Exception event-driven（tracing/autotracing） Function：Automatically tracks system abnormal states and triggers context information capture when exceptions occur Characteristics： When system abnormalities occur, autotracing automatically triggers and captures relevant context information Event data is stored locally in real-time and also sent to remote ES, while you can also generate Prometheus metrics for observation Suitable for significant performance overhead， such as triggering capture when detecting metrics rising above certain thresholds or rising too rapidly Already Integrated：abnormal usage tracking (cpu idle), D-state tracking (dload), container internal/external contention (waitrate), sudden memory allocation (memburst), disk abnormal tracking (iotracer) How to Add Autotracing AutoTracing only requires implementing the ITracingEvent interface and completing registration to add events to the system.\nThere is no implementation difference between AutoTracing and Event in the framework; they are only differentiated based on practical application scenarios.\n// ITracingEvent represents a autotracing or event type ITracingEvent interface { Start(ctx context.Context) error } 1. Create Structure type exampleTracing struct{} 2. Register Callback Function func init() { tracing.RegisterEventTracing(\"example\", newExample) } func newExample() (*tracing.EventTracingAttr, error) { return \u0026tracing.EventTracingAttr{ TracingData: \u0026exampleTracing{}, Internal: 10, // Interval in seconds before re-enabling tracing Flag: tracing.FlagTracing, // Mark as tracing type; | tracing.FlagMetric (optional) }, nil } 3. Implement ITracingEvent func (t *exampleTracing) Start(ctx context.Context) error { // detect your care about ... // Store data to ES and locally storage.Save(\"example\", ccontainerID, time.Now(), tracerData) } Additionally, you can optionally implement the Collector interface to output in Prometheus format:\nfunc (c *exampleTracing) Update() ([]*metric.Data, error) { // from tracerData to prometheus.Metric ... return data, nil } The core/autotracing directory in the project has integrated various practical autotracing 示examples, along with rich underlying interfaces provided by the framework, including BPF program and map data interaction, container information, etc. For more details, refer to the corresponding code implementations.\n","categories":"","description":"","excerpt":"Overview Type：Exception event-driven（tracing/autotracing） …","ref":"/docs/en/latest/development/autotracing/","tags":"","title":"Add Autotracing"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/en/latest/key-feature/","tags":"","title":"Key Feature"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/zh/latest/key-feature/","tags":"","title":"核心特性"},{"body":"AutoTracing 与 Event 类型在框架实现上没有区别，只是针对不同的场景进行应用区分。\ntype ITracingEvent interface { Start(ctx context.Context) error } ","categories":"","description":"","excerpt":"AutoTracing 与 Event 类型在框架实现上没有区别，只是针对不同的场景进行应用区分。\ntype ITracingEvent …","ref":"/docs/zh/latest/development/autotracing/","tags":"","title":"自定义追踪"},{"body":"Overview The Metrics type is used to collect system performance and other indicator data. It can output in Prometheus format, serving as a data provider through the /metrics (curl localhost:\u003cport\u003e/metrics) .\nType：Metrics collection\nFunction：Collects performance metrics from various subsystems\nCharacteristics：\nMetrics are primarily used to collect system performance metrics such as CPU usage, memory usage, network statistics, etc. They are suitable for monitoring system performance and support real-time analysis and long-term trend observation. Metrics can come from regular procfs/sysfs collection or be generated from tracing types (autotracing, event). Outputs in Prometheus format for seamless integration into the Prometheus observability ecosystem. Already Integrated：\ncpu (sys, usr, util, load, nr_running…) memory（vmstat, memory_stat, directreclaim, asyncreclaim…） IO (d2c, q2c, freeze, flush…) Network（arp, socket mem, qdisc, netstat, netdev, socketstat…） How to Add Statistical Metrics Simply implement the Collector interface and complete registration to add metrics to the system.\ntype Collector interface { // Get new metrics and expose them via prometheus registry. Update() ([]*Data, error) } 1. Create a Structure Create a structure that implements the Collector interface in the core/metrics directory:\ntype exampleMetric struct{ } 2. Register Callback Function func init() { tracing.RegisterEventTracing(\"example\", newExample) } func newExample() (*tracing.EventTracingAttr, error) { return \u0026tracing.EventTracingAttr{ TracingData: \u0026exampleMetric{}, Flag: tracing.FlagMetric, // Mark as Metric type }, nil } 3. Implement the Update Method func (c *exampleMetric) Update() ([]*metric.Data, error) { // do something ... return []*metric.Data{ metric.NewGaugeData(\"example\", value, \"description of example\", nil), }, nil } The core/metrics directory in the project has integrated various practical Metrics examples, along with rich underlying interfaces provided by the framework, including BPF program and map data interaction, container information, etc. For more details, refer to the corresponding code implementations.\n","categories":"","description":"","excerpt":"Overview The Metrics type is used to collect system performance and …","ref":"/docs/en/v2.1.0/tutorials/addcase/how-to-add-metrcis/","tags":"","title":"Add Metrics"},{"body":"概述 Metrics 类型用于采集系统性能等指标数据，可输出为 Prometheus 格式，作为服务端对外提供数据，通过接口 /metrics (curl localhost:\u003cport\u003e/metrics) 获取。\n类型：指标数据采集\n功能：采集各子系统的性能指标数据\n特点：\nmetrics 主要用于采集系统的性能指标，如 CPU 使用率、内存使用率、网络等，适合用于监控系统的性能指标，支持实时分析和长期趋势观察。 指标数据可以来自常规 procfs/sysfs 采集，也可以从 tracing (autotracing, event) 类型生成指标数据 Prometheus 格式输出，便于无缝集成到 Prometheus 观测体系 已集成：\ncpu (sys, usr, util, load, nr_running…) memory（vmstat, memory_stat, directreclaim, asyncreclaim…） IO (d2c, q2c, freeze, flush…) 网络（arp, socket mem, qdisc, netstat, netdev, socketstat…） 如何添加统计指标 只需实现 Collector 接口并完成注册，即可将指标添加到系统中。\ntype Collector interface { // Get new metrics and expose them via prometheus registry. Update() ([]*Data, error) } 1. 创建结构体 在 core/metrics 目录下创建实现 Collector 接口的结构体：\ntype exampleMetric struct{ } 2. 注册回调函数 func init() { tracing.RegisterEventTracing(\"example\", newExample) } func newExample() (*tracing.EventTracingAttr, error) { return \u0026tracing.EventTracingAttr{ TracingData: \u0026exampleMetric{}, Flag: tracing.FlagMetric, // 标记为 Metric 类型 }, nil } 3. 实现 Update 方法 func (c *exampleMetric) Update() ([]*metric.Data, error) { // do something ... return []*metric.Data{ metric.NewGaugeData(\"example\", value, \"description of example\", nil), }, nil } 在项目 core/metrics 目录下已集成了多种实际场景的 Metrics 示例，以及框架提供的丰富底层接口，包括 bpf prog, map 数据交互、容器信息等，更多详情可参考对应代码实现。\n","categories":"","description":"","excerpt":"概述 Metrics 类型用于采集系统性能等指标数据，可输出为 Prometheus 格式，作为服务端对外提供数据， …","ref":"/docs/zh/v2.1.0/tutorials/addcase/how-to-add-metrcis/","tags":"","title":"如何添加自定义 Metrics"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/en/latest/best-practice/","tags":"","title":"Best Practice"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/zh/latest/best-practice/","tags":"","title":"应用实践"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/zh/v2.1.0/","tags":"","title":"v2.1.0"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/en/v2.1.0/","tags":"","title":"v2.1.0"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/en/latest/development/","tags":"","title":"Development"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/zh/latest/development/","tags":"","title":"开发手册"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/en/latest/faq/","tags":"","title":"Frequenty Asked Questions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/zh/latest/faq/","tags":"","title":"常见问题"},{"body":"example en/docs/v1.0/_index.md\n","categories":"","description":"","excerpt":"example en/docs/v1.0/_index.md\n","ref":"/docs/en/v2.0.0/","tags":"","title":"v2.0.0"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/en/v2.1.0/tutorials/addcase/","tags":"","title":"Add Cases"},{"body":"HUATUO currently supports automatic tracing for the following metrics:\nTracing Name Core Function Scenario cpusys Host sys surge detection Service glitches caused by abnormal system load cpuidle Container CPU idle drop detection, providing call stacks, flame graphs, process context info, etc. Abnormal container CPU usage, helping identify process hotspots dload Tracks container loadavg and process states, automatically captures D-state process call info in containers System D-state surges are often related to unavailable resources or long-held locks; R-state process surges often indicate poor business logic design waitrate Container resource contention detection; provides info on contending containers during scheduling conflicts Container contention can cause service glitches; existing metrics lack specific contending container details; waitrate tracing provides this info for mixed-deployment resource isolation reference memburst Records context info during sudden memory allocations Detects short-term, large memory allocation events on the host, which may trigger direct reclaim or OOM iotracing Detects abnormal host disk I/O latency. Outputs context info like accessed filenames/paths, disk devices, inode numbers, containers, etc. Frequent disk I/O bandwidth saturation or access surges leading to application request latency or system performance jitter CPUSYS System mode CPU time reflects kernel execution overhead, including system calls, interrupt handling, kernel thread scheduling, memory management, lock contention, etc. Abnormal increases in this metric typically indicate kernel-level performance bottlenecks: frequent system calls, hardware device exceptions, lock contention, or memory reclaim pressure (e.g., kswapd direct reclaim).\nWhen cpusys detects an anomaly in this metric, it automatically captures system call stacks and generates flame graphs to help identify the root cause. It considers both sustained high CPU Sys usage and sudden Sys spikes, with trigger conditions including:\nCPU Sys usage \u003e Threshold A CPU Sys usage increase over a unit time \u003e Threshold B CPUIDLE In K8S container environments, a sudden drop in CPU idle time (i.e., the proportion of time the CPU is idle) usually indicates that processes within the container are excessively consuming CPU resources, potentially causing business latency, scheduling contention, or even overall system performance degradation.\ncpuidle automatically triggers the capture of call stacks to generate flame graphs. Trigger conditions:\nCPU Sys usage \u003e Threshold A CPU User usage \u003e Threshold B \u0026\u0026 CPU User usage increase over unit time \u003e Threshold C CPU Usage \u003e Threshold D \u0026\u0026 CPU Usage increase over unit time \u003e Threshold E DLOAD The D state is a special process state where a process is blocked waiting for kernel or hardware resources. Unlike normal sleep (S state), D-state processes cannot be forcibly terminated (even with SIGKILL) and do not respond to interrupt signals. This state typically occurs during I/O operations (e.g., direct disk read/write) or hardware driver failures. System D-state surges often relate to unavailable resources or long-held locks, while runnable process surges often indicate poor business logic design. dload uses netlink to obtain the count of running + uninterruptible processes in a container, calculates the D-state process contribution to the load over the past 1 minute via a sliding window algorithm. When the smoothed D-state process load value exceeds the threshold, it triggers the collection of container runtime status and D-state process information.\nMemBurst memburst detects short-term, large memory allocation events on the host. Sudden memory allocations may trigger direct reclaim or even OOM, so context information is recorded when such allocations occur.\nIOTracing When I/O bandwidth is saturated or disk access surges suddenly, the system may experience increased request latency, performance jitter, or even overall instability due to I/O resource contention.\niotracing outputs context information—such as accessed filenames/paths, disk devices, inode numbers, and container names—during periods of high host disk load or abnormal I/O latency.\n","categories":"","description":"","excerpt":"HUATUO currently supports automatic tracing for the following metrics: …","ref":"/docs/en/v2.1.0/concepts/integrated/autotracing/","tags":"","title":"Autotracing"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/en/v2.1.0/tutorials/","tags":"","title":"Tutorials"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/zh/v2.1.0/tutorials/","tags":"","title":"教程"},{"body":"在本地开发、验证使用 docker compose 是最便捷的方式，可通过该方式快速地在本地搭建部署一套完整的环境自行管理采集器、ES、prometheus、grafana 等组件。\ndocker compose --project-directory ./build/docker up docker compose 建议使用 plugins 方式安装，参考 https://docs.docker.com/compose/install/linux/\n","categories":"","description":"","excerpt":"在本地开发、验证使用 docker compose 是最便捷的方式，可通过该方式快速地在本地搭建部署一套完整的环境自行管理采集 …","ref":"/docs/zh/v2.1.0/tutorials/addcase/","tags":"","title":"添加自定义采集"},{"body":"HUATUO 已支持自动追踪指标如下：\n追踪名称 核心功能 场景 cpusys 宿主 sys 突增检测 由于系统负载异常导致业务毛刺问题 cpuidle 容器 cpu idle 掉底检测，提供调用栈，火焰图，进程上下文信息等 容器 cpu 使用异常，帮助业务描绘进程热点 dload 跟踪容器loadavg状态进程状态，自动抓取容器 D 状态进程调用信息 系统 D 状态突增通常和资源不可用或者锁被长期持有相关，R 状态进程数量突增往往是业务代码设计不合理导致 waitrate 容器资源争抢检测，容器调度被争抢时提供正在争抢的容器信息 容器被争抢可能会引起业务毛刺，已存在争抢指标缺乏具体正在争抢的容器信息，通过 waitrate 追踪可以获取参与争抢的容器信息，给混部资源隔离提供参考 memburst 记录内存突发分配时上下文信息 宿主机短时间内大量分配内存，检测宿主机上短时间内大量分配内存事件。突发性内存分配可能引发直接回收或者 oom 等 iotracing 检测宿主磁盘 IO 延迟异常。输出访问的文件名和路径、磁盘设备、inode 号、容器等上下文信息 频繁出现磁盘 IO 带宽打满、磁盘访问突增，进而导致应用请求延迟或者系统性能抖动 CPUSYS 系统态 CPU 时间反映内核执行开销，包括系统调用、中断处理、内核线程调度、内存管理及锁竞争等操作。该指标异常升高，通常表明存在内核级性能瓶颈：高频系统调用、硬件设备异常、锁争用或内存回收压力（kswapd 直接回收）等。\ncpusys 检测到该指标异常时，自动会触发抓取系统的调用栈并生成火焰图，帮助定位问题根因。 既考虑到系统 cpu sys 达到阈值，或者sys 突发毛刺带来的问题，其中触发条件如下：\nCPU Sys 使用率 \u003e 阈值 A CPU Sys 使用率单位时间内增长 \u003e 阈值 B CPUIDLE K8S 容器环境，CPU idle 时间（即 CPU 处于空闲状态的时间比例）的突然下降通常表明容器内进程正在过度消耗 CPU 资源，可能引发业务延迟、调度争抢甚至整体系统性能下降。\ncpuidle 自动会触发抓取调用栈生成火焰图，触发条件：\nCPU Sys 使用率 \u003e 阈值 A CPU User 使用率 \u003e 阈值 B \u0026\u0026 CPU User 使用率单位时间增长 \u003e 阈值 C CPU Usage \u003e 阈值 D \u0026\u0026 CPU Usage 单位时间增长 \u003e 阈值 E DLOAD D 状态是一种特殊的进程状态，指进程因等待内核或硬件资源而进入的一种特殊阻塞状态。与普通睡眠（S 状态）不同，D 状态进程无法被强制终止（包括 SIGKILL），也不会响应中断信号。该状态通常发生在 I/O 操作（如直接读写磁盘）、硬件驱动故障时。系统 D 状态突增往往和资源不可用或者锁被长期持有导致，可运行进程突增往往是业务代码设计不合理导致。dload 借助 netlink 获取容器 running + uninterruptible 进程数量，通过滑动窗口算法计算出过去 1 分钟内容器 D 进程对负载做出的贡献值，当平滑计算后的 D 状态进程负载值超过阈值的时候，表示容器内的 D 状态进程数量出现异常，开始触发收集容器运行情况、D 状态进程信息。\nMemBurst memburst 用于检测宿主机上短时间内大量分配内存的情况，突发性内存分配可能引发直接回收甚至 OOM，所以一旦突发性内存分配就需要记录相关信息。\nIOTracing 当 I/O 带宽被占满 或 磁盘访问量突增 时，系统可能因 I/O 资源竞争而出现 请求延迟升高、性能抖动，甚至影响整个系统的稳定性。\niotracing 在宿主磁盘负载高、IO 延迟异常时，输出异常时 IO 访问的文件名和路径、磁盘设备、inode 号，容器名等上下文信息。\n","categories":"","description":"","excerpt":"HUATUO 已支持自动追踪指标如下：\n追踪名称 核心功能 场景 cpusys 宿主 sys …","ref":"/docs/zh/v2.1.0/concepts/integrated/autotracing/","tags":"","title":"自动追踪（Autotracing）"},{"body":"HUATUO currently supports the following exception context capture events:\nEvent Name Core Functionality Scenarios softirq Detects delayed response or prolonged disabling of host soft interrupts, and outputs kernel call stacks and process information when soft interrupts are disabled for extended periods., etc. This type of issue severely impacts network transmission/reception, leading to business spikes or timeout issues dropwatch Detects TCP packet loss and outputs host and network context information when packet loss occurs This type of issue mainly causes business spikes and latency net_rx_latency Captures latency events in network receive path from driver, protocol stack, to user-space receive process For network latency issues in the receive direction where the exact delay location is unclear, net_rx_latency calculates latency at the driver, protocol stack, and user copy paths using skb NIC ingress timestamps, filters timeout packets via preset thresholds, and locates the delay position oom Detects OOM events on the host or within containers When OOM occurs at host level or container dimension, captures process information triggering OOM, killed process information, and container details to troubleshoot memory leaks, abnormal exits, etc. softlockup When a softlockup occurs on the system, collects target process information and CPU details, and retrieves kernel stack information from all CPUs System softlockup events hungtask Provides count of all D-state processes in the system and kernel stack information Used to locate transient D-state process scenarios, preserving the scene for later problem tracking memreclaim Records process information when memory reclamation exceeds time threshold When memory pressure is excessively high, if a process requests memory at this time, it may enter direct reclamation (synchronous phase), potentially causing business process stalls. Recording the direct reclamation entry time helps assess the severity of impact on the process netdev Detects network device status changes Network card flapping, slave abnormalities in bond environments, etc. lacp Detects LACP status changes Detects LACP negotiation status in bond mode 4 Detect the long-term disabling of soft interrupts Feature Introduction\nThe Linux kernel contains various contexts such as process context, interrupt context, soft interrupt context, and NMI context. These contexts may share data, so to ensure data consistency and correctness, kernel code might disable soft or hard interrupts. Theoretically, the duration of single interrupt or soft interrupt disabling shouldn’t be too long. However, high-frequency system calls entering kernel mode and frequently executing interrupt disabling can also create a “long-term disable” phenomenon, slowing down system response. Issues related to “long interrupt or soft interrupt disabling” are very subtle with limited troubleshooting methods, yet have significant impact, typically manifesting as receive data timeouts in business applications. For this scenario, we built BPF-based detection capabilities for long hardware and software interrupt disables.\nExample\nBelow is an example of captured instances with overly long disabling interrupts, automatically uploaded to ES:\n{ \"_index\": \"***_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-06-11T16:05:16.251152703+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"comm\": \"observe-agent\", \"stack\": \"stack:\\nscheduler_tick/ffffffffa471dbc0 [kernel]\\nupdate_process_times/ffffffffa4789240 [kernel]\\ntick_sched_handle.isra.8/ffffffffa479afa0 [kernel]\\ntick_sched_timer/ffffffffa479b000 [kernel]\\n__hrtimer_run_queues/ffffffffa4789b60 [kernel]\\nhrtimer_interrupt/ffffffffa478a610 [kernel]\\n__sysvec_apic_timer_interrupt/ffffffffa4661a60 [kernel]\\nasm_call_sysvec_on_stack/ffffffffa5201130 [kernel]\\nsysvec_apic_timer_interrupt/ffffffffa5090500 [kernel]\\nasm_sysvec_apic_timer_interrupt/ffffffffa5200d30 [kernel]\\ndump_stack/ffffffffa506335e [kernel]\\ndump_header/ffffffffa5058eb0 [kernel]\\noom_kill_process.cold.9/ffffffffa505921a [kernel]\\nout_of_memory/ffffffffa48a1740 [kernel]\\nmem_cgroup_out_of_memory/ffffffffa495ff70 [kernel]\\ntry_charge/ffffffffa4964ff0 [kernel]\\nmem_cgroup_charge/ffffffffa4968de0 [kernel]\\n__add_to_page_cache_locked/ffffffffa4895c30 [kernel]\\nadd_to_page_cache_lru/ffffffffa48961a0 [kernel]\\npagecache_get_page/ffffffffa4897ad0 [kernel]\\ngrab_cache_page_write_begin/ffffffffa4899d00 [kernel]\\niomap_write_begin/ffffffffa49fddc0 [kernel]\\niomap_write_actor/ffffffffa49fe980 [kernel]\\niomap_apply/ffffffffa49fbd20 [kernel]\\niomap_file_buffered_write/ffffffffa49fc040 [kernel]\\nxfs_file_buffered_aio_write/ffffffffc0f3bed0 [xfs]\\nnew_sync_write/ffffffffa497ffb0 [kernel]\\nvfs_write/ffffffffa4982520 [kernel]\\nksys_write/ffffffffa4982880 [kernel]\\ndo_syscall_64/ffffffffa508d190 [kernel]\\nentry_SYSCALL_64_after_hwframe/ffffffffa5200078 [kernel]\", \"now\": 5532940660025295, \"offtime\": 237328905, \"cpu\": 1, \"threshold\": 100000000, \"pid\": 688073 }, \"tracer_time\": \"2025-06-11 16:05:16.251 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-06-11 16:05:16.251 +0800\", \"region\": \"***\", \"tracer_name\": \"softirq\", \"es_index_time\": 1749629116268 }, \"fields\": { \"time\": [ \"2025-06-11T08:05:16.251Z\" ] }, \"_ignored\": [ \"tracer_data.stack\" ], \"_version\": 1, \"sort\": [ 1749629116251 ] } The local host also stores identical data:\n2025-06-11 16:05:16 *** Region=*** { \"hostname\": \"***\", \"region\": \"***\", \"uploaded_time\": \"2025-06-11T16:05:16.251152703+08:00\", \"time\": \"2025-06-11 16:05:16.251 +0800\", \"tracer_name\": \"softirq\", \"tracer_time\": \"2025-06-11 16:05:16.251 +0800\", \"tracer_type\": \"auto\", \"tracer_data\": { \"offtime\": 237328905, \"threshold\": 100000000, \"comm\": \"observe-agent\", \"pid\": 688073, \"cpu\": 1, \"now\": 5532940660025295, \"stack\": \"stack:\\nscheduler_tick/ffffffffa471dbc0 [kernel]\\nupdate_process_times/ffffffffa4789240 [kernel]\\ntick_sched_handle.isra.8/ffffffffa479afa0 [kernel]\\ntick_sched_timer/ffffffffa479b000 [kernel]\\n__hrtimer_run_queues/ffffffffa4789b60 [kernel]\\nhrtimer_interrupt/ffffffffa478a610 [kernel]\\n__sysvec_apic_timer_interrupt/ffffffffa4661a60 [kernel]\\nasm_call_sysvec_on_stack/ffffffffa5201130 [kernel]\\nsysvec_apic_timer_interrupt/ffffffffa5090500 [kernel]\\nasm_sysvec_apic_timer_interrupt/ffffffffa5200d30 [kernel]\\ndump_stack/ffffffffa506335e [kernel]\\ndump_header/ffffffffa5058eb0 [kernel]\\noom_kill_process.cold.9/ffffffffa505921a [kernel]\\nout_of_memory/ffffffffa48a1740 [kernel]\\nmem_cgroup_out_of_memory/ffffffffa495ff70 [kernel]\\ntry_charge/ffffffffa4964ff0 [kernel]\\nmem_cgroup_charge/ffffffffa4968de0 [kernel]\\n__add_to_page_cache_locked/ffffffffa4895c30 [kernel]\\nadd_to_page_cache_lru/ffffffffa48961a0 [kernel]\\npagecache_get_page/ffffffffa4897ad0 [kernel]\\ngrab_cache_page_write_begin/ffffffffa4899d00 [kernel]\\niomap_write_begin/ffffffffa49fddc0 [kernel]\\niomap_write_actor/ffffffffa49fe980 [kernel]\\niomap_apply/ffffffffa49fbd20 [kernel]\\niomap_file_buffered_write/ffffffffa49fc040 [kernel]\\nxfs_file_buffered_aio_write/ffffffffc0f3bed0 [xfs]\\nnew_sync_write/ffffffffa497ffb0 [kernel]\\nvfs_write/ffffffffa4982520 [kernel]\\nksys_write/ffffffffa4982880 [kernel]\\ndo_syscall_64/ffffffffa508d190 [kernel]\\nentry_SYSCALL_64_after_hwframe/ffffffffa5200078 [kernel]\" } } Protocol Stack Packet Loss Detection Feature Introduction\nDuring packet transmission and reception, packets may be lost due to various reasons, potentially causing business request delays or even timeouts. dropwatch uses eBPF to observe kernel network packet discards, outputting packet loss network context such as source/destination addresses, source/destination ports, seq, seqack, pid, comm, stack information, etc. dropwatch mainly detects TCP protocol-related packet loss, using pre-set probes to filter packets and determine packet loss locations for root cause analysis.\nExample\nInformation captured by dropwatch is automatically uploaded to ES. Below is an example where kubelet failed to send data packet due to device packet loss:\n{ \"_index\": \"***_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-06-11T16:58:15.100223795+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"comm\": \"kubelet\", \"stack\": \"kfree_skb/ffffffff9a0cd5c0 [kernel]\\nkfree_skb/ffffffff9a0cd5c0 [kernel]\\nkfree_skb_list/ffffffff9a0cd670 [kernel]\\n__dev_queue_xmit/ffffffff9a0ea020 [kernel]\\nip_finish_output2/ffffffff9a18a720 [kernel]\\n__ip_queue_xmit/ffffffff9a18d280 [kernel]\\n__tcp_transmit_skb/ffffffff9a1ad890 [kernel]\\ntcp_connect/ffffffff9a1ae610 [kernel]\\ntcp_v4_connect/ffffffff9a1b3450 [kernel]\\n__inet_stream_connect/ffffffff9a1d25f0 [kernel]\\ninet_stream_connect/ffffffff9a1d2860 [kernel]\\n__sys_connect/ffffffff9a0c1170 [kernel]\\n__x64_sys_connect/ffffffff9a0c1240 [kernel]\\ndo_syscall_64/ffffffff9a2ea9f0 [kernel]\\nentry_SYSCALL_64_after_hwframe/ffffffff9a400078 [kernel]\", \"saddr\": \"10.79.68.62\", \"pid\": 1687046, \"type\": \"common_drop\", \"queue_mapping\": 11, \"dport\": 2052, \"pkt_len\": 74, \"ack_seq\": 0, \"daddr\": \"10.179.142.26\", \"state\": \"SYN_SENT\", \"src_hostname\": \"***\", \"sport\": 15402, \"dest_hostname\": \"***\", \"seq\": 1902752773, \"max_ack_backlog\": 0 }, \"tracer_time\": \"2025-06-11 16:58:15.099 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-06-11 16:58:15.099 +0800\", \"region\": \"***\", \"tracer_name\": \"dropwatch\", \"es_index_time\": 1749632295120 }, \"fields\": { \"time\": [ \"2025-06-11T08:58:15.099Z\" ] }, \"_ignored\": [ \"tracer_data.stack\" ], \"_version\": 1, \"sort\": [ 1749632295099 ] } The local host also stores identical data:\n2025-06-11 16:58:15 Host=*** Region=*** { \"hostname\": \"***\", \"region\": \"***\", \"uploaded_time\": \"2025-06-11T16:58:15.100223795+08:00\", \"time\": \"2025-06-11 16:58:15.099 +0800\", \"tracer_name\": \"dropwatch\", \"tracer_time\": \"2025-06-11 16:58:15.099 +0800\", \"tracer_type\": \"auto\", \"tracer_data\": { \"type\": \"common_drop\", \"comm\": \"kubelet\", \"pid\": 1687046, \"saddr\": \"10.79.68.62\", \"daddr\": \"10.179.142.26\", \"sport\": 15402, \"dport\": 2052, \"src_hostname\": ***\", \"dest_hostname\": \"***\", \"max_ack_backlog\": 0, \"seq\": 1902752773, \"ack_seq\": 0, \"queue_mapping\": 11, \"pkt_len\": 74, \"state\": \"SYN_SENT\", \"stack\": \"kfree_skb/ffffffff9a0cd5c0 [kernel]\\nkfree_skb/ffffffff9a0cd5c0 [kernel]\\nkfree_skb_list/ffffffff9a0cd670 [kernel]\\n__dev_queue_xmit/ffffffff9a0ea020 [kernel]\\nip_finish_output2/ffffffff9a18a720 [kernel]\\n__ip_queue_xmit/ffffffff9a18d280 [kernel]\\n__tcp_transmit_skb/ffffffff9a1ad890 [kernel]\\ntcp_connect/ffffffff9a1ae610 [kernel]\\ntcp_v4_connect/ffffffff9a1b3450 [kernel]\\n__inet_stream_connect/ffffffff9a1d25f0 [kernel]\\ninet_stream_connect/ffffffff9a1d2860 [kernel]\\n__sys_connect/ffffffff9a0c1170 [kernel]\\n__x64_sys_connect/ffffffff9a0c1240 [kernel]\\ndo_syscall_64/ffffffff9a2ea9f0 [kernel]\\nentry_SYSCALL_64_after_hwframe/ffffffff9a400078 [kernel]\" } } Protocol Stack Receive Latency Feature Introduction\nOnline business network latency issues are difficult to locate, as problems can occur in any direction or stage. For example, receive direction latency might be caused by issues in drivers, protocol stack, or user programs. Therefore, we developed net_rx_latency detection functionality, leveraging skb NIC ingress timestamps to check latency at driver, protocol stack, and user-space layers. When receive latency reaches thresholds, eBPF captures network context information (five-tuple, latency location, process info, etc.). Receive path: NIC -\u003e Driver -\u003e Protocol Stack -\u003e User Active Receive\nExample\nA business container received packets from the kernel with a latency over 90 seconds, tracked via net_rx_latency, ES query output:\n{ \"_index\": \"***_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"tracer_data\": { \"dport\": 49000, \"pkt_len\": 26064, \"comm\": \"nginx\", \"ack_seq\": 689410995, \"saddr\": \"10.156.248.76\", \"pid\": 2921092, \"where\": \"TO_USER_COPY\", \"state\": \"ESTABLISHED\", \"daddr\": \"10.134.72.4\", \"sport\": 9213, \"seq\": 1009085774, \"latency_ms\": 95973 }, \"container_host_namespace\": \"***\", \"container_hostname\": \"***.docker\", \"es_index_time\": 1749628496541, \"uploaded_time\": \"2025-06-11T15:54:56.404864955+08:00\", \"hostname\": \"***\", \"container_type\": \"normal\", \"tracer_time\": \"2025-06-11 15:54:56.404 +0800\", \"time\": \"2025-06-11 15:54:56.404 +0800\", \"region\": \"***\", \"container_level\": \"1\", \"container_id\": \"***\", \"tracer_name\": \"net_rx_latency\" }, \"fields\": { \"time\": [ \"2025-06-11T07:54:56.404Z\" ] }, \"_version\": 1, \"sort\": [ 1749628496404 ] } The local host also stores identical data:\n2025-06-11 15:54:46 Host=*** Region=*** ContainerHost=***.docker ContainerID=*** ContainerType=normal ContainerLevel=1 { \"hostname\": \"***\", \"region\": \"***\", \"container_id\": \"***\", \"container_hostname\": \"***.docker\", \"container_host_namespace\": \"***\", \"container_type\": \"normal\", \"container_level\": \"1\", \"uploaded_time\": \"2025-06-11T15:54:46.129136232+08:00\", \"time\": \"2025-06-11 15:54:46.129 +0800\", \"tracer_time\": \"2025-06-11 15:54:46.129 +0800\", \"tracer_name\": \"net_rx_latency\", \"tracer_data\": { \"comm\": \"nginx\", \"pid\": 2921092, \"where\": \"TO_USER_COPY\", \"latency_ms\": 95973, \"state\": \"ESTABLISHED\", \"saddr\": \"10.156.248.76\", \"daddr\": \"10.134.72.4\", \"sport\": 9213, \"dport\": 49000, \"seq\": 1009024958, \"ack_seq\": 689410995, \"pkt_len\": 20272 } } Host/Container Memory Overused Feature Introduction\nWhen programs request more memory than available system or process limits during runtime, it can cause system or application crashes. Common in memory leaks, big data processing, or insufficient resource configuration scenarios. By inserting BPF hooks in the OOM kernel flow, detailed OOM context information is captured and passed to user space, including process information, killed process information, and container details.\nExample\nWhen OOM occurs in a container, captured information:\n{ \"_index\": \"***_cases_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-06-11T17:09:07.236482841+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"victim_process_name\": \"java\", \"trigger_memcg_css\": \"0xff4b8d8be3818000\", \"victim_container_hostname\": \"***.docker\", \"victim_memcg_css\": \"0xff4b8d8be3818000\", \"trigger_process_name\": \"java\", \"victim_pid\": 3218745, \"trigger_pid\": 3218804, \"trigger_container_hostname\": \"***.docker\", \"victim_container_id\": \"***\", \"trigger_container_id\": \"***\", \"tracer_time\": \"2025-06-11 17:09:07.236 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-06-11 17:09:07.236 +0800\", \"region\": \"***\", \"tracer_name\": \"oom\", \"es_index_time\": 1749632947258 }, \"fields\": { \"time\": [ \"2025-06-11T09:09:07.236Z\" ] }, \"_version\": 1, \"sort\": [ 1749632947236 ] } Additionally, oom event implements Collector interface, which enables collecting statistics on host OOM occurrences via Prometheus, distinguishing between events from the host and containers.\nKernel Softlockup Feature Introduction\nSoftlockup is an abnormal state detected by the Linux kernel where a kernel thread (or process) on a CPU core occupies the CPU for a long time without scheduling, preventing the system from responding normally to other tasks. Causes include kernel code bugs, CPU overload, device driver issues, and others. When a softlockup occurs in the system, information about the target process and CPU is collected, kernel stack information from all CPUs is retrieved, and the number of occurrences of the issue is recorded.\nProcess Blocking Feature Introduction\nA D-state process (also known as Uninterruptible Sleep) is a special process state indicating that the process is blocked while waiting for certain system resources and cannot be awakened by signals or external interrupts. Common scenarios include disk I/O operations, kernel blocking, hardware failures, etc. hungtask captures the kernel stacks of all D-state processes within the system and records the count of such processes. It is used to locate transient scenarios where D-state processes appear momentarily, enabling root cause analysis even after the scenario has resolved.\nExample\n{ \"_index\": \"***_2025-06-10\", \"_type\": \"_doc\", \"_id\": \"8yyOV5cBGoYArUxjSdvr\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-06-10T09:57:12.202191192+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"cpus_stack\": \"2025-06-10 09:57:14 sysrq: Show backtrace of all active CPUs\\n2025-06-10 09:57:14 NMI backtrace for cpu 33\\n2025-06-10 09:57:14 CPU: 33 PID: 768309 Comm: huatuo-bamai Kdump: loaded Tainted: G S W OEL 5.10.0-216.0.0.115.v1.0.x86_64 #1\\n2025-06-10 09:57:14 Hardware name: Inspur SA5212M5/YZMB-00882-104, BIOS 4.1.12 11/27/2019\\n2025-06-10 09:57:14 Call Trace:\\n2025-06-10 09:57:14 dump_stack+0x57/0x6e\\n2025-06-10 09:57:14 nmi_cpu_backtrace.cold.0+0x30/0x65\\n2025-06-10 09:57:14 ? lapic_can_unplug_cpu+0x80/0x80\\n2025-06-10 09:57:14 nmi_trigger_cpumask_backtrace+0xdf/0xf0\\n2025-06-10 09:57:14 arch_trigger_cpumask_backtrace+0x15/0x20\\n2025-06-10 09:57:14 sysrq_handle_showallcpus+0x14/0x90\\n2025-06-10 09:57:14 __handle_sysrq.cold.8+0x77/0xe8\\n2025-06-10 09:57:14 write_sysrq_trigger+0x3d/0x60\\n2025-06-10 09:57:14 proc_reg_write+0x38/0x80\\n2025-06-10 09:57:14 vfs_write+0xdb/0x250\\n2025-06-10 09:57:14 ksys_write+0x59/0xd0\\n2025-06-10 09:57:14 do_syscall_64+0x39/0x80\\n2025-06-10 09:57:14 entry_SYSCALL_64_after_hwframe+0x62/0xc7\\n2025-06-10 09:57:14 RIP: 0033:0x4088ae\\n2025-06-10 09:57:14 Code: 48 83 ec 38 e8 13 00 00 00 48 83 c4 38 5d c3 cc cc cc cc cc cc cc cc cc cc cc cc cc 49 89 f2 48 89 fa 48 89 ce 48 89 df 0f 05 \u003c48\u003e 3d 01 f0 ff ff 76 15 48 f7 d8 48 89 c1 48 c7 c0 ff ff ff ff 48\\n2025-06-10 09:57:14 RSP: 002b:000000c000adcc60 EFLAGS: 00000212 ORIG_RAX: 0000000000000001\\n2025-06-10 09:57:14 RAX: ffffffffffffffda RBX: 0000000000000013 RCX: 00000000004088ae\\n2025-06-10 09:57:14 RDX: 0000000000000001 RSI: 000000000274ab18 RDI: 0000000000000013\\n2025-06-10 09:57:14 RBP: 000000c000adcca0 R08: 0000000000000000 R09: 0000000000000000\\n2025-06-10 09:57:14 R10: 0000000000000000 R11: 0000000000000212 R12: 000000c000adcdc0\\n2025-06-10 09:57:14 R13: 0000000000000002 R14: 000000c000caa540 R15: 0000000000000000\\n2025-06-10 09:57:14 Sending NMI from CPU 33 to CPUs 0-32,34-95:\\n2025-06-10 09:57:14 NMI backtrace for cpu 52 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 54 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 7 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 81 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 60 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 2 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 21 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 69 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 58 skipped: idling at intel_idle+0x6f/ ... \"pid\": 2567042 }, \"tracer_time\": \"2025-06-10 09:57:12.202 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-06-10 09:57:12.202 +0800\", \"region\": \"***\", \"tracer_name\": \"hungtask\", \"es_index_time\": 1749520632297 }, \"fields\": { \"time\": [ \"2025-06-10T01:57:12.202Z\" ] }, \"_ignored\": [ \"tracer_data.blocked_processes_stack\", \"tracer_data.cpus_stack\" ], \"_version\": 1, \"sort\": [ 1749520632202 ] } Additionally, the hungtask event implements the Collector interface, which also enables collecting statistics on host hungtask occurrences via Prometheus.\nContainer/Host Memory Reclamation Feature Introduction\nWhen memory pressure is excessively high, if a process requests memory at this time, it may enter direct reclamation. This phase involves synchronous reclamation and may cause business process stalls. Recording the time when a process enters direct reclamation helps us assess the severity of impact from direct reclamation on that process. The memreclaim event calculates whether the same process remains in direct reclamation for over 900ms within a 1-second cycle; if so, it records the process’s contextual information.\nExample\nWhen a business container’s chrome process enters direct reclamation, the ES query output is as follows:\n{ \"_index\": \"***_cases_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"tracer_data\": { \"comm\": \"chrome\", \"deltatime\": 1412702917, \"pid\": 1896137 }, \"container_host_namespace\": \"***\", \"container_hostname\": \"***.docker\", \"es_index_time\": 1749641583290, \"uploaded_time\": \"2025-06-11T19:33:03.26754495+08:00\", \"hostname\": \"***\", \"container_type\": \"normal\", \"tracer_time\": \"2025-06-11 19:33:03.267 +0800\", \"time\": \"2025-06-11 19:33:03.267 +0800\", \"region\": \"***\", \"container_level\": \"102\", \"container_id\": \"921d0ec0a20c\", \"tracer_name\": \"directreclaim\" }, \"fields\": { \"time\": [ \"2025-06-11T11:33:03.267Z\" ] }, \"_version\": 1, \"sort\": [ 1749641583267 ] } Network Device Status Feature Introduction\nNetwork card status changes often cause severe network issues, directly impacting overall host network quality, such as down/up states, MTU changes, etc. Taking the down state as an example, possible causes include operations by privileged processes, underlying cable issues, optical module failures, peer switch problems, etc. The netdev event is designed to detect network device status changes and currently implements monitoring for network card down/up events, distinguishing between administrator-initiated and underlying cause-induced status changes.\nExample\nWhen an administrator operation causes the eth1 network card to go down, the ES query event output is as follows:\n{ \"_index\": \"***_cases_2025-05-30\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-05-30T17:47:50.406913037+08:00\", \"hostname\": \"localhost.localdomain\", \"tracer_data\": { \"ifname\": \"eth1\", \"start\": false, \"index\": 3, \"linkstatus\": \"linkStatusAdminDown, linkStatusCarrierDown\", \"mac\": \"5c:6f:69:34:dc:72\" }, \"tracer_time\": \"2025-05-30 17:47:50.406 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-05-30 17:47:50.406 +0800\", \"region\": \"***\", \"tracer_name\": \"netdev_event\", \"es_index_time\": 1748598470407 }, \"fields\": { \"time\": [ \"2025-05-30T09:47:50.406Z\" ] }, \"_version\": 1, \"sort\": [ 1748598470406 ] } LACP Protocol Status Feature Introduction\nBond is a technology provided by the Linux system kernel that bundles multiple physical network interfaces into a single logical interface. Through bonding, bandwidth aggregation, failover, or load balancing can be achieved. LACP is a protocol defined by the IEEE 802.3ad standard for dynamically managing Link Aggregation Groups (LAG). Currently, there is no elegant method to obtain physical host LACP protocol negotiation exception events. HUATUO implements the lacp event, which uses BPF to instrument key protocol paths. When a change in link aggregation status is detected, it triggers an event to record relevant information.\nExample\nWhen the host network card eth1 experiences physical layer down/up fluctuations, the LACP dynamic negotiation status becomes abnormal. The ES query output is as follows:\n{ \"_index\": \"***_cases_2025-05-30\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-05-30T17:47:48.513318579+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"content\": \"/proc/net/bonding/bond0\\nEthernet Channel Bonding Driver: v4.18.0 (Apr 7, 2025)\\n\\nBonding Mode: load balancing (round-robin)\\nMII Status: down\\nMII Polling Interval (ms): 0\\nUp Delay (ms): 0\\nDown Delay (ms): 0\\nPeer Notification Delay (ms): 0\\n/proc/net/bonding/bond4\\nEthernet Channel Bonding Driver: v4.18.0 (Apr 7, 2025)\\n\\nBonding Mode: IEEE 802.3ad Dynamic link aggregation\\nTransmit Hash Policy: layer3+4 (1)\\nMII Status: up\\nMII Polling Interval (ms): 100\\nUp Delay (ms): 0\\nDown Delay (ms): 0\\nPeer Notification Delay (ms): 1000\\n\\n802.3ad info\\nLACP rate: fast\\nMin links: 0\\nAggregator selection policy (ad_select): stable\\nSystem priority: 65535\\nSystem MAC address: 5c:6f:69:34:dc:72\\nActive Aggregator Info:\\n\\tAggregator ID: 1\\n\\tNumber of ports: 2\\n\\tActor Key: 21\\n\\tPartner Key: 50013\\n\\tPartner Mac Address: 00:00:5e:00:01:01\\n\\nSlave Interface: eth0\\nMII Status: up\\nSpeed: 25000 Mbps\\nDuplex: full\\nLink Failure Count: 0\\nPermanent HW addr: 5c:6f:69:34:dc:72\\nSlave queue ID: 0\\nSlave active: 1\\nSlave sm_vars: 0x172\\nAggregator ID: 1\\nAggregator active: 1\\nActor Churn State: none\\nPartner Churn State: none\\nActor Churned Count: 0\\nPartner Churned Count: 0\\ndetails actor lacp pdu:\\n system priority: 65535\\n system mac address: 5c:6f:69:34:dc:72\\n port key: 21\\n port priority: 255\\n port number: 1\\n port state: 63\\ndetails partner lacp pdu:\\n system priority: 200\\n system mac address: 00:00:5e:00:01:01\\n oper key: 50013\\n port priority: 32768\\n port number: 16397\\n port state: 63\\n\\nSlave Interface: eth1\\nMII Status: up\\nSpeed: 25000 Mbps\\nDuplex: full\\nLink Failure Count: 17\\nPermanent HW addr: 5c:6f:69:34:dc:73\\nSlave queue ID: 0\\nSlave active: 0\\nSlave sm_vars: 0x172\\nAggregator ID: 1\\nAggregator active: 1\\nActor Churn State: monitoring\\nPartner Churn State: monitoring\\nActor Churned Count: 2\\nPartner Churned Count: 2\\ndetails actor lacp pdu:\\n system priority: 65535\\n system mac address: 5c:6f:69:34:dc:72\\n port key: 21\\n port priority: 255\\n port number: 2\\n port state: 15\\ndetails partner lacp pdu:\\n system priority: 200\\n system mac address: 00:00:5e:00:01:01\\n oper key: 50013\\n port priority: 32768\\n port number: 32781\\n port state: 31\\n\" }, \"tracer_time\": \"2025-05-30 17:47:48.513 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-05-30 17:47:48.513 +0800\", \"region\": \"***\", \"tracer_name\": \"lacp\", \"es_index_time\": 1748598468514 }, \"fields\": { \"time\": [ \"2025-05-30T09:47:48.513Z\" ] }, \"_ignored\": [ \"tracer_data.content\" ], \"_version\": 1, \"sort\": [ 1748598468513 ] } ","categories":"","description":"","excerpt":"HUATUO currently supports the following exception context capture …","ref":"/docs/en/v2.1.0/concepts/integrated/events/","tags":"","title":"Events"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/en/v2.1.0/concepts/integrated/","tags":"","title":"Integrated Capability"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/zh/v2.1.0/concepts/integrated/","tags":"","title":"已集成功能"},{"body":"总览 HUATUO 目前支持的异常上下文捕获事件如下：\n事件名称 核心功能 场景 softirq 宿主软中断延迟响应或长期关闭，输出长时间关闭软中断的内核调用栈，进程信息等 该类问题会严重影响网络收发，进而导致业务毛刺或者超时等其他问题 dropwatch TCP 数据包丢包检测，输出发生丢包时主机、网络上下文信息等 该类问题主要会引起业务毛刺和延迟 net_rx_latency 在网络收方向获取数据包从驱动、协议栈、到用户主动收过程的延迟事件 网络延迟问题中有一类是数据传输阶段收方向存在延迟，但不清楚是延迟位置，net_rx_latency 根据 skb 入网卡时间戳依次在驱动、协议栈和用户拷贝数据等路径计算延迟，通过预先设定的阈值过滤超时的数据包，定位延迟位置 oom 检测宿主或容器内 oom 事件 当宿主机层面或者容器维度发生 oom 事件时，能够获取触发 oom 的进程信息、被 kill 的进程信息以及容器信息，便于定位进程内存泄漏、异常退出等问题 softlockup 当系统上发生 softlockup 时，收集目标进程信息以及 cpu 信息，同时获取各个 cpu 上的内核栈信息 系统发生 softlockup hungtask 提供系统内所有 D 状态进程数量、内核栈信息 用于定位瞬时出现 D 进程的场景，能及时保留现场便于后期问题跟踪 memreclaim 进程进入直接回收的耗时，超过时间阈值，记录进程信息 内存压力过大时，如果此时进程申请内存，有可能进入直接回收，此时处于同步回收阶段，可能会造成业务进程的卡顿，此时记录进程进入直接回收的时间，有助于我们判断此进程被直接回收影响的剧烈程度 netdev 检测网卡状态变化 网卡抖动、bond 环境下 slave 异常等 lacp 检测 lacp 状态变化 bond 模式 4 下，监控 lacp 协商状态 软中断关闭过长检测 功能介绍\nLinux 内核存在进程上下文，中断上下文，软中断上下文，NMI 上下文等概念，这些上下文之间可能存在共享数据情况，因此为了确保数据的一致性，正确性，内核代码可能会关闭软中断或者硬中断。从理论角度，单次关闭中断或者软中断时间不能太长，但高频的系统调用，陷入内核态频繁执行关闭中断或软中断，同样会造\"长时间关闭\"的现象，拖慢了系统的响应。“关闭中断，软中断时间过长”这类问题非常隐蔽，且定位手段有限，同时影响又非常大，体现在业务应用上一般为接收数据超时。针对这种场景我们基于BPF技术构建了检测硬件中断，软件中断关闭过长的能力。\n示例\n如下为抓取到的关闭中断过长的实例，这些信息被自动上传到 ES.\n{ \"_index\": \"***_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-06-11T16:05:16.251152703+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"comm\": \"observe-agent\", \"stack\": \"stack:\\nscheduler_tick/ffffffffa471dbc0 [kernel]\\nupdate_process_times/ffffffffa4789240 [kernel]\\ntick_sched_handle.isra.8/ffffffffa479afa0 [kernel]\\ntick_sched_timer/ffffffffa479b000 [kernel]\\n__hrtimer_run_queues/ffffffffa4789b60 [kernel]\\nhrtimer_interrupt/ffffffffa478a610 [kernel]\\n__sysvec_apic_timer_interrupt/ffffffffa4661a60 [kernel]\\nasm_call_sysvec_on_stack/ffffffffa5201130 [kernel]\\nsysvec_apic_timer_interrupt/ffffffffa5090500 [kernel]\\nasm_sysvec_apic_timer_interrupt/ffffffffa5200d30 [kernel]\\ndump_stack/ffffffffa506335e [kernel]\\ndump_header/ffffffffa5058eb0 [kernel]\\noom_kill_process.cold.9/ffffffffa505921a [kernel]\\nout_of_memory/ffffffffa48a1740 [kernel]\\nmem_cgroup_out_of_memory/ffffffffa495ff70 [kernel]\\ntry_charge/ffffffffa4964ff0 [kernel]\\nmem_cgroup_charge/ffffffffa4968de0 [kernel]\\n__add_to_page_cache_locked/ffffffffa4895c30 [kernel]\\nadd_to_page_cache_lru/ffffffffa48961a0 [kernel]\\npagecache_get_page/ffffffffa4897ad0 [kernel]\\ngrab_cache_page_write_begin/ffffffffa4899d00 [kernel]\\niomap_write_begin/ffffffffa49fddc0 [kernel]\\niomap_write_actor/ffffffffa49fe980 [kernel]\\niomap_apply/ffffffffa49fbd20 [kernel]\\niomap_file_buffered_write/ffffffffa49fc040 [kernel]\\nxfs_file_buffered_aio_write/ffffffffc0f3bed0 [xfs]\\nnew_sync_write/ffffffffa497ffb0 [kernel]\\nvfs_write/ffffffffa4982520 [kernel]\\nksys_write/ffffffffa4982880 [kernel]\\ndo_syscall_64/ffffffffa508d190 [kernel]\\nentry_SYSCALL_64_after_hwframe/ffffffffa5200078 [kernel]\", \"now\": 5532940660025295, \"offtime\": 237328905, \"cpu\": 1, \"threshold\": 100000000, \"pid\": 688073 }, \"tracer_time\": \"2025-06-11 16:05:16.251 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-06-11 16:05:16.251 +0800\", \"region\": \"***\", \"tracer_name\": \"softirq\", \"es_index_time\": 1749629116268 }, \"fields\": { \"time\": [ \"2025-06-11T08:05:16.251Z\" ] }, \"_ignored\": [ \"tracer_data.stack\" ], \"_version\": 1, \"sort\": [ 1749629116251 ] } 本地物理机也会存储一份相同的数据：\n2025-06-11 16:05:16 *** Region=*** { \"hostname\": \"***\", \"region\": \"***\", \"uploaded_time\": \"2025-06-11T16:05:16.251152703+08:00\", \"time\": \"2025-06-11 16:05:16.251 +0800\", \"tracer_name\": \"softirq\", \"tracer_time\": \"2025-06-11 16:05:16.251 +0800\", \"tracer_type\": \"auto\", \"tracer_data\": { \"offtime\": 237328905, \"threshold\": 100000000, \"comm\": \"observe-agent\", \"pid\": 688073, \"cpu\": 1, \"now\": 5532940660025295, \"stack\": \"stack:\\nscheduler_tick/ffffffffa471dbc0 [kernel]\\nupdate_process_times/ffffffffa4789240 [kernel]\\ntick_sched_handle.isra.8/ffffffffa479afa0 [kernel]\\ntick_sched_timer/ffffffffa479b000 [kernel]\\n__hrtimer_run_queues/ffffffffa4789b60 [kernel]\\nhrtimer_interrupt/ffffffffa478a610 [kernel]\\n__sysvec_apic_timer_interrupt/ffffffffa4661a60 [kernel]\\nasm_call_sysvec_on_stack/ffffffffa5201130 [kernel]\\nsysvec_apic_timer_interrupt/ffffffffa5090500 [kernel]\\nasm_sysvec_apic_timer_interrupt/ffffffffa5200d30 [kernel]\\ndump_stack/ffffffffa506335e [kernel]\\ndump_header/ffffffffa5058eb0 [kernel]\\noom_kill_process.cold.9/ffffffffa505921a [kernel]\\nout_of_memory/ffffffffa48a1740 [kernel]\\nmem_cgroup_out_of_memory/ffffffffa495ff70 [kernel]\\ntry_charge/ffffffffa4964ff0 [kernel]\\nmem_cgroup_charge/ffffffffa4968de0 [kernel]\\n__add_to_page_cache_locked/ffffffffa4895c30 [kernel]\\nadd_to_page_cache_lru/ffffffffa48961a0 [kernel]\\npagecache_get_page/ffffffffa4897ad0 [kernel]\\ngrab_cache_page_write_begin/ffffffffa4899d00 [kernel]\\niomap_write_begin/ffffffffa49fddc0 [kernel]\\niomap_write_actor/ffffffffa49fe980 [kernel]\\niomap_apply/ffffffffa49fbd20 [kernel]\\niomap_file_buffered_write/ffffffffa49fc040 [kernel]\\nxfs_file_buffered_aio_write/ffffffffc0f3bed0 [xfs]\\nnew_sync_write/ffffffffa497ffb0 [kernel]\\nvfs_write/ffffffffa4982520 [kernel]\\nksys_write/ffffffffa4982880 [kernel]\\ndo_syscall_64/ffffffffa508d190 [kernel]\\nentry_SYSCALL_64_after_hwframe/ffffffffa5200078 [kernel]\" } } 协议栈丢包检测 功能介绍\n在数据包收发过程中由于各类原因，可能出现丢包的现象，丢包可能会导致业务请求延迟，甚至超时。dropwatch 借助 eBPF 观测内核网络数据包丢弃情况，输出丢包网络上下文，如：源目的地址，源目的端口，seq, seqack, pid, comm, stack 信息等。dorpwatch 主要用于检测 TCP 协议相关的丢包，通过预先埋点过滤数据包，确定丢包位置以便于排查丢包根因。\n示例\n通过 dropwatch 抓取到的相关信息会自动上传到 ES。如下为抓取到的一案例：kubelet 在发送 SYN 时，由于设备丢包，导致数据包发送失败。\n{ \"_index\": \"***_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-06-11T16:58:15.100223795+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"comm\": \"kubelet\", \"stack\": \"kfree_skb/ffffffff9a0cd5c0 [kernel]\\nkfree_skb/ffffffff9a0cd5c0 [kernel]\\nkfree_skb_list/ffffffff9a0cd670 [kernel]\\n__dev_queue_xmit/ffffffff9a0ea020 [kernel]\\nip_finish_output2/ffffffff9a18a720 [kernel]\\n__ip_queue_xmit/ffffffff9a18d280 [kernel]\\n__tcp_transmit_skb/ffffffff9a1ad890 [kernel]\\ntcp_connect/ffffffff9a1ae610 [kernel]\\ntcp_v4_connect/ffffffff9a1b3450 [kernel]\\n__inet_stream_connect/ffffffff9a1d25f0 [kernel]\\ninet_stream_connect/ffffffff9a1d2860 [kernel]\\n__sys_connect/ffffffff9a0c1170 [kernel]\\n__x64_sys_connect/ffffffff9a0c1240 [kernel]\\ndo_syscall_64/ffffffff9a2ea9f0 [kernel]\\nentry_SYSCALL_64_after_hwframe/ffffffff9a400078 [kernel]\", \"saddr\": \"10.79.68.62\", \"pid\": 1687046, \"type\": \"common_drop\", \"queue_mapping\": 11, \"dport\": 2052, \"pkt_len\": 74, \"ack_seq\": 0, \"daddr\": \"10.179.142.26\", \"state\": \"SYN_SENT\", \"src_hostname\": \"***\", \"sport\": 15402, \"dest_hostname\": \"***\", \"seq\": 1902752773, \"max_ack_backlog\": 0 }, \"tracer_time\": \"2025-06-11 16:58:15.099 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-06-11 16:58:15.099 +0800\", \"region\": \"***\", \"tracer_name\": \"dropwatch\", \"es_index_time\": 1749632295120 }, \"fields\": { \"time\": [ \"2025-06-11T08:58:15.099Z\" ] }, \"_ignored\": [ \"tracer_data.stack\" ], \"_version\": 1, \"sort\": [ 1749632295099 ] } 本地物理机也会存储一份相同的数据：\n2025-06-11 16:58:15 Host=*** Region=*** { \"hostname\": \"***\", \"region\": \"***\", \"uploaded_time\": \"2025-06-11T16:58:15.100223795+08:00\", \"time\": \"2025-06-11 16:58:15.099 +0800\", \"tracer_name\": \"dropwatch\", \"tracer_time\": \"2025-06-11 16:58:15.099 +0800\", \"tracer_type\": \"auto\", \"tracer_data\": { \"type\": \"common_drop\", \"comm\": \"kubelet\", \"pid\": 1687046, \"saddr\": \"10.79.68.62\", \"daddr\": \"10.179.142.26\", \"sport\": 15402, \"dport\": 2052, \"src_hostname\": ***\", \"dest_hostname\": \"***\", \"max_ack_backlog\": 0, \"seq\": 1902752773, \"ack_seq\": 0, \"queue_mapping\": 11, \"pkt_len\": 74, \"state\": \"SYN_SENT\", \"stack\": \"kfree_skb/ffffffff9a0cd5c0 [kernel]\\nkfree_skb/ffffffff9a0cd5c0 [kernel]\\nkfree_skb_list/ffffffff9a0cd670 [kernel]\\n__dev_queue_xmit/ffffffff9a0ea020 [kernel]\\nip_finish_output2/ffffffff9a18a720 [kernel]\\n__ip_queue_xmit/ffffffff9a18d280 [kernel]\\n__tcp_transmit_skb/ffffffff9a1ad890 [kernel]\\ntcp_connect/ffffffff9a1ae610 [kernel]\\ntcp_v4_connect/ffffffff9a1b3450 [kernel]\\n__inet_stream_connect/ffffffff9a1d25f0 [kernel]\\ninet_stream_connect/ffffffff9a1d2860 [kernel]\\n__sys_connect/ffffffff9a0c1170 [kernel]\\n__x64_sys_connect/ffffffff9a0c1240 [kernel]\\ndo_syscall_64/ffffffff9a2ea9f0 [kernel]\\nentry_SYSCALL_64_after_hwframe/ffffffff9a400078 [kernel]\" } } 协议栈收包延迟 功能介绍\n线上业务网络延迟问题是比较难定位的，任何方向，任何的阶段都有可能出现问题。比如收方向的延迟，驱动、协议栈、用户程序等都有可能出现问题，因此我们开发了 net_rx_latency 检测功能，借助 skb 入网卡的时间戳，在驱动，协议栈层，用户态层检查延迟时间，当收包延迟达到阈值时，借助 eBPF 获取网络上下文信息（五元组、延迟位置、进程信息等）。收方向传输路径示意：网卡 -\u003e 驱动 -\u003e 协议栈 -\u003e 用户主动收\n示例\n一个业务容器从内核收包延迟超过 90s，通过 net_rx_latency 追踪，ES 查询输出如下：\n{ \"_index\": \"***_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"tracer_data\": { \"dport\": 49000, \"pkt_len\": 26064, \"comm\": \"nginx\", \"ack_seq\": 689410995, \"saddr\": \"10.156.248.76\", \"pid\": 2921092, \"where\": \"TO_USER_COPY\", \"state\": \"ESTABLISHED\", \"daddr\": \"10.134.72.4\", \"sport\": 9213, \"seq\": 1009085774, \"latency_ms\": 95973 }, \"container_host_namespace\": \"***\", \"container_hostname\": \"***.docker\", \"es_index_time\": 1749628496541, \"uploaded_time\": \"2025-06-11T15:54:56.404864955+08:00\", \"hostname\": \"***\", \"container_type\": \"normal\", \"tracer_time\": \"2025-06-11 15:54:56.404 +0800\", \"time\": \"2025-06-11 15:54:56.404 +0800\", \"region\": \"***\", \"container_level\": \"1\", \"container_id\": \"***\", \"tracer_name\": \"net_rx_latency\" }, \"fields\": { \"time\": [ \"2025-06-11T07:54:56.404Z\" ] }, \"_version\": 1, \"sort\": [ 1749628496404 ] } 本地物理机也会存储一份相同的数据：\n2025-06-11 15:54:46 Host=*** Region=*** ContainerHost=***.docker ContainerID=*** ContainerType=normal ContainerLevel=1 { \"hostname\": \"***\", \"region\": \"***\", \"container_id\": \"***\", \"container_hostname\": \"***.docker\", \"container_host_namespace\": \"***\", \"container_type\": \"normal\", \"container_level\": \"1\", \"uploaded_time\": \"2025-06-11T15:54:46.129136232+08:00\", \"time\": \"2025-06-11 15:54:46.129 +0800\", \"tracer_time\": \"2025-06-11 15:54:46.129 +0800\", \"tracer_name\": \"net_rx_latency\", \"tracer_data\": { \"comm\": \"nginx\", \"pid\": 2921092, \"where\": \"TO_USER_COPY\", \"latency_ms\": 95973, \"state\": \"ESTABLISHED\", \"saddr\": \"10.156.248.76\", \"daddr\": \"10.134.72.4\", \"sport\": 9213, \"dport\": 49000, \"seq\": 1009024958, \"ack_seq\": 689410995, \"pkt_len\": 20272 } } 物理机、容器内存超用 功能介绍\n程序运行时申请的内存超过了系统或进程可用的内存上限，导致系统或应用程序崩溃。常见于内存泄漏、大数据处理或资源配置不足的场景。通过在 oom 的内核流程插入 BPF 钩子，获取 oom 上下文的详细信息并传递到用户态。这些信息包括进程信息、被 kill 的进程信息、容器信息。\n示例\n一个容器内发生 oom 时，被抓取的信息如下：\n{ \"_index\": \"***_cases_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-06-11T17:09:07.236482841+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"victim_process_name\": \"java\", \"trigger_memcg_css\": \"0xff4b8d8be3818000\", \"victim_container_hostname\": \"***.docker\", \"victim_memcg_css\": \"0xff4b8d8be3818000\", \"trigger_process_name\": \"java\", \"victim_pid\": 3218745, \"trigger_pid\": 3218804, \"trigger_container_hostname\": \"***.docker\", \"victim_container_id\": \"***\", \"trigger_container_id\": \"***\", \"tracer_time\": \"2025-06-11 17:09:07.236 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-06-11 17:09:07.236 +0800\", \"region\": \"***\", \"tracer_name\": \"oom\", \"es_index_time\": 1749632947258 }, \"fields\": { \"time\": [ \"2025-06-11T09:09:07.236Z\" ] }, \"_version\": 1, \"sort\": [ 1749632947236 ] } 另外 oom event 还实现了 Collector 接口，这样还会通过 Prometheus 统计宿主 oom 发生的次数，并区分宿主机和容器的事件。\n内核 softlockup 功能介绍\nsoftlockup 是 Linux 内核检测到的一种异常状态，指某个 CPU 核心上的内核线程（或进程）长时间占用 CPU 且不调度，导致系统无法正常响应其他任务。如内核代码 bug、cpu 过载、设备驱动问题等都会导致 softlockup。当系统发生 softlockup 时，收集目标进程的信息以及 cpu 信息，获取各个 cpu 上的内核栈信息同时保存问题的发生次数。\n进程阻塞 功能介绍\nD 状态进程（也称为不可中断睡眠状态，Uninterruptible）是一种特殊的进程状态，表示进程因等待某些系统资源而阻塞，且不能被信号或外部中断唤醒。常见场景如：磁盘 I/O 操作、内核阻塞、硬件故障等。hungtask 捕获系统内所有 D 状态进程的内核栈并保存 D 进程的数量。用于定位瞬间出现一些 D 进程的场景，可以在现场消失后仍然分析到问题根因。\n示例\n{ \"_index\": \"***_2025-06-10\", \"_type\": \"_doc\", \"_id\": \"8yyOV5cBGoYArUxjSdvr\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-06-10T09:57:12.202191192+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"cpus_stack\": \"2025-06-10 09:57:14 sysrq: Show backtrace of all active CPUs\\n2025-06-10 09:57:14 NMI backtrace for cpu 33\\n2025-06-10 09:57:14 CPU: 33 PID: 768309 Comm: huatuo-bamai Kdump: loaded Tainted: G S W OEL 5.10.0-216.0.0.115.v1.0.x86_64 #1\\n2025-06-10 09:57:14 Hardware name: Inspur SA5212M5/YZMB-00882-104, BIOS 4.1.12 11/27/2019\\n2025-06-10 09:57:14 Call Trace:\\n2025-06-10 09:57:14 dump_stack+0x57/0x6e\\n2025-06-10 09:57:14 nmi_cpu_backtrace.cold.0+0x30/0x65\\n2025-06-10 09:57:14 ? lapic_can_unplug_cpu+0x80/0x80\\n2025-06-10 09:57:14 nmi_trigger_cpumask_backtrace+0xdf/0xf0\\n2025-06-10 09:57:14 arch_trigger_cpumask_backtrace+0x15/0x20\\n2025-06-10 09:57:14 sysrq_handle_showallcpus+0x14/0x90\\n2025-06-10 09:57:14 __handle_sysrq.cold.8+0x77/0xe8\\n2025-06-10 09:57:14 write_sysrq_trigger+0x3d/0x60\\n2025-06-10 09:57:14 proc_reg_write+0x38/0x80\\n2025-06-10 09:57:14 vfs_write+0xdb/0x250\\n2025-06-10 09:57:14 ksys_write+0x59/0xd0\\n2025-06-10 09:57:14 do_syscall_64+0x39/0x80\\n2025-06-10 09:57:14 entry_SYSCALL_64_after_hwframe+0x62/0xc7\\n2025-06-10 09:57:14 RIP: 0033:0x4088ae\\n2025-06-10 09:57:14 Code: 48 83 ec 38 e8 13 00 00 00 48 83 c4 38 5d c3 cc cc cc cc cc cc cc cc cc cc cc cc cc 49 89 f2 48 89 fa 48 89 ce 48 89 df 0f 05 \u003c48\u003e 3d 01 f0 ff ff 76 15 48 f7 d8 48 89 c1 48 c7 c0 ff ff ff ff 48\\n2025-06-10 09:57:14 RSP: 002b:000000c000adcc60 EFLAGS: 00000212 ORIG_RAX: 0000000000000001\\n2025-06-10 09:57:14 RAX: ffffffffffffffda RBX: 0000000000000013 RCX: 00000000004088ae\\n2025-06-10 09:57:14 RDX: 0000000000000001 RSI: 000000000274ab18 RDI: 0000000000000013\\n2025-06-10 09:57:14 RBP: 000000c000adcca0 R08: 0000000000000000 R09: 0000000000000000\\n2025-06-10 09:57:14 R10: 0000000000000000 R11: 0000000000000212 R12: 000000c000adcdc0\\n2025-06-10 09:57:14 R13: 0000000000000002 R14: 000000c000caa540 R15: 0000000000000000\\n2025-06-10 09:57:14 Sending NMI from CPU 33 to CPUs 0-32,34-95:\\n2025-06-10 09:57:14 NMI backtrace for cpu 52 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 54 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 7 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 81 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 60 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 2 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 21 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 69 skipped: idling at intel_idle+0x6f/0xc0\\n2025-06-10 09:57:14 NMI backtrace for cpu 58 skipped: idling at intel_idle+0x6f/ ... \"pid\": 2567042 }, \"tracer_time\": \"2025-06-10 09:57:12.202 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-06-10 09:57:12.202 +0800\", \"region\": \"***\", \"tracer_name\": \"hungtask\", \"es_index_time\": 1749520632297 }, \"fields\": { \"time\": [ \"2025-06-10T01:57:12.202Z\" ] }, \"_ignored\": [ \"tracer_data.blocked_processes_stack\", \"tracer_data.cpus_stack\" ], \"_version\": 1, \"sort\": [ 1749520632202 ] } 另外 hungtask event 还实现了 Collector 接口，这样还会通过 Prometheus 统计宿主 hungtask 发生的次数。\n容器、物理机内存回收 功能介绍\n内存压力过大时，如果此时进程申请内存，有可能进入直接回收，此时处于同步回收阶段，可能会造成业务进程的卡顿，在此记录进程进入直接回收的时间，有助于我们判断此进程被直接回收影响的剧烈程度。memreclaim event 计算同一个进程在 1s 周期，若进程处在直接回收状态超过 900ms， 则记录其上下文信息。\n示例\n业务容器的 chrome 进程进入直接回收状态，ES 查询输出如下：\n{ \"_index\": \"***_cases_2025-06-11\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"tracer_data\": { \"comm\": \"chrome\", \"deltatime\": 1412702917, \"pid\": 1896137 }, \"container_host_namespace\": \"***\", \"container_hostname\": \"***.docker\", \"es_index_time\": 1749641583290, \"uploaded_time\": \"2025-06-11T19:33:03.26754495+08:00\", \"hostname\": \"***\", \"container_type\": \"normal\", \"tracer_time\": \"2025-06-11 19:33:03.267 +0800\", \"time\": \"2025-06-11 19:33:03.267 +0800\", \"region\": \"***\", \"container_level\": \"102\", \"container_id\": \"921d0ec0a20c\", \"tracer_name\": \"directreclaim\" }, \"fields\": { \"time\": [ \"2025-06-11T11:33:03.267Z\" ] }, \"_version\": 1, \"sort\": [ 1749641583267 ] } 网络设备状态 功能介绍\n网卡状态变化通常容易造成严重的网络问题，直接影响整机网络质量，如 down/up, MTU 改变等。以 down 状态为例，可能是有权限的进程操作、底层线缆、光模块、对端交换机等问题导致，netdev event 用于检测网络设备的状态变化，目前已实现网卡 down, up 的监控，并区分管理员或底层原因导致的网卡状态变化。\n示例\n一次管理员操作导致 eth1 网卡 down 时，ES 查询到事件输出如下：\n{ \"_index\": \"***_cases_2025-05-30\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-05-30T17:47:50.406913037+08:00\", \"hostname\": \"localhost.localdomain\", \"tracer_data\": { \"ifname\": \"eth1\", \"start\": false, \"index\": 3, \"linkstatus\": \"linkStatusAdminDown, linkStatusCarrierDown\", \"mac\": \"5c:6f:69:34:dc:72\" }, \"tracer_time\": \"2025-05-30 17:47:50.406 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-05-30 17:47:50.406 +0800\", \"region\": \"***\", \"tracer_name\": \"netdev_event\", \"es_index_time\": 1748598470407 }, \"fields\": { \"time\": [ \"2025-05-30T09:47:50.406Z\" ] }, \"_version\": 1, \"sort\": [ 1748598470406 ] } LACP 协议状态 功能介绍\nBond 是 Linux 系统内核提供的一种将多个物理网络接口绑定为一个逻辑接口的技术。通过绑定，可以实现带宽叠加、故障切换或负载均衡。LACP 是 IEEE 802.3ad 标准定义的协议，用于动态管理链路聚合组（LAG）。目前没有优雅获取物理机LACP 协议协商异常事件的方法，HUATUO 实现了 lacp event，通过 BPF 在协议关键路径插桩检测到链路聚合状态发生变化时，触发事件记录相关信息。\n示例\n在宿主网卡 eth1 出现物理层 down/up 抖动时，lacp 动态协商状态异常，ES 查询输出如下：\n{ \"_index\": \"***_cases_2025-05-30\", \"_type\": \"_doc\", \"_id\": \"***\", \"_score\": 0, \"_source\": { \"uploaded_time\": \"2025-05-30T17:47:48.513318579+08:00\", \"hostname\": \"***\", \"tracer_data\": { \"content\": \"/proc/net/bonding/bond0\\nEthernet Channel Bonding Driver: v4.18.0 (Apr 7, 2025)\\n\\nBonding Mode: load balancing (round-robin)\\nMII Status: down\\nMII Polling Interval (ms): 0\\nUp Delay (ms): 0\\nDown Delay (ms): 0\\nPeer Notification Delay (ms): 0\\n/proc/net/bonding/bond4\\nEthernet Channel Bonding Driver: v4.18.0 (Apr 7, 2025)\\n\\nBonding Mode: IEEE 802.3ad Dynamic link aggregation\\nTransmit Hash Policy: layer3+4 (1)\\nMII Status: up\\nMII Polling Interval (ms): 100\\nUp Delay (ms): 0\\nDown Delay (ms): 0\\nPeer Notification Delay (ms): 1000\\n\\n802.3ad info\\nLACP rate: fast\\nMin links: 0\\nAggregator selection policy (ad_select): stable\\nSystem priority: 65535\\nSystem MAC address: 5c:6f:69:34:dc:72\\nActive Aggregator Info:\\n\\tAggregator ID: 1\\n\\tNumber of ports: 2\\n\\tActor Key: 21\\n\\tPartner Key: 50013\\n\\tPartner Mac Address: 00:00:5e:00:01:01\\n\\nSlave Interface: eth0\\nMII Status: up\\nSpeed: 25000 Mbps\\nDuplex: full\\nLink Failure Count: 0\\nPermanent HW addr: 5c:6f:69:34:dc:72\\nSlave queue ID: 0\\nSlave active: 1\\nSlave sm_vars: 0x172\\nAggregator ID: 1\\nAggregator active: 1\\nActor Churn State: none\\nPartner Churn State: none\\nActor Churned Count: 0\\nPartner Churned Count: 0\\ndetails actor lacp pdu:\\n system priority: 65535\\n system mac address: 5c:6f:69:34:dc:72\\n port key: 21\\n port priority: 255\\n port number: 1\\n port state: 63\\ndetails partner lacp pdu:\\n system priority: 200\\n system mac address: 00:00:5e:00:01:01\\n oper key: 50013\\n port priority: 32768\\n port number: 16397\\n port state: 63\\n\\nSlave Interface: eth1\\nMII Status: up\\nSpeed: 25000 Mbps\\nDuplex: full\\nLink Failure Count: 17\\nPermanent HW addr: 5c:6f:69:34:dc:73\\nSlave queue ID: 0\\nSlave active: 0\\nSlave sm_vars: 0x172\\nAggregator ID: 1\\nAggregator active: 1\\nActor Churn State: monitoring\\nPartner Churn State: monitoring\\nActor Churned Count: 2\\nPartner Churned Count: 2\\ndetails actor lacp pdu:\\n system priority: 65535\\n system mac address: 5c:6f:69:34:dc:72\\n port key: 21\\n port priority: 255\\n port number: 2\\n port state: 15\\ndetails partner lacp pdu:\\n system priority: 200\\n system mac address: 00:00:5e:00:01:01\\n oper key: 50013\\n port priority: 32768\\n port number: 32781\\n port state: 31\\n\" }, \"tracer_time\": \"2025-05-30 17:47:48.513 +0800\", \"tracer_type\": \"auto\", \"time\": \"2025-05-30 17:47:48.513 +0800\", \"region\": \"***\", \"tracer_name\": \"lacp\", \"es_index_time\": 1748598468514 }, \"fields\": { \"time\": [ \"2025-05-30T09:47:48.513Z\" ] }, \"_ignored\": [ \"tracer_data.content\" ], \"_version\": 1, \"sort\": [ 1748598468513 ] } ","categories":"","description":"","excerpt":"总览 HUATUO 目前支持的异常上下文捕获事件如下：\n事件名称 核心功能 场景 softirq 宿主软中断延迟响应或长期关闭，输出长时间关 …","ref":"/docs/zh/v2.1.0/concepts/integrated/events/","tags":"","title":"异常事件（Event）"},{"body":" Subsystem Metric Description Unit Dimension Source cpu cpu_util_sys Time of running kernel processes percentage of host % host Calculate base on cpuacct.stat and cpuacct.usage cpu cpu_util_usr Time of running user processes percentage of host % host Calculate base on cpuacct.stat and cpuacct.usage cpu cpu_util_total Total time of running percentage of host % host Calculate base on cpuacct.stat and cpuacct.usage cpu cpu_util_container_sys Time of running kernel processes percentage of container % container Calculate base on cpuacct.stat and cpuacct.usage cpu cpu_util_container_usr Time of running user processes percentage of container % container Calculate base on cpuacct.stat and cpuacct.usage cpu cpu_util_container_total Total time of running percentage of container % container Calculate base on cpuacct.stat and cpuacct.usage cpu cpu_stat_container_burst_time Cumulative wall-time (in nanoseconds) that any CPUs has used above quota in respective periods ns container cpu.stat cpu cpu_stat_container_nr_bursts Number of periods burst occurs count container cpu.stat cpu cpu_stat_container_nr_throttled Number of times the group has been throttled/limited count container cpu.stat cpu cpu_stat_container_exter_wait_rate Wait rate caused by processes outside the container % container Calculate base on throttled_time/hierarchy_wait_sum/inner_wait_sum read from cpu.stat cpu cpu_stat_container_inner_wait_rate Wait rate caused by processes inside the container % container Calculate base on throttled_time/hierarchy_wait_sum/inner_wait_sum read from cpu.stat cpu cpu_stat_container_throttle_wait_rate Wait rate caused by throttle of container % container Calculate base on throttled_time/hierarchy_wait_sum/inner_wait_sum read from cpu.stat cpu cpu_stat_container_wait_rate Total wait rate: exter_wait_rate + inner_wait_rate + throttle_wait_rate % container Calculate base on throttled_time/hierarchy_wait_sum/inner_wait_sum read from cpu.stat cpu loadavg_container_container_nr_running The number of running tasks in the container count container get from kernel via netlink cpu loadavg_container_container_nr_uninterruptible The number of uninterruptible tasks in the container count container get from kernel via netlink cpu loadavg_load1 System load avg over the last 1 minute count host proc fs cpu loadavg_load5 System load avg over the last 5 minute count host proc fs cpu loadavg_load15 system load avg over the last 15 minute count host proc fs cpu monsoftirq_latency The number of NET_RX/NET_TX irq latency happend in the following regions:\n0~10 us\n100us ~ 1ms\n10us ~ 100us\n1ms ~ inf count host hook the softirq event and do time statistics via bpf cpu runqlat_container_nlat_01 The number of times when schedule latency of processes in the container is within 0~10ms count container hook the scheduling switch event and do time statistics via bpf cpu runqlat_container_nlat_02 The number of times when schedule latency of processes in the container is within 10~20ms count container hook the scheduling switch event and do time statistics via bpf cpu runqlat_container_nlat_03 The number of times when schedule latency of processes in the container is within 20~50ms count container hook the scheduling switch event and do time statistics via bpf cpu runqlat_container_nlat_04 The number of times when schedule latency of processes in the container is more than 50ms count container hook the scheduling switch event and do time statistics via bpf cpu runqlat_g_nlat_01 The number of times when schedule latency of processes in the host is within\n0~10ms count host hook the scheduling switch event and do time statistics via bpf cpu runqlat_g_nlat_02 The number of times when schedule latency of processes in the host is within 10~20ms count host hook the scheduling switch event and do time statistics via bpf cpu runqlat_g_nlat_03 The number of times when schedule latency of processes in the host is within 20~50ms count host hook the scheduling switch event and do time statistics via bpf cpu runqlat_g_nlat_04 The number of times when schedule latency of processes in the host is more than 50ms count host hook the scheduling switch event and do time statistics via bpf cpu reschedipi_oversell_probability The possibility of cpu overselling exists on the host where the vm is located 0-1 host hook the scheduling ipi event and do time statistics via bpf memory buddyinfo_blocks Kernel memory allocator information pages host proc fs memory memory_events_container_watermark_inc Counts of memory allocation watermark increasing count container memory.events memory memory_events_container_watermark_dec Counts of memory allocation watermark decreasing count container memory.events memory memory_others_container_local_direct_reclaim_time Time speed in page allocation in memory cgroup nanosecond container memory.local_direct_reclaim_time memory memory_others_container_directstall_time Memory cgroup’s direct reclaim time in try_charge nanosecond container memory.directstall_stat memory memory_others_container_asyncreclaim_time Memory cgroup’s direct reclaim time in cgroup async memory reclaim nanosecond container memory.asynreclaim_stat memory priority_reclaim_kswapd Kswapd’s reclaim stat in priority reclaiming pages host proc fs memory priority_reclaim_direct Direct reclaim stat in priority reclaiming pages host proc fs memory memory_stat_container_writeback Bytes of file/anon cache that are queued for syncing to disk bytes container memory.stat memory memory_stat_container_unevictable Bytes of memory that cannot be reclaimed (mlocked etc) bytes container memory.stat memory memory_stat_container_shmem Bytes of shmem memory bytes container memory.stat memory memory_stat_container_pgsteal_kswapd Bytes of reclaimed memory by kswapd and cswapd bytes container memory.stat memory memory_stat_container_pgsteal_globalkswapd Bytes of reclaimed memory by kswapd bytes container memory.stat memory memory_stat_container_pgsteal_globaldirect Bytes of reclaimed memory by direct reclaim during page allocation bytes container memory.stat memory memory_stat_container_pgsteal_direct Bytes of reclaimed memory by direct reclaim during page allocation and try_charge bytes container memory.stat memory memory_stat_container_pgsteal_cswapd Bytes of reclaimed memory by cswapd bytes container memory.stat memory memory_stat_container_pgscan_kswapd Bytes of scanned memory by kswapd and cswapd bytes container memory.stat memory memory_stat_container_pgscan_globalkswapd Bytes of scanned memory by kswapd bytes container memory.stat memory memory_stat_container_pgscan_globaldirect Bytes of scanned memory by direct reclaim during page allocation bytes container memory.stat memory memory_stat_container_pgscan_direct Bytes of scanned memory by direct reclaim during page allocation and try_charge bytes container memory.stat memory memory_stat_container_pgscan_cswapd Bytes of scanned memory by cswapd bytes container memory.stat memory memory_stat_container_pgrefill Bytes of memory that is scanned in active list bytes container memory.stat memory memory_stat_container_pgdeactivate Bytes of memory that is deactivated into inactive list bytes container memory.stat memory memory_stat_container_inactive_file Bytes of file-backed memory on inactive lru list. bytes container memory.stat memory memory_stat_container_inactive_anon Bytes of anonymous and swap cache memory on inactive lru list bytes container memory.stat memory memory_stat_container_dirty Bytes that are waiting to get written back to the disk bytes container memory.stat memory memory_stat_container_active_file Bytes of file-backed memory on active lru list bytes container memory.stat memory memory_stat_container_active_anon Bytes of anonymous and swap cache memory on active lru list bytes container memory.stat memory mountpoint_perm_ro Whether mountpoint is readonly or not bool host proc fs memory vmstat_allocstall_normal Host direct reclaim count on normal zone count host /proc/vmstat memory vmstat_allocstall_movable Host direct reclaim count on movable zone count host /proc/vmstat memory vmstat_compact_stall Count of memory compaction count host /proc/vmstat memory vmstat_nr_active_anon Number of anonymous pages on active lru pages host /proc/vmstat memory vmstat_nr_active_file Number of file-backed pages on active lru pages host /proc/vmstat memory vmstat_nr_boost_pages Number of pages in kswapd boosting pages host /proc/vmstat memory vmstat_nr_dirty Number of dirty pages pages host /proc/vmstat memory vmstat_nr_free_pages Number of free pages pages host /proc/vmstat memory vmstat_nr_inactive_anon Number of anonymous pages on inactive lru pages host /proc/vmstat memory vmstat_nr_inactive_file Number of file-backed pages on inactive lru pages host /proc/vmstat memory vmstat_nr_kswapd_boost Count of kswapd boosting pages host /proc/vmstat memory vmstat_nr_mlock Number of locked pages pages host /proc/vmstat memory vmstat_nr_shmem Number of shmem pages pages host /proc/vmstat memory vmstat_nr_slab_reclaimable Number of relcaimable slab pages pages host /proc/vmstat memory vmstat_nr_slab_unreclaimable Number of unrelcaimable slab pages pages host /proc/vmstat memory vmstat_nr_unevictable Number of unevictable pages pages host /proc/vmstat memory vmstat_nr_writeback Number of writebacking pages pages host /proc/vmstat memory vmstat_numa_pages_migrated Number of pages in numa migrating pages host /proc/vmstat memory vmstat_pgdeactivate Number of pages which are deactivated into inactive lru pages host /proc/vmstat memory vmstat_pgrefill Number of pages which are scanned on active lru pages host /proc/vmstat memory vmstat_pgscan_direct Number of pages which are scanned in direct reclaim pages host /proc/vmstat memory vmstat_pgscan_kswapd Number of pages which are scanned in kswapd reclaim pages host /proc/vmstat memory vmstat_pgsteal_direct Number of pages which are reclaimed in direct reclaim pages host /proc/vmstat memory vmstat_pgsteal_kswapd Number of pages which are reclaimed in kswapd reclaim pages host /proc/vmstat memory hungtask_happened Count of hungtask events count host performance and statistics monitoring for BPF Programs memory oom_happened Count of oom events count host,container performance and statistics monitoring for BPF Programs memory softlockup_happened Count of softlockup events count host performance and statistics monitoring for BPF Programs memory mmhostbpf_compactionstat Time speed in memory compaction nanosecond host performance and statistics monitoring for BPF Programs memory mmhostbpf_allocstallstat Time speed in memory direct reclaim on host nanosecond host performance and statistics monitoring for BPF Programs memory mmcgroupbpf_container_directstallcount Count of cgroup’s try_charge direct reclaim count container performance and statistics monitoring for BPF Programs IO iolatency_disk_d2c Statistics of io latency when accessing the disk, including the time consumed by the driver and hardware components count host performance and statistics monitoring for BPF Programs IO iolatency_disk_q2c Statistics of io latency for the entire io lifecycle when accessing the disk count host performance and statistics monitoring for BPF Programs IO iolatency_container_d2c Statistics of io latency when accessing the disk, including the time consumed by the driver and hardware components count container performance and statistics monitoring for BPF Programs IO iolatency_container_q2c Statistics of io latency for the entire io lifecycle when accessing the disk count container performance and statistics monitoring for BPF Programs IO iolatency_disk_flush Statistics of delay for flush operations on disk raid device count host performance and statistics monitoring for BPF Programs IO iolatency_container_flush Statistics of delay for flush operations on disk raid devices caused by containers count container performance and statistics monitoring for BPF Programs IO iolatency_disk_freeze Statistics of disk freeze events count host performance and statistics monitoring for BPF Programs network tcp_mem_limit_pages System TCP total memory size limit pages system proc fs network tcp_mem_usage_bytes The total number of bytes of TCP memory used by the system bytes system tcp_mem_usage_pages * page_size network tcp_mem_usage_pages The total size of TCP memory used by the system pages system proc fs network tcp_mem_usage_percent The percentage of TCP memory used by the system to the limit size % system tcp_mem_usage_pages / tcp_mem_limit_pages network arp_entries The number of arp cache entries count host,container proc fs network arp_total Total number of arp cache entries count system proc fs network qdisc_backlog The number of bytes queued to be sent bytes host sum of same level(parent major) for a device network qdisc_bytes_total The number of bytes sent bytes host sum of same level(parent major) for a device network qdisc_current_queue_length The number of packets queued for sending count host sum of same level(parent major) for a device network qdisc_drops_total The number of discarded packets count host sum of same level(parent major) for a device network qdisc_overlimits_total The number of queued packets exceeds the limit count host sum of same level(parent major) for a device network qdisc_packets_total The number of packets sent count host sum of same level(parent major) for a device network qdisc_requeues_total The number of packets that were not sent successfully and were requeued count host sum of same level(parent major) for a device network ethtool_hardware_rx_dropped_errors Statistics of inbound packet droped or errors of interface count host related to hardware drivers, such as mlx, ixgbe, bnxt_en, etc. network netdev_receive_bytes_total Number of good received bytes bytes host,container proc fs network netdev_receive_compressed_total Number of correctly received compressed packets count host,container proc fs network netdev_receive_dropped_total Number of packets received but not processed count host,container proc fs network netdev_receive_errors_total Total number of bad packets received on this network device count host,container proc fs network netdev_receive_fifo_total Receiver FIFO error counter count host,container proc fs network netdev_receive_frame_total Receiver frame alignment errors count host,container proc fs network netdev_receive_multicast_total Multicast packets received. For hardware interfaces this statistic is commonly calculated at the device level (unlike rx_packets) and therefore may include packets which did not reach the host count host,container proc fs network netdev_receive_packets_total Number of good packets received by the interface count host,container proc fs network netdev_transmit_bytes_total Number of good transmitted bytes, corresponding to tx_packets bytes host,container proc fs network netdev_transmit_carrier_total Number of frame transmission errors due to loss of carrier during transmission count host,container proc fs network netdev_transmit_colls_total Number of collisions during packet transmissions count host,container proc fs network netdev_transmit_compressed_total Number of transmitted compressed packets count host,container proc fs network netdev_transmit_dropped_total Number of packets dropped on their way to transmission, e.g. due to lack of resources count host,container proc fs network netdev_transmit_errors_total Total number of transmit problems count host,container proc fs network netdev_transmit_fifo_total Number of frame transmission errors due to device FIFO underrun / underflow count host,container proc fs network netdev_transmit_packets_total Number of packets successfully transmitted count host,container proc fs network netstat_TcpExt_ArpFilter - count host,container proc fs network netstat_TcpExt_BusyPollRxPackets - count host,container proc fs network netstat_TcpExt_DelayedACKLocked A delayed ACK timer expires, but the TCP stack can’t send an ACK immediately due to the socket is locked by a userspace program. The TCP stack will send a pure ACK later (after the userspace program unlock the socket). When the TCP stack sends the pure ACK later, the TCP stack will also update TcpExtDelayedACKs and exit the delayed ACK mode count host,container proc fs network netstat_TcpExt_DelayedACKLost It will be updated when the TCP stack receives a packet which has been ACKed. A Delayed ACK loss might cause this issue, but it would also be triggered by other reasons, such as a packet is duplicated in the network count host,container proc fs network netstat_TcpExt_DelayedACKs A delayed ACK timer expires. The TCP stack will send a pure ACK packet and exit the delayed ACK mode count host,container proc fs network netstat_TcpExt_EmbryonicRsts resets received for embryonic SYN_RECV sockets count host,container proc fs network netstat_TcpExt_IPReversePathFilter - count host,container proc fs network netstat_TcpExt_ListenDrops When kernel receives a SYN from a client, and if the TCP accept queue is full, kernel will drop the SYN and add 1 to TcpExtListenOverflows. At the same time kernel will also add 1 to TcpExtListenDrops. When a TCP socket is in LISTEN state, and kernel need to drop a packet, kernel would always add 1 to TcpExtListenDrops. So increase TcpExtListenOverflows would let TcpExtListenDrops increasing at the same time, but TcpExtListenDrops would also increase without TcpExtListenOverflows increasing, e.g. a memory allocation fail would also let TcpExtListenDrops increase count host,container proc fs network netstat_TcpExt_ListenOverflows When kernel receives a SYN from a client, and if the TCP accept queue is full, kernel will drop the SYN and add 1 to TcpExtListenOverflows. At the same time kernel will also add 1 to TcpExtListenDrops. When a TCP socket is in LISTEN state, and kernel need to drop a packet, kernel would always add 1 to TcpExtListenDrops. So increase TcpExtListenOverflows would let TcpExtListenDrops increasing at the same time, but TcpExtListenDrops would also increase without TcpExtListenOverflows increasing, e.g. a memory allocation fail would also let TcpExtListenDrops increase count host,container proc fs network netstat_TcpExt_LockDroppedIcmps ICMP packets dropped because socket was locked count host,container proc fs network netstat_TcpExt_OfoPruned The TCP stack tries to discard packet on the out of order queue count host,container proc fs network netstat_TcpExt_OutOfWindowIcmps ICMP pkts dropped because they were out-of-window count host,container proc fs network netstat_TcpExt_PAWSActive Packets are dropped by PAWS in Syn-Sent status count host,container proc fs network netstat_TcpExt_PAWSEstab Packets are dropped by PAWS in any status other than Syn-Sent count host,container proc fs network netstat_TcpExt_PFMemallocDrop - count host,container proc fs network netstat_TcpExt_PruneCalled The TCP stack tries to reclaim memory for a socket. After updates this counter, the TCP stack will try to collapse the out of order queue and the receiving queue. If the memory is still not enough, the TCP stack will try to discard packets from the out of order queue (and update the TcpExtOfoPruned counter) count host,container proc fs network netstat_TcpExt_RcvPruned After ‘collapse’ and discard packets from the out of order queue, if the actually used memory is still larger than the max allowed memory, this counter will be updated. It means the ‘prune’ fails count host,container proc fs network netstat_TcpExt_SyncookiesFailed The MSS decoded from the SYN cookie is invalid. When this counter is updated, the received packet won’t be treated as a SYN cookie and the TcpExtSyncookiesRecv counter won’t be updated count host,container proc fs network netstat_TcpExt_SyncookiesRecv How many reply packets of the SYN cookies the TCP stack receives count host,container proc fs network netstat_TcpExt_SyncookiesSent It indicates how many SYN cookies are sent count host,container proc fs network netstat_TcpExt_TCPACKSkippedChallenge The ACK is skipped if the ACK is a challenge ACK count host,container proc fs network netstat_TcpExt_TCPACKSkippedFinWait2 The ACK is skipped in Fin-Wait-2 status, the reason would be either PAWS check fails or the received sequence number is out of window count host,container proc fs network netstat_TcpExt_TCPACKSkippedPAWS The ACK is skipped due to PAWS (Protect Against Wrapped Sequence numbers) check fails count host,container proc fs network netstat_TcpExt_TCPACKSkippedSeq The sequence number is out of window and the timestamp passes the PAWS check and the TCP status is not Syn-Recv, Fin-Wait-2, and Time-Wait count host,container proc fs network netstat_TcpExt_TCPACKSkippedSynRecv The ACK is skipped in Syn-Recv status. The Syn-Recv status means the TCP stack receives a SYN and replies SYN+ACK count host,container proc fs network netstat_TcpExt_TCPACKSkippedTimeWait The ACK is skipped in Time-Wait status, the reason would be either PAWS check failed or the received sequence number is out of window count host,container proc fs network netstat_TcpExt_TCPAbortFailed The kernel TCP layer will send RST if the RFC2525 2.17 section is satisfied. If an internal error occurs during this process, TcpExtTCPAbortFailed will be increased count host,container proc fs network netstat_TcpExt_TCPAbortOnClose Number of sockets closed when the user-mode program has data in the buffer count host,container proc fs network netstat_TcpExt_TCPAbortOnData It means TCP layer has data in flight, but need to close the connection count host,container proc fs network netstat_TcpExt_TCPAbortOnLinger When a TCP connection comes into FIN_WAIT_2 state, instead of waiting for the fin packet from the other side, kernel could send a RST and delete the socket immediately count host,container proc fs network netstat_TcpExt_TCPAbortOnMemory When an application closes a TCP connection, kernel still need to track the connection, let it complete the TCP disconnect process count host,container proc fs network netstat_TcpExt_TCPAbortOnTimeout This counter will increase when any of the TCP timers expire. In such situation, kernel won’t send RST, just give up the connection count host,container proc fs network netstat_TcpExt_TCPAckCompressed - count host,container proc fs network netstat_TcpExt_TCPAutoCorking When sending packets, the TCP layer will try to merge small packets to a bigger one count host,container proc fs network netstat_TcpExt_TCPBacklogDrop - count host,container proc fs network netstat_TcpExt_TCPChallengeACK The number of challenge acks sent count host,container proc fs network netstat_TcpExt_TCPDSACKIgnoredNoUndo When a DSACK block is invalid, one of these two counters would be updated. Which counter will be updated depends on the undo_marker flag of the TCP socket count host,container proc fs network netstat_TcpExt_TCPDSACKIgnoredOld When a DSACK block is invalid, one of these two counters would be updated. Which counter will be updated depends on the undo_marker flag of the TCP socket count host,container proc fs network netstat_TcpExt_TCPDSACKOfoRecv The TCP stack receives a DSACK, which indicate an out of order duplicate packet is received count host,container proc fs network netstat_TcpExt_TCPDSACKOfoSent The TCP stack receives an out of order duplicate packet, so it sends a DSACK to the sender count host,container proc fs network netstat_TcpExt_TCPDSACKOldSent The TCP stack receives a duplicate packet which has been acked, so it sends a DSACK to the sender count host,container proc fs network netstat_TcpExt_TCPDSACKRecv The TCP stack receives a DSACK, which indicates an acknowledged duplicate packet is received count host,container proc fs network netstat_TcpExt_TCPDSACKUndo Congestion window recovered without slow start using DSACK count host,container proc fs network netstat_TcpExt_TCPDeferAcceptDrop - count host,container proc fs network netstat_TcpExt_TCPDelivered - count host,container proc fs network netstat_TcpExt_TCPDeliveredCE - count host,container proc fs network netstat_TcpExt_TCPFastOpenActive When the TCP stack receives an ACK packet in the SYN-SENT status, and the ACK packet acknowledges the data in the SYN packet, the TCP stack understand the TFO cookie is accepted by the other side, then it updates this counter count host,container proc fs network netstat_TcpExt_TCPFastOpenActiveFail Fast Open attempts (SYN/data) failed because the remote does not accept it or the attempts timed out count host,container proc fs network netstat_TcpExt_TCPFastOpenBlackhole - count host,container proc fs network netstat_TcpExt_TCPFastOpenCookieReqd This counter indicates how many times a client wants to request a TFO cookie count host,container proc fs network netstat_TcpExt_TCPFastOpenListenOverflow When the pending fast open request number is larger than fastopenq-\u003emax_qlen, the TCP stack will reject the fast open request and update this counter count host,container proc fs network netstat_TcpExt_TCPFastOpenPassive This counter indicates how many times the TCP stack accepts the fast open request count host,container proc fs network netstat_TcpExt_TCPFastOpenPassiveFail This counter indicates how many times the TCP stack rejects the fast open request. It is caused by either the TFO cookie is invalid or the TCP stack finds an error during the socket creating process count host,container proc fs network netstat_TcpExt_TCPFastRetrans The TCP stack wants to retransmit a packet and the congestion control state is not ‘Loss’ count host,container proc fs network netstat_TcpExt_TCPFromZeroWindowAdv The TCP receive window is set to no-zero value from zero count host,container proc fs network netstat_TcpExt_TCPFullUndo - count host,container proc fs network netstat_TcpExt_TCPHPAcks If a packet set ACK flag and has no data, it is a pure ACK packet, if kernel handles it in the fast path, TcpExtTCPHPAcks will increase 1 count host,container proc fs network netstat_TcpExt_TCPHPHits If a TCP packet has data (which means it is not a pure ACK packet), and this packet is handled in the fast path, TcpExtTCPHPHits will increase 1 count host,container proc fs network netstat_TcpExt_TCPHystartDelayCwnd The sum of CWND detected by packet delay. Dividing this value by TcpExtTCPHystartDelayDetect is the average CWND which detected by the packet delay count host,container proc fs network netstat_TcpExt_TCPHystartDelayDetect How many times the packet delay threshold is detected count host,container proc fs network netstat_TcpExt_TCPHystartTrainCwnd The sum of CWND detected by ACK train length. Dividing this value by TcpExtTCPHystartTrainDetect is the average CWND which detected by the ACK train length count host,container proc fs network netstat_TcpExt_TCPHystartTrainDetect How many times the ACK train length threshold is detected count host,container proc fs network netstat_TcpExt_TCPKeepAlive This counter indicates many keepalive packets were sent. The keepalive won’t be enabled by default. A userspace program could enable it by setting the SO_KEEPALIVE socket option count host,container proc fs network netstat_TcpExt_TCPLossFailures Number of connections that enter the TCP_CA_Loss phase and then undergo RTO timeout count host,container proc fs network netstat_TcpExt_TCPLossProbeRecovery A packet loss is detected and recovered by TLP count host,container proc fs network netstat_TcpExt_TCPLossProbes A TLP probe packet is sent count host,container proc fs network netstat_TcpExt_TCPLossUndo - count host,container proc fs network netstat_TcpExt_TCPLostRetransmit A SACK points out that a retransmission packet is lost again count host,container proc fs network netstat_TcpExt_TCPMD5Failure - count host,container proc fs network netstat_TcpExt_TCPMD5NotFound - count host,container proc fs network netstat_TcpExt_TCPMD5Unexpected - count host,container proc fs network netstat_TcpExt_TCPMTUPFail - count host,container proc fs network netstat_TcpExt_TCPMTUPSuccess - count host,container proc fs network netstat_TcpExt_TCPMemoryPressures Number of times TCP ran low on memory count host,container proc fs network netstat_TcpExt_TCPMemoryPressuresChrono - count host,container proc fs network netstat_TcpExt_TCPMinTTLDrop - count host,container proc fs network netstat_TcpExt_TCPOFODrop The TCP layer receives an out of order packet but doesn’t have enough memory, so drops it. Such packets won’t be counted into TcpExtTCPOFOQueue count host,container proc fs network netstat_TcpExt_TCPOFOMerge The received out of order packet has an overlay with the previous packet. the overlay part will be dropped. All of TcpExtTCPOFOMerge packets will also be counted into TcpExtTCPOFOQueue count host,container proc fs network netstat_TcpExt_TCPOFOQueue The TCP layer receives an out of order packet and has enough memory to queue it count host,container proc fs network netstat_TcpExt_TCPOrigDataSent Number of outgoing packets with original data (excluding retransmission but including data-in-SYN). This counter is different from TcpOutSegs because TcpOutSegs also tracks pure ACKs. TCPOrigDataSent is more useful to track the TCP retransmission rate count host,container proc fs network netstat_TcpExt_TCPPartialUndo Detected some erroneous retransmits, a partial ACK arrived while were fast retransmitting, so able to partially undo some of our CWND reduction count host,container proc fs network netstat_TcpExt_TCPPureAcks If a packet set ACK flag and has no data, it is a pure ACK packet, if kernel handles it in the fast path, TcpExtTCPHPAcks will increase 1, if kernel handles it in the slow path, TcpExtTCPPureAcks will increase 1 count host,container proc fs network netstat_TcpExt_TCPRcvCoalesce When packets are received by the TCP layer and are not be read by the application, the TCP layer will try to merge them. This counter indicate how many packets are merged in such situation. If GRO is enabled, lots of packets would be merged by GRO, these packets wouldn’t be counted to TcpExtTCPRcvCoalesce count host,container proc fs network netstat_TcpExt_TCPRcvCollapsed This counter indicates how many skbs are freed during ‘collapse’ count host,container proc fs network netstat_TcpExt_TCPRenoFailures Number of failures that enter the TCP_CA_Disorder phase and then undergo RTO count host,container proc fs network netstat_TcpExt_TCPRenoRecovery When the congestion control comes into Recovery state, if sack is used, TcpExtTCPSackRecovery increases 1, if sack is not used, TcpExtTCPRenoRecovery increases 1. These two counters mean the TCP stack begins to retransmit the lost packets count host,container proc fs network netstat_TcpExt_TCPRenoRecoveryFail Number of connections that enter the Recovery phase and then undergo RTO count host,container proc fs network netstat_TcpExt_TCPRenoReorder The reorder packet is detected by fast recovery. It would only be used if SACK is disabled count host,container proc fs network netstat_TcpExt_TCPReqQFullDoCookies - count host,container proc fs network netstat_TcpExt_TCPReqQFullDrop - count host,container proc fs network netstat_TcpExt_TCPRetransFail The TCP stack tries to deliver a retransmission packet to lower layers but the lower layers return an error count host,container proc fs network netstat_TcpExt_TCPSACKDiscard This counter indicates how many SACK blocks are invalid. If the invalid SACK block is caused by ACK recording, the TCP stack will only ignore it and won’t update this counter count host,container proc fs network netstat_TcpExt_TCPSACKReneging A packet was acknowledged by SACK, but the receiver has dropped this packet, so the sender needs to retransmit this packet count host,container proc fs network netstat_TcpExt_TCPSACKReorder The reorder packet detected by SACK count host,container proc fs network netstat_TcpExt_TCPSYNChallenge The number of challenge acks sent in response to SYN packets count host,container proc fs network netstat_TcpExt_TCPSackFailures Number of failures that enter the TCP_CA_Disorder phase and then undergo RTO count host,container proc fs network netstat_TcpExt_TCPSackMerged A skb is merged count host,container proc fs network netstat_TcpExt_TCPSackRecovery When the congestion control comes into Recovery state, if sack is used, TcpExtTCPSackRecovery increases 1, if sack is not used, TcpExtTCPRenoRecovery increases 1. These two counters mean the TCP stack begins to retransmit the lost packets count host,container proc fs network netstat_TcpExt_TCPSackRecoveryFail When the congestion control comes into Recovery state, if sack is used, TcpExtTCPSackRecovery increases 1 count host,container proc fs network netstat_TcpExt_TCPSackShiftFallback A skb should be shifted or merged, but the TCP stack doesn’t do it for some reasons count host,container proc fs network netstat_TcpExt_TCPSackShifted A skb is shifted count host,container proc fs network netstat_TcpExt_TCPSlowStartRetrans The TCP stack wants to retransmit a packet and the congestion control state is ‘Loss’ count host,container proc fs network netstat_TcpExt_TCPSpuriousRTOs The spurious retransmission timeout detected by the F-RTO algorithm count host,container proc fs network netstat_TcpExt_TCPSpuriousRtxHostQueues When the TCP stack wants to retransmit a packet, and finds that packet is not lost in the network, but the packet is not sent yet, the TCP stack would give up the retransmission and update this counter. It might happen if a packet stays too long time in a qdisc or driver queue count host,container proc fs network netstat_TcpExt_TCPSynRetrans Number of SYN and SYN/ACK retransmits to break down retransmissions into SYN, fast-retransmits, timeout retransmits, etc count host,container proc fs network netstat_TcpExt_TCPTSReorder The reorder packet is detected when a hole is filled count host,container proc fs network netstat_TcpExt_TCPTimeWaitOverflow Number of TIME_WAIT sockets unable to be allocated due to limit exceeding count host,container proc fs network netstat_TcpExt_TCPTimeouts TCP timeout events count host,container proc fs network netstat_TcpExt_TCPToZeroWindowAdv The TCP receive window is set to zero from a no-zero value count host,container proc fs network netstat_TcpExt_TCPWantZeroWindowAdv Depending on current memory usage, the TCP stack tries to set receive window to zero. But the receive window might still be a no-zero value count host,container proc fs network netstat_TcpExt_TCPWinProbe Number of ACK packets to be sent at regular intervals to make sure a reverse ACK packet opening back a window has not been lost count host,container proc fs network netstat_TcpExt_TCPWqueueTooBig - count host,container proc fs network netstat_TcpExt_TW TCP sockets finished time wait in fast timer count host,container proc fs network netstat_TcpExt_TWKilled TCP sockets finished time wait in slow timer count host,container proc fs network netstat_TcpExt_TWRecycled Time wait sockets recycled by time stamp count host,container proc fs network netstat_Tcp_ActiveOpens It means the TCP layer sends a SYN, and come into the SYN-SENT state. Every time TcpActiveOpens increases 1, TcpOutSegs should always increase 1 count host,container proc fs network netstat_Tcp_AttemptFails The number of times TCP connections have made a direct transition to the CLOSED state from either the SYN-SENT state or the SYN-RCVD state, plus the number of times TCP connections have made a direct transition to the LISTEN state from the SYN-RCVD state count host,container proc fs network netstat_Tcp_CurrEstab The number of TCP connections for which the current state is either ESTABLISHED or CLOSE-WAIT count host,container proc fs network netstat_Tcp_EstabResets The number of times TCP connections have made a direct transition to the CLOSED state from either the ESTABLISHED state or the CLOSE-WAIT state count host,container proc fs network netstat_Tcp_InCsumErrors Incremented when a TCP checksum failure is detected count host,container proc fs network netstat_Tcp_InErrs The total number of segments received in error (e.g., bad TCP checksums) count host,container proc fs network netstat_Tcp_InSegs The number of packets received by the TCP layer. As mentioned in RFC1213, it includes the packets received in error, such as checksum error, invalid TCP header and so on count host,container proc fs network netstat_Tcp_MaxConn The limit on the total number of TCP connections the entity can support. In entities where the maximum number of connections is dynamic, this object should contain the value -1 count host,container proc fs network netstat_Tcp_OutRsts The number of TCP segments sent containing the RST flag count host,container proc fs network netstat_Tcp_OutSegs The total number of segments sent, including those on current connections but excluding those containing only retransmitted octets count host,container proc fs network netstat_Tcp_PassiveOpens The number of times TCP connections have made a direct transition to the SYN-RCVD state from the LISTEN state count host,container proc fs network netstat_Tcp_RetransSegs The total number of segments retransmitted - that is, the number of TCP segments transmitted containing one or more previously transmitted octets count host,container proc fs network netstat_Tcp_RtoAlgorithm The algorithm used to determine the timeout value used for retransmitting unacknowledged octets count host,container proc fs network netstat_Tcp_RtoMax The maximum value permitted by a TCP implementation for the retransmission timeout, measured in milliseconds. More refined semantics for objects of this type depend upon the algorithm used to determine the retransmission timeout count host,container proc fs network netstat_Tcp_RtoMin The minimum value permitted by a TCP implementation for the retransmission timeout, measured in milliseconds. More refined semantics for objects of this type depend upon the algorithm used to determine the retransmission timeout count host,container proc fs network sockstat_FRAG_inuse - count host,container proc fs network sockstat_FRAG_memory - pages host,container proc fs network sockstat_RAW_inuse Number of RAW socket used count host,container proc fs network sockstat_TCP_alloc The number of TCP sockets that have been allocated count host,container proc fs network sockstat_TCP_inuse Established TCP socket number count host,container proc fs network sockstat_TCP_mem The total size of TCP memory used by the system pages system proc fs network sockstat_TCP_mem_bytes The total size of TCP memory used by the system bytes system sockstat_TCP_mem * page_size network sockstat_TCP_orphan Number of TCP connections waiting to be closed count host,container proc fs network sockstat_TCP_tw Number of TCP sockets to be terminated count host,container proc fs network sockstat_UDPLITE_inuse - count host,container proc fs network sockstat_UDP_inuse Number of UDP socket used count host,container proc fs network sockstat_UDP_mem The total size of udp memory used by the system pages system proc fs network sockstat_UDP_mem_bytes The total number of bytes of udp memory used by the system bytes system sockstat_UDP_mem * page_size network sockstat_sockets_used The number of sockets used by the system count system proc fs ","categories":"","description":"","excerpt":" Subsystem Metric Description Unit Dimension Source cpu cpu_util_sys …","ref":"/docs/en/v2.1.0/concepts/integrated/metrics/","tags":"","title":"Metrics"},{"body":"该文档汇总了当前 v1.0 版本支持的所有的指标，涉及CPU，内存，网络，IO。\n子系统 指标 描述 单位 统计纬度 指标来源 cpu cpu_util_sys cpu 系统态利用率 % 宿主 基于 cgroup cpuacct.stat 和 cpuacct.usage 计算 cpu cpu_util_usr cpu 用户态利用率 % 宿主 基于 cgroup cpuacct.stat 和 cpuacct.usage 计算 cpu cpu_util_total 容器 cpu 总利用率 % 宿主 基于 cgroup cpuacct.stat 和 cpuacct.usage 计算 cpu cpu_util_container_sys 容器 cpu 系统态利用率 % 容器 基于 cgroup cpuacct.stat 和 cpuacct.usage 计算 cpu cpu_util_container_usr 容器 cpu 用户态利用率 % 容器 基于 cgroup cpuacct.stat 和 cpuacct.usage 计算 cpu cpu_util_container_total 容器 cpu 总利用率 % 容器 基于 cgroup cpuacct.stat 和 cpuacct.usage 计算 cpu cpu_stat_container_burst_time 累计墙时（以纳秒为单位），周期内突发超出配额的时间 纳秒(ns) 容器 基于 cpu.stat 读取 cpu cpu_stat_container_nr_bursts 周期内突发次数 计数 容器 基于 cpu.stat 读取 cpu cpu_stat_container_nr_throttled cgroup 被 throttled/limited 的次数 计数 容器 基于 cpu.stat 读取 cpu cpu_stat_container_exter_wait_rate 容器外进程导致的等待率 % 容器 基于 cpu.stat 读取的 throttled_time hierarchy_wait_sum inner_wait_sum 计算 cpu cpu_stat_container_inner_wait_rate 容器内部进程导致的等待率 % 容器 基于 cpu.stat 读取的 throttled_time hierarchy_wait_sum inner_wait_sum 计算 cpu cpu_stat_container_throttle_wait_rate 容器被限制而引起的等待率 % 容器 基于 cpu.stat 读取的 throttled_time hierarchy_wait_sum inner_wait_sum 计算 cpu cpu_stat_container_wait_rate 总的等待率: exter_wait_rate + inner_wait_rate + throttle_wait_rate % 容器 基于 cpu.stat 读取的 throttled_time hierarchy_wait_sum inner_wait_sum 计算 cpu loadavg_container_container_nr_running 容器中运行的任务数量 计数 容器 从内核通过 netlink 获取 cpu loadavg_container_container_nr_uninterruptible 容器中不可中断任务的数量 计数 容器 从内核通过 netlink 获取 cpu loadavg_load1 系统过去 1 分钟的平均负载 计数 宿主 procfs cpu loadavg_load5 系统过去 5 分钟的平均负载 计数 宿主 procfs cpu loadavg_load15 系统过去 15 分钟的平均负载 计数 宿主 procfs cpu softirq_latency 在不同时间域发生的 NET_RX/NET_TX 中断延迟次数：\n0~10 us\n100us ~ 1ms\n10us ~ 100us\n1ms ~ inf 计数 宿主 BPF 软中断埋点统计 cpu runqlat_container_nlat_01 容器中进程调度延迟在 0~10 毫秒内的次数 计数 容器 bpf 调度切换埋点统计 cpu runqlat_container_nlat_02 容器中进程调度延迟在 10~20 毫秒之间的次数 计数 容器 bpf 调度切换埋点统计 cpu runqlat_container_nlat_03 容器中进程调度延迟在 20~50 毫秒之间的次数 计数 容器 bpf 调度切换埋点统计 cpu runqlat_container_nlat_04 容器中进程调度延迟超过 50 毫秒的次数 计数 容器 bpf 调度切换埋点统计 cpu runqlat_g_nlat_01 宿主中进程调度延迟在范围内 0～10 毫秒的次数 计数 宿主 bpf 调度切换埋点统计 cpu runqlat_g_nlat_02 宿主中进程调度延迟在范围内 10～20 毫秒的次数 计数 宿主 bpf 调度切换埋点统计 cpu runqlat_g_nlat_03 宿主中进程调度延迟在范围内 20～50 毫秒的次数 计数 宿主 bpf 调度切换埋点统计 cpu runqlat_g_nlat_04 宿主中进程调度延迟超过 50 毫秒的次数 计数 宿主 bpf 调度切换埋点统计 cpu reschedipi_oversell_probability vm 中 cpu 超卖检测 0-1 宿主 bpf 调度 ipi 埋点统计 memory buddyinfo_blocks 内核伙伴系统内存分配 页计数 宿主 procfs memory memory_events_container_watermark_inc 内存水位计数 计数 容器 memory.events memory memory_events_container_watermark_dec 内存水位计数 计数 容器 memory.events memory memory_others_container_local_direct_reclaim_time cgroup 中页分配速度 纳秒(ns) 容器 memory.local_direct_reclaim_time memory memory_others_container_directstall_time 直接回收时间 纳秒(ns) 容器 memory.directstall_stat memory memory_others_container_asyncreclaim_time 异步回收时间 纳秒(ns) 容器 memory.asynreclaim_stat memory memory_stat_container_writeback 匿名/文件 cache sync 到磁盘排队字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_unevictable 无法回收的内存（如 mlocked） 字节(Bytes) 容器 memory.stat memory memory_stat_container_shmem 共享内存字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgsteal_kswapd kswapd 和 cswapd 回收的内存字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgsteal_globalkswapd 由 kswapd 回收的字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgsteal_globaldirect 过页面分配直接回收的内存字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgsteal_direct 页分配和 try_charge 期间直接回收的内存字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgsteal_cswapd 由 cswapd 回收的字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgscan_kswapd kswapd 和 cswapd 扫描的内存字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgscan_globalkswapd kswapd 扫描的内存字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgscan_globaldirect 扫描内存中通过直接回收在页面分配期间的字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgscan_direct 扫描内存的字节数，在页面分配和 try_charge 期间通过直接回收的字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgscan_cswapd 由 cswapd 扫描内存的字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgrefill 内存中扫描的字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_pgdeactivate 内存中未激活的部分被添加到非活动列表中 字节(Bytes) 容器 memory.stat memory memory_stat_container_inactive_file 文件内存中不活跃的 LRU 列表的字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_inactive_anon 匿名和交换缓存内存中不活跃的 LRU 列表的字节数 字节(Bytes) 容器 memory.stat memory memory_stat_container_dirty 等待写入磁盘的字节 字节(Bytes) 容器 memory.stat memory memory_stat_container_active_file 活跃内存中文件内存的大小 字节(Bytes) 容器 memory.stat memory memory_stat_container_active_anon 活跃内存中匿名和交换内存的大小 字节(Bytes) 容器 memory.stat memory mountpoint_perm_ro 挂在点是否为只读 布尔(bool) 宿主 procfs memory vmstat_allocstall_normal 宿主在 normal 域直接回收 计数 宿主 /proc/vmstat memory vmstat_allocstall_movable 宿主在 movable 域直接回收 计数 宿主 /proc/vmstat memory vmstat_compact_stall 内存压缩计数 计数 宿主 /proc/vmstat memory vmstat_nr_active_anon 活跃的匿名页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_active_file 活跃的文件页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_boost_pages kswapd boosting 页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_dirty 脏页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_free_pages 释放的页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_inactive_anon 非活跃的匿名页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_inactive_file 非活跃的文件页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_kswapd_boost kswapd boosting 次数计数 页计数 宿主 /proc/vmstat memory vmstat_nr_mlock 锁定的页面数量 页计数 宿主 /proc/vmstat memory vmstat_nr_shmem 共享内存页面数 页计数 宿主 /proc/vmstat memory vmstat_nr_slab_reclaimable 可回收的 slab 页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_slab_unreclaimable 无法回收的 slab 页数量 页计数 宿主 /proc/vmstat memory vmstat_nr_unevictable 不可驱逐页面数量 页计数 宿主 /proc/vmstat memory vmstat_nr_writeback 写入页面数 页计数 宿主 /proc/vmstat memory vmstat_numa_pages_migrated NUMA 迁移中的页面数 页计数 宿主 /proc/vmstat memory vmstat_pgdeactivate 页数被停用进入非活动 LRU 页计数 宿主 /proc/vmstat memory vmstat_pgrefill 扫描的活跃 LRU 页面数 页计数 宿主 /proc/vmstat memory vmstat_pgscan_direct 扫描的页数 页计数 宿主 /proc/vmstat memory vmstat_pgscan_kswapd 扫描的页面数量，由 kswapd 回收的数量 页计数 宿主 /proc/vmstat memory vmstat_pgsteal_direct 直接回收的页面 页计数 宿主 /proc/vmstat memory vmstat_pgsteal_kswapd 被 kswapd 回收的数量 页计数 宿主 /proc/vmstat memory hungtask_counter hungtask 事件计数 计数 宿主 BPF 埋点统计 memory oom_host_counter oom 事件计数 计数 宿主 BPF 埋点统计 memory oom_container_counter oom 事件计数 计数 容器 BPF 埋点统计 memory softlockup_counter softlockup 事件计数 计数 宿主 BPF 埋点统计 memory memory_free_compaction 内存压缩的速度 纳秒(ns) 宿主 bpf 埋点统计 memory memory_free_allocstall 内存中主机直接回收速度 纳秒(ns) 宿主 bpf 埋点统计 memory memory_cgroup_container_directstall cgroup 尝试直接回收的计数 计数 容器 bpf 埋点统计 IO iolatency_disk_d2c 磁盘访问时的 io 延迟统计，包括驱动程序和硬件组件消耗的时间 计数 宿主 bpf 埋点统计 IO iolatency_disk_q2c 磁盘访问整个 I/O 生命周期时的 I/O 延迟统计 计数 宿主 bpf 埋点统计 IO iolatency_container_d2c 磁盘访问时的 I/O 延迟统计，包括驱动程序和硬件组件消耗的时间 计数 容器 bpf 埋点统计 IO iolatency_container_q2c 磁盘访问整个 I/O 生命周期时的 I/O 延迟统计 计数 容器 bpf 埋点统计 IO iolatency_disk_flush 磁盘 RAID 设备刷新操作延迟统计 计数 宿主 bpf 埋点统计 IO iolatency_container_flush 磁盘 RAID 设备上由容器引起的刷新操作延迟统计 计数 容器 bpf 埋点统计 IO iolatency_disk_freeze 磁盘 freese 事件 计数 宿主 bpf 埋点统计 network tcp_mem_limit_pages 系统 TCP 总内存大小限制 页计数 系统 procfs network tcp_mem_usage_bytes 系统使用的 TCP 内存总字节数 字节(Bytes) 系统 tcp_mem_usage_pages * page_size network tcp_mem_usage_pages 系统使用的 TCP 内存总量 页计数 系统 procfs network tcp_mem_usage_percent 系统使用的 TCP 内存百分比（相对 TCP 内存总限制） % 系统 tcp_mem_usage_pages / tcp_mem_limit_pages network arp_entries arp 缓存条目数量 计数 宿主，容器 procfs network arp_total 总 arp 缓存条目数 计数 系统 procfs network qdisc_backlog 待发送的字节数 字节(Bytes) 宿主 netlink qdisc 统计 network qdisc_bytes_total 已发送的字节数 字节(Bytes) 宿主 netlink qdisc 统计 network qdisc_current_queue_length 排队等待发送的包数量 计数 宿主 netlink qdisc 统计 network qdisc_drops_total 丢弃的数据包数量 计数 宿主 netlink qdisc 统计 network qdisc_overlimits_total 排队数据包里超限的数量 计数 宿主 netlink qdisc 统计 network qdisc_packets_total 已发送的包数量 计数 宿主 netlink qdisc 统计 network qdisc_requeues_total 重新入队的数量 计数 宿主 netlink qdisc 统计 network ethtool_hardware_rx_dropped_errors 接口接收丢包统计 计数 宿主 硬件驱动相关, 如 mlx, ixgbe, bnxt_en, etc. network netdev_receive_bytes_total 接口接收的字节数 字节(Bytes) 宿主，容器 procfs network netdev_receive_compressed_total 接口接收的压缩包数量 计数 宿主，容器 procfs network netdev_receive_dropped_total 接口接收丢弃的包数量 计数 宿主，容器 procfs network netdev_receive_errors_total 接口接收检测到错误的包数量 计数 宿主，容器 procfs network netdev_receive_fifo_total 接口接收 fifo 缓冲区错误数量 计数 宿主，容器 procfs network netdev_receive_frame_total 接口接收帧对齐错误 计数 宿主，容器 procfs network netdev_receive_multicast_total 多播数据包已接收的包数量，对于硬件接口，此统计通常在设备层计算（与 rx_packets 不同），因此可能包括未到达的数据包 计数 宿主，容器 procfs network netdev_receive_packets_total 接口接收到的有效数据包数量 计数 宿主，容器 procfs network netdev_transmit_bytes_total 接口发送的字节数 字节(Bytes) 宿主，容器 procfs network netdev_transmit_carrier_total 接口发送过程中由于载波丢失导致的帧传输错误数量 计数 宿主，容器 procfs network netdev_transmit_colls_total 接口发送碰撞计数 计数 宿主，容器 procfs network netdev_transmit_compressed_total 接口发送压缩数据包数量 计数 宿主，容器 procfs network netdev_transmit_dropped_total 数据包在传输过程中丢失的数量，如资源不足 计数 宿主，容器 procfs network netdev_transmit_errors_total 发送错误计数 计数 宿主，容器 procfs network netdev_transmit_fifo_total 帧传输错误数量 计数 宿主，容器 procfs network netdev_transmit_packets_total 发送数据包计数 计数 宿主，容器 procfs network netstat_TcpExt_ArpFilter 因 ARP 过滤规则而被拒绝的 ARP 请求/响应包数量 计数 宿主，容器 procfs network netstat_TcpExt_BusyPollRxPackets 通过 busy polling​​ 机制接收到的网络数据包数量 计数 宿主，容器 procfs network netstat_TcpExt_DelayedACKLocked 由于用户态锁住了sock，而无法发送delayed ack的次数 计数 宿主，容器 procfs network netstat_TcpExt_DelayedACKLost 当收到已确认的包时，它将被更新。延迟 ACK 丢失可能会引起这个问题，但其他原因也可能触发，例如网络中重复的包。 计数 宿主，容器 procfs network netstat_TcpExt_DelayedACKs 延迟的 ACK 定时器已过期。TCP 堆栈将发送一个纯 ACK 数据包并退出延迟 ACK 模式 计数 宿主，容器 procfs network netstat_TcpExt_EmbryonicRsts 收到初始 SYN_RECV 套接字的重置 计数 宿主，容器 procfs network netstat_TcpExt_IPReversePathFilter - 计数 宿主，容器 procfs network netstat_TcpExt_ListenDrops 当内核收到客户端的 SYN 请求时，如果 TCP 接受队列已满，内核将丢弃 SYN 并将 TcpExtListenOverflows 加 1。同时，内核也会将 TcpExtListenDrops 加 1。当一个 TCP 套接字处于监听状态，且内核需要丢弃一个数据包时，内核会始终将 TcpExtListenDrops 加 1。因此，增加 TcpExtListenOverflows 会导致 TcpExtListenDrops 同时增加，但 TcpExtListenDrops 也会在没有 TcpExtListenOverflows 增加的情况下增加，例如内存分配失败也会导致 TcpExtListenDrops 增加。 计数 宿主，容器 procfs network netstat_TcpExt_ListenOverflows 当内核收到客户端的 SYN 请求时，如果 TCP 接受队列已满，内核将丢弃 SYN 并将 TcpExtListenOverflows 加 1。同时，内核也会将 TcpExtListenDrops 加 1。当一个 TCP 套接字处于监听状态，且内核需要丢弃一个数据包时，内核会始终将 TcpExtListenDrops 加 1。因此，增加 TcpExtListenOverflows 会导致 TcpExtListenDrops 同时增加，但 TcpExtListenDrops 也会在没有 TcpExtListenOverflows 增加的情况下增加，例如内存分配失败也会导致 TcpExtListenDrops 增加。 计数 宿主，容器 procfs network netstat_TcpExt_LockDroppedIcmps 由于套接字被锁定，ICMP 数据包被丢弃 计数 宿主，容器 procfs network netstat_TcpExt_OfoPruned 协议栈尝试在乱序队列中丢弃数据包 计数 宿主，容器 procfs network netstat_TcpExt_OutOfWindowIcmps ICMP 数据包因超出窗口而被丢弃 计数 宿主，容器 procfs network netstat_TcpExt_PAWSActive 数据包在 Syn-Sent 状态被 PAWS 丢弃 计数 宿主，容器 procfs network netstat_TcpExt_PAWSEstab 数据包在除 Syn-Sent 之外的所有状态下都会被 PAWS 丢弃 计数 宿主，容器 procfs network netstat_TcpExt_PFMemallocDrop - 计数 宿主，容器 procfs network netstat_TcpExt_PruneCalled 协议栈尝试回收套接字内存。更新此计数器后，将尝试合并乱序队列和接收队列。如果内存仍然不足，将尝试丢弃乱序队列中的数据包（并更新 TcpExtOfoPruned 计数器）。 计数 宿主，容器 procfs network netstat_TcpExt_RcvPruned 在从顺序错误的队列中‘collapse’和丢弃数据包后，如果实际使用的内存仍然大于最大允许内存，则此计数器将被更新。这意味着‘prune’失败 计数 宿主，容器 procfs network netstat_TcpExt_SyncookiesFailed MSS 从 SYN cookie 解码出来的无效。当这个计数器更新时，接收到的数据包不会被当作 SYN cookie 处理，并且 TcpExtSyncookiesRecv 计数器不会更新 计数 宿主，容器 procfs network netstat_TcpExt_SyncookiesRecv 接收了多少个 SYN cookies 的回复数据包 计数 宿主，容器 procfs network netstat_TcpExt_SyncookiesSent 发送了多少个 SYN cookies 计数 宿主，容器 procfs network netstat_TcpExt_TCPACKSkippedChallenge ACK 为 challenge ACK 时，将跳过 ACK 计数 宿主，容器 procfs network netstat_TcpExt_TCPACKSkippedFinWait2 ACK 在 Fin-Wait-2 状态被跳过，原因可能是 PAWS 检查失败或接收到的序列号超出窗口 计数 宿主，容器 procfs network netstat_TcpExt_TCPACKSkippedPAWS 由于 PAWS（保护包装序列号）检查失败，ACK 被跳过 计数 宿主，容器 procfs network netstat_TcpExt_TCPACKSkippedSeq 序列号超出窗口范围，时间戳通过 PAWS 检查，TCP 状态不是 Syn-Recv、Fin-Wait-2 和 Time-Wait 计数 宿主，容器 procfs network netstat_TcpExt_TCPACKSkippedSynRecv ACK 在 Syn-Recv 状态中被跳过。Syn-Recv 状态表示协议栈收到一个 SYN 并回复 SYN+ACK 计数 宿主，容器 procfs network netstat_TcpExt_TCPACKSkippedTimeWait CK 在 Time-Wait 状态中被跳过，原因可能是 PAWS 检查失败或接收到的序列号超出窗口 计数 宿主，容器 procfs network netstat_TcpExt_TCPAbortFailed 内核 TCP 层将在满足 RFC2525 2.17 节时发送 RST。如果在处理过程中发生内部错误，TcpExtTCPAbortFailed 将增加 计数 宿主，容器 procfs network netstat_TcpExt_TCPAbortOnClose 用户模式程序缓冲区中有数据时关闭的套接字数量 计数 宿主，容器 procfs network netstat_TcpExt_TCPAbortOnData TCP 层有正在传输的数据，但需要关闭连接 计数 宿主，容器 procfs network netstat_TcpExt_TCPAbortOnLinger 当 TCP 连接进入 FIN_WAIT_2 状态时，内核不会等待来自另一侧的 fin 包，而是发送 RST 并立即删除套接字 计数 宿主，容器 procfs network netstat_TcpExt_TCPAbortOnMemory 当一个应用程序关闭 TCP 连接时，内核仍然需要跟踪该连接，让它完成 TCP 断开过程 计数 宿主，容器 procfs network netstat_TcpExt_TCPAbortOnTimeout 此计数器将在任何 TCP 计时器到期时增加。在这种情况下，内核不会发送 RST，而是放弃连接 计数 宿主，容器 procfs network netstat_TcpExt_TCPAckCompressed - 计数 宿主，容器 procfs network netstat_TcpExt_TCPAutoCorking 发送数据包时，TCP 层会尝试将小数据包合并成更大的一个 计数 宿主，容器 procfs network netstat_TcpExt_TCPBacklogDrop - 计数 宿主，容器 procfs network netstat_TcpExt_TCPChallengeACK challenge ack 发送的数量 计数 宿主，容器 procfs network netstat_TcpExt_TCPDSACKIgnoredNoUndo 当 DSACK 块无效时，这两个计数器中的一个将被更新。哪个计数器将被更新取决于 TCP 套接字的 undo_marker 标志 计数 宿主，容器 procfs network netstat_TcpExt_TCPDSACKIgnoredOld 当 DSACK 块无效时，这两个计数器中的一个将被更新。哪个计数器将被更新取决于 TCP 套接字的 undo_marker 标志 计数 宿主，容器 procfs network netstat_TcpExt_TCPDSACKOfoRecv 收到一个 DSACK，表示收到一个顺序错误的重复数据包 计数 宿主，容器 procfs network netstat_TcpExt_TCPDSACKOfoSent 收到一个乱序的重复数据包，因此向发送者发送 DSACK 计数 宿主，容器 procfs network netstat_TcpExt_TCPDSACKOldSent 收到一个已确认的重复数据包，因此向发送者发送 DSACK 计数 宿主，容器 procfs network netstat_TcpExt_TCPDSACKRecv 收到一个 DSACK，表示收到了一个已确认的重复数据包 计数 宿主，容器 procfs network netstat_TcpExt_TCPDSACKUndo - 计数 宿主，容器 procfs network netstat_TcpExt_TCPDeferAcceptDrop - 计数 宿主，容器 procfs network netstat_TcpExt_TCPDelivered - 计数 宿主，容器 procfs network netstat_TcpExt_TCPDeliveredCE - 计数 宿主，容器 procfs network netstat_TcpExt_TCPFastOpenActive 当 TCP 栈在 SYN-SENT 状态接收到一个 ACK 包，并且 ACK 包确认了 SYN 包中的数据，理解 TFO cookie 已被对方接受，然后它更新这个计数器 计数 宿主，容器 procfs network netstat_TcpExt_TCPFastOpenActiveFail Fast Open 失败 计数 宿主，容器 procfs network netstat_TcpExt_TCPFastOpenBlackhole - 计数 宿主，容器 procfs network netstat_TcpExt_TCPFastOpenCookieReqd 客户端想要请求 TFO cookie 的次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPFastOpenListenOverflow 挂起的 Fast Open 请求数量大于 fastopenq-\u003emax_qlen 时，协议栈将拒绝 Fast Open 请求并更新此计数器 计数 宿主，容器 procfs network netstat_TcpExt_TCPFastOpenPassive 指示 TCP 堆栈接受 Fast Open 请求的次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPFastOpenPassiveFail 协议栈拒绝 Fast Open 的次数，这是由于 TFO cookie 无效或 在创建套接字过程中发现错误所引起的 计数 宿主，容器 procfs network netstat_TcpExt_TCPFastRetrans 快速重传 计数 宿主，容器 procfs network netstat_TcpExt_TCPFromZeroWindowAdv TCP 接收窗口设置为非零值 计数 宿主，容器 procfs network netstat_TcpExt_TCPFullUndo - 计数 宿主，容器 procfs network netstat_TcpExt_TCPHPAcks 如果数据包设置了 ACK 标志且没有数据，则是一个纯 ACK 数据包，如果内核在快速路径中处理它，TcpExtTCPHPAcks 将增加 1 计数 宿主，容器 procfs network netstat_TcpExt_TCPHPHits 如果 TCP 数据包包含数据（这意味着它不是一个纯 ACK 数据包），并且此数据包在快速路径中处理，TcpExtTCPHPHits 将增加 1 计数 宿主，容器 procfs network netstat_TcpExt_TCPHystartDelayCwnd CWND 检测到的包延迟总和。将此值除以 TcpExtTCPHystartDelayDetect，即为通过包延迟检测到的平均 CWND 计数 宿主，容器 procfs network netstat_TcpExt_TCPHystartDelayDetect 检测到数据包延迟阈值次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPHystartTrainCwnd TCP Hystart 训练中使用的拥塞窗口大小，将此值除以 TcpExtTCPHystartTrainDetect 得到由 ACK 训练长度检测到的平均 CWND 计数 宿主，容器 procfs network netstat_TcpExt_TCPHystartTrainDetect TCP Hystart 训练检测的次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPKeepAlive 此计数器指示已发送的保活数据包。默认情况下不会启用保活功能。用户空间程序可以通过设置 SO_KEEPALIVE 套接字选项来启用它。 计数 宿主，容器 procfs network netstat_TcpExt_TCPLossFailures 丢失数据包而进行恢复失败的次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPLossProbeRecovery 检测到丢失的数据包恢复的次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPLossProbes TCP 检测到丢失的数据包数量，通常用于检测网络拥塞或丢包 计数 宿主，容器 procfs network netstat_TcpExt_TCPLossUndo TCP重传数据包成功到达目标端口，但之前已经由于超时或拥塞丢失，因此被视为“撤销”丢失的数据包数量 计数 宿主，容器 procfs network netstat_TcpExt_TCPLostRetransmit 丢包重传个数 计数 宿主，容器 procfs network netstat_TcpExt_TCPMD5Failure 校验错误 计数 宿主，容器 procfs network netstat_TcpExt_TCPMD5NotFound 校验错误 计数 宿主，容器 procfs network netstat_TcpExt_TCPMD5Unexpected 校验错误 计数 宿主，容器 procfs network netstat_TcpExt_TCPMTUPFail 使用 DSACK 无需慢启动即可恢复拥塞窗口 计数 宿主，容器 procfs network netstat_TcpExt_TCPMTUPSuccess - 计数 宿主，容器 procfs network netstat_TcpExt_TCPMemoryPressures 到达 tcp 内存压力位 low 的次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPMemoryPressuresChrono - 计数 宿主，容器 procfs network netstat_TcpExt_TCPMinTTLDrop - 计数 宿主，容器 procfs network netstat_TcpExt_TCPOFODrop TCP 层接收到一个乱序的数据包，但内存不足，因此丢弃它。此类数据包不会计入 TcpExtTCPOFOQueue 计数 计数 宿主，容器 procfs network netstat_TcpExt_TCPOFOMerge 接收到的顺序错误的包与上一个包有重叠。重叠部分将被丢弃。所有 TcpExtTCPOFOMerge 包也将计入 TcpExtTCPOFOQueue 计数 宿主，容器 procfs network netstat_TcpExt_TCPOFOQueue TCP 层接收到一个乱序的数据包，并且有足够的内存来排队它 计数 宿主，容器 procfs network netstat_TcpExt_TCPOrigDataSent 发送原始数据（不包括重传但包括 SYN 中的数据）的包数量。此计数器与 TcpOutSegs 不同，因为 TcpOutSegs 还跟踪纯 ACK。TCPOrigDataSent 更有助于跟踪 TCP 重传率 计数 宿主，容器 procfs network netstat_TcpExt_TCPPartialUndo 检测到一些错误的重传，在我们快速重传的同时，收到了部分确认，因此能够部分撤销我们的一些 CWND 减少 计数 宿主，容器 procfs network netstat_TcpExt_TCPPureAcks 如果数据包设置了 ACK 标志且没有数据，则是一个纯 ACK 数据包，如果内核在快速路径中处理它，TcpExtTCPHPAcks 将增加 1，如果内核在慢速路径中处理它，TcpExtTCPPureAcks 将增加 1 计数 宿主，容器 procfs network netstat_TcpExt_TCPRcvCoalesce 当数据包被 TCP 层接收但未被应用程序读取时，TCP 层会尝试合并它们。这个计数器表示在这种情况下合并了多少个数据包。如果启用了 GRO，GRO 会合并大量数据包，这些数据包不会被计算到 TcpExtTCPRcvCoalesce 中 计数 宿主，容器 procfs network netstat_TcpExt_TCPRcvCollapsed 在“崩溃”过程中释放了多少个 skbs 计数 宿主，容器 procfs network netstat_TcpExt_TCPRenoFailures TCP_CA_Disorder 阶段进入并经历 RTO 的重传失败次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPRenoRecovery 当拥塞控制进入恢复状态时，如果使用 sack，TcpExtTCPSackRecovery 增加 1，如果不使用 sack，TcpExtTCPRenoRecovery 增加 1。这两个计数器意味着协议栈开始重传丢失的数据包 计数 宿主，容器 procfs network netstat_TcpExt_TCPRenoRecoveryFail 进入恢复阶段并 RTO 的连接数 计数 宿主，容器 procfs network netstat_TcpExt_TCPRenoReorder 重排序数据包被快速恢复检测到。只有在 SACK 被禁用时才会使用 计数 宿主，容器 procfs network netstat_TcpExt_TCPReqQFullDoCookies - 计数 宿主，容器 procfs network netstat_TcpExt_TCPReqQFullDrop - 计数 宿主，容器 procfs network netstat_TcpExt_TCPRetransFail 尝试将重传数据包发送到下层，但下层返回错误 计数 宿主，容器 procfs network netstat_TcpExt_TCPSACKDiscard 有多少个 SACK 块无效。如果无效的 SACK 块是由 ACK 记录引起的，tcp 栈只会忽略它，而不会更新此计数器 计数 宿主，容器 procfs network netstat_TcpExt_TCPSACKReneging 一个数据包被 SACK 确认，但接收方已丢弃此数据包，因此发送方需要重传此数据包 计数 宿主，容器 procfs network netstat_TcpExt_TCPSACKReorder SACK 检测到的重排序数据包 计数 宿主，容器 procfs network netstat_TcpExt_TCPSYNChallenge 响应 SYN 数据包发送的 Challenge ack 数 计数 宿主，容器 procfs network netstat_TcpExt_TCPSackFailures TCP_CA_Disorder 阶段进入并经历 RTO 的重传失败次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPSackMerged skb 已合并计数 计数 宿主，容器 procfs network netstat_TcpExt_TCPSackRecovery 当拥塞控制进入恢复状态时，如果使用 sack，TcpExtTCPSackRecovery 增加 1，如果不使用 sack，TcpExtTCPRenoRecovery 增加 1。这两个计数器意味着 TCP 栈开始重传丢失的数据包 计数 宿主，容器 procfs network netstat_TcpExt_TCPSackRecoveryFail SACK 恢复失败的次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPSackShiftFallback skb 应该被移动或合并，但由于某些原因，TCP 堆栈没有这样做 计数 宿主，容器 procfs network netstat_TcpExt_TCPSackShifted skb 被移位 计数 宿主，容器 procfs network netstat_TcpExt_TCPSlowStartRetrans 重新传输一个数据包，拥塞控制状态为“丢失” 计数 宿主，容器 procfs network netstat_TcpExt_TCPSpuriousRTOs 虚假重传超时 计数 宿主，容器 procfs network netstat_TcpExt_TCPSpuriousRtxHostQueues 当 TCP 栈想要重传一个数据包，发现该数据包并未在网络中丢失，但数据包尚未发送，TCP 栈将放弃重传并更新此计数器。这可能会发生在数据包在 qdisc 或驱动程序队列中停留时间过长的情况下 计数 宿主，容器 procfs network netstat_TcpExt_TCPSynRetrans SYN 和 SYN/ACK 重传次数，将重传分解为 SYN、快速重传、超时重传等 计数 宿主，容器 procfs network netstat_TcpExt_TCPTSReorder tcp 栈在接收到时间截包而进行乱序包阀值调整的次数 计数 宿主，容器 procfs network netstat_TcpExt_TCPTimeWaitOverflow TIME_WAIT 状态的套接字因超出限制而无法分配的数量 计数 宿主，容器 procfs network netstat_TcpExt_TCPTimeouts TCP 超时事件 计数 宿主，容器 procfs network netstat_TcpExt_TCPToZeroWindowAdv TCP 接收窗口从非零值设置为零 计数 宿主，容器 procfs network netstat_TcpExt_TCPWantZeroWindowAdv 根据当前内存使用情况，TCP 栈尝试将接收窗口设置为零。但接收窗口可能仍然是一个非零值 计数 宿主，容器 procfs network netstat_TcpExt_TCPWinProbe 定期发送的 ACK 数据包数量，以确保打开窗口的反向 ACK 数据包没有丢失 计数 宿主，容器 procfs network netstat_TcpExt_TCPWqueueTooBig - 计数 宿主，容器 procfs network netstat_TcpExt_TW TCP 套接字在快速计时器中完成 time wait 状态 计数 宿主，容器 procfs network netstat_TcpExt_TWKilled TCP 套接字在慢速计时器中完成 time wait 状态 计数 宿主，容器 procfs network netstat_TcpExt_TWRecycled 等待套接字通过时间戳回收 计数 宿主，容器 procfs network netstat_Tcp_ActiveOpens TCP 层发送一个 SYN，进入 SYN-SENT 状态。每当 TcpActiveOpens 增加 1 时，TcpOutSegs 应该始终增加 1 计数 宿主，容器 procfs network netstat_Tcp_AttemptFails TCP 连接从 SYN-SENT 状态或 SYN-RCVD 状态直接过渡到 CLOSED 状态次数，加上 TCP 连接从 SYN-RCVD 状态直接过渡到 LISTEN 状态次数 计数 宿主，容器 procfs network netstat_Tcp_CurrEstab TCP 连接数，当前状态为 ESTABLISHED 或 CLOSE-WAIT 计数 宿主，容器 procfs network netstat_Tcp_EstabResets TCP 连接从 ESTABLISHED 状态或 CLOSE-WAIT 状态直接过渡到 CLOSED 状态次数 计数 宿主，容器 procfs network netstat_Tcp_InCsumErrors TCP 校验和错误 计数 宿主，容器 procfs network netstat_Tcp_InErrs 错误接收到的段总数（例如，错误的 TCP 校验和） 计数 宿主，容器 procfs network netstat_Tcp_InSegs TCP 层接收到的数据包数量。如 RFC1213 所述，包括接收到的错误数据包，如校验和错误、无效 TCP 头等 计数 宿主，容器 procfs network netstat_Tcp_MaxConn 可以支持的总 TCP 连接数限制，在最大连接数动态的实体中，此对象应包含值-1 计数 宿主，容器 procfs network netstat_Tcp_OutRsts TCP 段中包含 RST 标志的数量 计数 宿主，容器 procfs network netstat_Tcp_OutSegs 发送的总段数，包括当前连接上的段，但不包括仅包含重传字节的段 计数 宿主，容器 procfs network netstat_Tcp_PassiveOpens TCP 连接从监听状态直接过渡到 SYN-RCVD 状态的次数 计数 宿主，容器 procfs network netstat_Tcp_RetransSegs 总重传段数 - 即包含一个或多个先前已传输字节的 TCP 段传输的数量 计数 宿主，容器 procfs network netstat_Tcp_RtoAlgorithm The algorithm used to determine the timeout value used for retransmitting unacknowledged octets 计数 宿主，容器 procfs network netstat_Tcp_RtoMax TCP 实现允许的重传超时最大值，以毫秒为单位 毫秒 宿主，容器 procfs network netstat_Tcp_RtoMin TCP 实现允许的重传超时最小值，以毫秒为单位 毫秒 宿主，容器 procfs network sockstat_FRAG_inuse - 计数 宿主，容器 procfs network sockstat_FRAG_memory - 页计数 宿主，容器 procfs network sockstat_RAW_inuse 使用的 RAW 套接字数量 计数 宿主，容器 procfs network sockstat_TCP_alloc TCP 已分配的套接字数量 计数 宿主，容器 procfs network sockstat_TCP_inuse 已建立的 TCP 套接字数量 计数 宿主，容器 procfs network sockstat_TCP_mem 系统使用的 TCP 内存总量 页计数 系统 procfs network sockstat_TCP_mem_bytes 系统使用的 TCP 内存总量 字节(Bytes) 系统 sockstat_TCP_mem * page_size network sockstat_TCP_orphan TCP 等待关闭的连接数 计数 宿主，容器 procfs network sockstat_TCP_tw TCP 套接字终止数量 计数 宿主，容器 procfs network sockstat_UDPLITE_inuse - 计数 宿主，容器 procfs network sockstat_UDP_inuse 使用的 UDP 套接字数量 计数 宿主，容器 procfs network sockstat_UDP_mem 系统使用的 UDP 内存总量 页计数 系统 procfs network sockstat_UDP_mem_bytes 系统使用的 UDP 内存字节数总和 字节(Bytes) 系统 sockstat_UDP_mem * page_size network sockstat_sockets_used 系统使用 socket 数量 计数 系统 procfs ","categories":"","description":"","excerpt":"该文档汇总了当前 v1.0 版本支持的所有的指标，涉及CPU，内存，网络，IO。\n子系统 指标 描述 单位 统计纬度 指标来源 cpu …","ref":"/docs/zh/v2.1.0/concepts/integrated/metrics/","tags":"","title":"指标（Metrics）"},{"body":"example zh/docs/v2.0/_index.md\nThis is a placeholder page that shows you how to use this template site.\n","categories":"","description":"","excerpt":"example zh/docs/v2.0/_index.md\nThis is a placeholder page that shows …","ref":"/docs/zh/v2.0.0/","tags":"","title":"v2.0.0"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/zh/latest/contribute/","tags":"","title":"贡献"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/en/latest/contribute/","tags":"","title":"Contribute"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/en/v2.1.0/contribute/","tags":"","title":"Contribute"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/zh/v2.1.0/contribute/","tags":"","title":"贡献"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/zh/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/en/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/zh/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/en/tags/","tags":"","title":"Tags"}]